1) Cel projektu

Należy zaprojektować, zbudować i ocenić dwa modele predykcyjne oceniające ryzyko (rating/scoring) na podstawie danych finansowych:

    Model interpretowalny (np. regresja logistyczna/scorecard/drzewo).
    Model „black-box” (np. XGBoost/LightGBM/SVM/sieć).

Celem jest nie tylko skuteczność predykcyjna, ale przede wszystkim zrozumienie procesu decyzyjnego tych modeli oraz prawidłowa kalibracja prawdopodobieństw do 4% (tendencja centralna PD). Wymagana jest również wyjaśnialność dwóch modeli.

2) Dane

    Zestawy danych (pliki zbiór_i.csv) zawierające zmienne finansowe i wskaźniki; zespoły pracują na przydzielonych zbiorach w oparciu o numer zbioru (i) na liście w danej grupie.
    Dołączony dokument opisujący zmienne.
    Cel (target): zdarzenie ryzyka w horyzoncie uzgodnionym na zajęciach, flaga deafult.

3) Zakres prac (etapy i rezultaty)

3.1. Walidacja i podziały

    Podział czasowy: train/validation/test lub K-fold time-series CV (min. 5 foldów).
    Dokumentacja schematu walidacji (diagram + opis).

3.2. Przygotowanie i EDA

    Kontrola jakości danych, braków, anomalii/obserwacji odstawjących, spójności czasowej; krótki Data Quality.
    EDA: rozkłady, korelacje, zależności z targetem.
    Przygotowanie cech: binning (prefer. monotoniczny), WoE/encoding, capping/outlier policy, standaryzacja.

3.3. Model interpretowalny (obowiązkowo)

    Wybór algorytmu: regresja logistyczna (możliwe spline’y/ograniczenia monotoniczności) lub drzewo/scorecard.
    Ocena jakości: AUC/ROC, PR-AUC, KS, log-loss, Brier score.
    Interpretacja globalna: znaki i wielkości współczynników/log-odds, ważność cech, krzywe PDP/ICE, profilowanie binów/WoE.
    Interpretacja lokalna: dekompozycja skoru/logitu na wkłady cech dla wybranych obserwacji (case studies).
    Stabilność: metryki na foldach/oknach czasu; VIF/kolinearność; testy monotoniczności kierunków wpływu.
    Wymóg budowy modelu interpretowalnego i omówienia wpływu cech wynika z archiwalnego opisu.

3.4. Model „black-box”

    Wybór algorytmu: XGBoost/LightGBM/SVM/sieć (płytka).
    Tuning (Bayes/Random/Grid) z kontrolą overfittingu.
    Wyjaśnialność model-agnostic: SHAP (summary, beeswarm, dependence), LIME (kilka obserwacji), „what-if”.
    Porównanie jakości i spójności wyjaśnień z modelem interpretowalnym.
    Analiza SHAP/LIME i prezentacja wyjaśnień była wymagana wcześniej.

3.5. Kalibracja probabilistyczna — obowiązkowo 4%

    Diagnostyka „pre-cal”: krzywe reliability, histogram predykcji, ECE/ACE, Brier (dekompozycja: kalibracja vs rozdzielczość).
    Metody: Platt scaling (logit), Isotonic regression, Beta calibration.
    Calibration-in-the-large: dostrojenie interceptu/slope, by średnia przewidywanych PD ≈ 4% (globalnie; opcjonalnie per segment).
    Walidacja „post-cal”: ponowna ocena ECE/Brier/krzywych; sprawdzenie stabilności per podgrupa.

3.6. Progi decyzji i mapowanie na ratingi

    Definicja funkcji kosztu/korzyści (TP/FP/FN/…); dobór progu operacyjnego (ROC/PR/cost curves).
    Mapowanie PD → klasy ratingowe (np. AAA…D) z wymogiem monotoniczności PD między klasami.
    Tabele decyzyjne i przykład wpływu progu na wyniki biznesowe.

3.7. Raport z interpretowalności/wyjaśnialności

    Global: 5–10 najważniejszych cech z komentarzem „dlaczego” + zgodność z intuicją/ekonomią.
    Lokalnie: 3–5 studiów przypadku (w tym jeden „graniczny”) z SHAP/LIME i wnioskami operacyjnymi.
    Równość i stabilność: szybki audyt podgrup (różnice w ECE/Brier/AUC/PD); komentarz, czy wyjaśnienia są stabilne.

3.8. Prezentacja dla odbiorców nietechnicznych

    10–12 slajdów: problem → dane → wyjaśnienia (global/local) → kalibracja 4% → progi i wpływ na decyzje → ryzyka/limity → rekomendacje.

4) Metryki (minimum)

    Skuteczność: AUC, PR-AUC, KS, log-loss.
    Kalibracja: Brier, ECE/ACE, calibration-in-the-large (intercept), calibration slope, krzywe reliability (pre/post-cal).
    Stabilność: wariancja metryk w CV, ewentualnie PSI/CSI dla cech i skoru (jeśli czas pozwala).
    Złożoność: liczba cech, ograniczenia monotoniczne, rozmiar drzewa/liczba reguł.

5) Wymagane materiały do oddania

    Repo/projekt z pipeline’em: EDA ->  cechy -> walidacja -> modele -> kalibracja 4% -> progi/ratingi.
    Notebook(i) + skrypty; plik requirements.txt/environment.yml.
    Raport techniczny (PDF/Markdown) z pełnymi wynikami i sekcją interpretowalności/wyjaśnialności oraz sekcją kalibracji 4% (krzywe, Brier, ECE).
    Prezentacja dla osób nietechnicznych (PDF lub PPTX).
    Plik MODEL_CARD.md: cel, dane, ograniczenia, ryzyka, wyjaśnienia global/lokal, wyniki kalibracji (4%), plan monitoringu.

6) Ramy organizacyjne

    Zespoły: 2 osoby.
    Technologie (przykładowe): Python (pandas, scikit-learn, xgboost/lightgbm, shap, lime, matplotlib).
    Kontrola wersji: krótki README z instrukcją uruchomienia.
    Replikowalność: seed, zapis podziałów, wersjonowanie cech.

7) Kryteria oceny (100 pkt)

    Poprawność danych i EDA – 3,5 pkt
    Walidacja i brak leaków – 3,5 pkt
    Model interpretowalny – 5,0 pkt
    Model „black-box” – 5,0 pkt
    Wyjaśnialność (global + lokal) – 5,5 pkt
    Kalibracja do 4% (pre/post, ECE/Brier, reliability) – 5,5 pkt
    Progi/ratingi + wpływ biznesowy – 4,0 pkt
    Prezentacja dla osób nietechnicznych – 1,5 pkt
    Model Card + dokumentacja – 1,5 pkt
    Razem: 35 pkt

8) Zasady i etyka

    Transparentność: wszystkie decyzje projektowe w README/raporcie.
    Reproducibility: wynik ma dać się odtworzyć one-clickiem (skrypt Make/run.py).
    Akademicka uczciwość: własna praca, cytowanie źródeł i bibliotek.

9) Minimalny „Definition of Done”

    Są dwa modele (interpretowalny i „black-box”).
    Jest komplet wyjaśnień (global + lokal) dla obu.
    Jest kalibracja do 4% i jej weryfikacja.
    Jest mapowanie PD -> ratingi + uzasadniony próg.
    Jest prezentacja i pełna dokumentacja.
