{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcee9b30",
   "metadata": {},
   "source": [
    "# üîß Projekt: Interpretowalne Modele ML dla Finans√≥w\n",
    "\n",
    "## 1. Setup i Instalacja Bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install_package(package_name, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"[OK] {package_name} ju≈º zainstalowany\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"[WARNING] {package_name} nie znaleziony. Instalujƒô...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name])\n",
    "            print(f\"[OK] {package_name} zainstalowany pomy≈õlnie\")\n",
    "            return True\n",
    "        except:\n",
    "            print(f\"[ERROR] Nie uda≈Ço siƒô zainstalowaƒá {package_name}\")\n",
    "            return False\n",
    "\n",
    "required_packages = [\n",
    "    ('xgboost', 'xgboost'),\n",
    "    ('lightgbm', 'lightgbm'),\n",
    "    ('scikit-optimize', 'skopt'),\n",
    "    ('imbalanced-learn', 'imblearn'),\n",
    "    ('shap', 'shap'),\n",
    "    ('lime', 'lime')\n",
    "]\n",
    "\n",
    "print(\"[INFO] SPRAWDZANIE I INSTALACJA WYMAGANYCH BIBLIOTEK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_installed = True\n",
    "for package_name, import_name in required_packages:\n",
    "    if not check_and_install_package(package_name, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "print(\"=\"*80)\n",
    "if all_installed:\n",
    "    print(\"[OK] Wszystkie biblioteki gotowe do u≈ºycia!\")\n",
    "else:\n",
    "    print(\"[WARNING] Niekt√≥re biblioteki nie zosta≈Çy zainstalowane - sprawd≈∫ b≈Çƒôdy powy≈ºej\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"[WARNING] xgboost not installed - uruchom kom√≥rkƒô instalacyjnƒÖ powy≈ºej\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    SMOTE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"[WARNING] imbalanced-learn not installed - uruchom kom√≥rkƒô instalacyjnƒÖ powy≈ºej\")\n",
    "    SMOTE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"[WARNING] shap not installed - uruchom kom√≥rkƒô instalacyjnƒÖ powy≈ºej\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"\\n[OK] Biblioteki za≈Çadowane\")\n",
    "print(f\"   XGBoost: {XGBOOST_AVAILABLE}\")\n",
    "print(f\"   SMOTE (imbalanced-learn): {SMOTE_AVAILABLE}\")\n",
    "print(f\"   SHAP: {SHAP_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f233d",
   "metadata": {},
   "source": [
    "## 2. Wczytanie i Podzia≈Ç Danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87872ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import load_and_prepare_data\n",
    "\n",
    "# Wczytaj i przygotuj dane u≈ºywajƒÖc modu≈Çu data_loader\n",
    "data = load_and_prepare_data(\n",
    "    filepath='zbior_10.csv',\n",
    "    target_column='default',\n",
    "    test_size=0.2,\n",
    "    val_size=0.25,\n",
    "    random_state=42,\n",
    "    impute_strategy='median',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Rozpakuj wyniki\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "X_train_full_combined = data['X_train_full_combined']\n",
    "y_train_full_combined = data['y_train_full_combined']\n",
    "numeric_cols = data['numeric_cols']\n",
    "categorical_cols = data['categorical_cols']\n",
    "\n",
    "# Utw√≥rz te≈º zmienne X i y dla kompatybilno≈õci z dalszym kodem\n",
    "X = pd.concat([X_train, X_val, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_val, y_test], axis=0)\n",
    "\n",
    "print(\"\\n[OK] Dane wczytane i przygotowane przy u≈ºyciu src.data_loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3257a",
   "metadata": {},
   "source": [
    "## 2.5. EDA - Analiza Surowych Danych\n",
    "\n",
    "Analiza danych przed transformacjami Box-Cox, winsoryzacjƒÖ i standaryzacjƒÖ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import load_and_prepare_data\n",
    "\n",
    "# Wczytaj i przygotuj dane w jednym kroku\n",
    "data = load_and_prepare_data(\n",
    "    filepath='zbior_10.csv',\n",
    "    target_column='default',\n",
    "    test_size=0.2,\n",
    "    val_size=0.25,\n",
    "    random_state=42,\n",
    "    impute_strategy='median',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Rozpakuj wyniki\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "X_train_full_combined = data['X_train_full_combined']\n",
    "y_train_full_combined = data['y_train_full_combined']\n",
    "numeric_cols = data['numeric_cols']\n",
    "categorical_cols = data['categorical_cols']\n",
    "\n",
    "print(\"\\n[OK] Dane wczytane i przygotowane przy u≈ºyciu src.data_loader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, log_loss, brier_score_loss\n",
    "\n",
    "def calculate_all_metrics(y_true, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_proba)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    ks_statistic = np.max(tpr - fpr)\n",
    "    \n",
    "    logloss = log_loss(y_true, y_proba)\n",
    "    brier = brier_score_loss(y_true, y_proba)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'ks_statistic': ks_statistic,\n",
    "        'log_loss': logloss,\n",
    "        'brier': brier\n",
    "    }\n",
    "\n",
    "def print_model_metrics(metrics, model_name):\n",
    "    print(f\"\\nüìä METRYKI: {model_name}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metryka':<20} {'Warto≈õƒá':>10}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Accuracy':<20} {metrics['accuracy']:>10.4f}\")\n",
    "    print(f\"{'Precision':<20} {metrics['precision']:>10.4f}\")\n",
    "    print(f\"{'Recall':<20} {metrics['recall']:>10.4f}\")\n",
    "    print(f\"{'F1-Score':<20} {metrics['f1']:>10.4f}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'ROC-AUC':<20} {metrics['roc_auc']:>10.4f}\")\n",
    "    print(f\"{'PR-AUC':<20} {metrics['pr_auc']:>10.4f}\")\n",
    "    print(f\"{'KS Statistic':<20} {metrics['ks_statistic']:>10.4f}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Log-Loss':<20} {metrics['log_loss']:>10.4f}\")\n",
    "    print(f\"{'Brier Score':<20} {metrics['brier']:>10.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Helper functions zdefiniowane (calculate_all_metrics, print_model_metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227abb8",
   "metadata": {},
   "source": [
    "## 2.5. Helper Functions - Metryki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d713d29",
   "metadata": {},
   "source": [
    "## 3. Klasy Pipeline'√≥w\n",
    "\n",
    "### 3.1 Funkcje Pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing_pipeline import (\n",
    "    identify_columns_to_drop,\n",
    "    calculate_winsorization_limits\n",
    ")\n",
    "\n",
    "print(\"[OK] Funkcje pomocnicze zaimportowane z src.preprocessing_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851fec8",
   "metadata": {},
   "source": [
    "### 3.2 InterpretableColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing_pipeline import InterpretableColumnTransformer\n",
    "\n",
    "print(\"[OK] InterpretableColumnTransformer zaimportowany z src.preprocessing_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551b30a",
   "metadata": {},
   "source": [
    "### 3.3 Full Pipeline (InterpretablePreprocessingPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing_pipeline import InterpretablePreprocessingPipeline\n",
    "\n",
    "print(\"[OK] InterpretablePreprocessingPipeline zaimportowany z src.preprocessing_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f334c2f",
   "metadata": {},
   "source": [
    "### 3.4 Minimal Pipeline (MinimalPreprocessingPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86359561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing_pipeline import MinimalPreprocessingPipeline\n",
    "\n",
    "print(\"[OK] MinimalPreprocessingPipeline zaimportowany z src.preprocessing_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d153ab",
   "metadata": {},
   "source": [
    "# üìä CZƒò≈öƒÜ I: FULL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353e641",
   "metadata": {},
   "source": [
    "## 4. Full Pipeline - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja i dopasowanie pipeline\n",
    "pipeline_full = InterpretablePreprocessingPipeline(\n",
    "    correlation_threshold=0.95,\n",
    "    keep_sparse_as_binary=True\n",
    ")\n",
    "\n",
    "print(\"Dopasowywanie Full Pipeline...\")\n",
    "X_train_full = pipeline_full.fit_transform(X_train, y_train)\n",
    "X_test_full = pipeline_full.transform(X_test)\n",
    "X_val_full = pipeline_full.transform(X_val)\n",
    "\n",
    "\n",
    "print(f\"\\n‚úÖ Full Pipeline gotowy\")\n",
    "print(f\"   Train: {X_train_full.shape}\")\n",
    "print(f\"   Test: {X_test_full.shape}\")\n",
    "print(f\"   NaN: {X_train_full.isna().sum().sum()}\")\n",
    "print(f\"   Inf: {np.isinf(X_train_full.values).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"[DATA] EDA - SUROWE DANE (przed Full Pipeline preprocessing)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train_raw_backup = X_train.copy()\n",
    "numeric_cols_raw = X_train_raw_backup.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. KORELACJA MIƒòDZY CECHAMI - Surowe dane\n",
    "# ============================================================================\n",
    "print(\"\\n[1] KORELACJA MIƒòDZY CECHAMI - Surowe dane (przed preprocessing)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "corr_matrix_raw = X_train_raw_backup[numeric_cols_raw].corr()\n",
    "\n",
    "print(f\"\\n[DATA] Liczba cech numerycznych: {len(numeric_cols_raw)}\")\n",
    "print(f\"   Shape macierzy: {corr_matrix_raw.shape}\")\n",
    "\n",
    "print(f\"[INFO] Zakres warto≈õci: [{corr_matrix_raw.min().min():.3f}, {corr_matrix_raw.max().max():.3f}]\")\n",
    "\n",
    "corr_values_raw = corr_matrix_raw.values[np.triu_indices_from(corr_matrix_raw.values, k=1)]\n",
    "\n",
    "high_corr_pairs_raw = []\n",
    "for i in range(len(corr_matrix_raw.columns)):\n",
    "    for j in range(i+1, len(corr_matrix_raw.columns)):\n",
    "        corr_val = corr_matrix_raw.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            feat1 = corr_matrix_raw.columns[i]\n",
    "            feat2 = corr_matrix_raw.columns[j]\n",
    "            high_corr_pairs_raw.append((feat1, feat2, corr_val))\n",
    "\n",
    "if len(high_corr_pairs_raw) > 0:\n",
    "    print(f\"\\n[WARNING] Znaleziono {len(high_corr_pairs_raw)} par cech o wysokiej korelacji (|r| > 0.7):\")\n",
    "    for feat1, feat2, corr_val in high_corr_pairs_raw[:15]:\n",
    "        direction = \"[+]\" if corr_val > 0 else \"[-]\"\n",
    "        print(f\"   {direction} {feat1:<35} <-> {feat2:<35} r = {corr_val:+.3f}\")\n",
    "else:\n",
    "    print(\"\\n[OK] Brak par cech o wysokiej korelacji (|r| > 0.7)\")\n",
    "\n",
    "print(f\"\\n[DATA] Statystyki korelacji (wszystkie pary cech):\")\n",
    "print(f\"   ≈örednia |r|:  {np.abs(corr_values_raw).mean():.3f}\")\n",
    "print(f\"   Mediana |r|:  {np.median(np.abs(corr_values_raw)):.3f}\")\n",
    "print(f\"   Max |r|:      {np.abs(corr_values_raw).max():.3f}\")\n",
    "\n",
    "print(f\"\\n[DATA] Wizualizacja macierzy korelacji (TOP 30 cech):\")\n",
    "\n",
    "target_corr_raw = []\n",
    "for col in numeric_cols_raw:\n",
    "    corr_with_target = X_train_raw_backup[col].corr(pd.Series(y_train.values))\n",
    "    target_corr_raw.append({\n",
    "        'Feature': col,\n",
    "        'Correlation': corr_with_target,\n",
    "        'Abs_Correlation': abs(corr_with_target)\n",
    "    })\n",
    "\n",
    "target_corr_raw_df = pd.DataFrame(target_corr_raw).sort_values('Abs_Correlation', ascending=False)\n",
    "top30_features_raw = target_corr_raw_df.head(30)['Feature'].tolist()\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seaborn\", \"-q\"])\n",
    "    import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix_raw[top30_features_raw].loc[top30_features_raw], \n",
    "            annot=False,\n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            vmin=-1, \n",
    "            vmax=1,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Macierz Korelacji - Surowe Dane (TOP 30 cech)', fontsize=14, pad=20)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=7)\n",
    "plt.yticks(rotation=0, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"   [OK] Heatmap wygenerowana dla TOP 30 cech\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. KORELACJA Z TARGETEM - Surowe dane\n",
    "# ============================================================================\n",
    "print(\"\\n\\n[2] KORELACJA Z TARGETEM - Surowe dane\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[TARGET] TOP 15 CECH najbardziej skorelowanych z ryzykiem defaultu:\")\n",
    "print(f\"{'Rank':<6} {'Feature':<45} {'Correlation':>12} {'Direction':<15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in target_corr_raw_df.head(15).iterrows():\n",
    "    rank = target_corr_raw_df.index.get_loc(idx) + 1\n",
    "    full_corr = X_train_raw_backup[row['Feature']].corr(pd.Series(y_train.values))\n",
    "    direction = \"[+] Higher = More Risk\" if full_corr > 0 else \"[-] Higher = Less Risk\"\n",
    "    print(f\"{rank:<6} {row['Feature']:<45} {full_corr:>+12.4f} {direction:<15}\")\n",
    "\n",
    "print(f\"\\n[DATA] Statystyki korelacji z targetem:\")\n",
    "print(f\"   ≈örednia |r|:  {target_corr_raw_df['Abs_Correlation'].mean():.4f}\")\n",
    "print(f\"   Mediana |r|:  {target_corr_raw_df['Abs_Correlation'].median():.4f}\")\n",
    "print(f\"   Max |r|:      {target_corr_raw_df['Abs_Correlation'].max():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PODSUMOWANIE\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"[DATA] PODSUMOWANIE - Surowe Dane (przed preprocessing)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[OK] WIELOKOLINEARNO≈öƒÜ:\")\n",
    "print(f\"   ‚Ä¢ Pary o |r| > 0.7: {len(high_corr_pairs_raw)}\")\n",
    "print(f\"   ‚Ä¢ ≈örednia |r|: {np.abs(corr_values_raw).mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ Status: {'[WARNING] WYSOKA wielokolinearno≈õƒá' if len(high_corr_pairs_raw) > 20 else '[OK] Umiarkowana wielokolinearno≈õƒá'}\")\n",
    "\n",
    "print(f\"\\n[OK] MOC PREDYKCYJNA:\")\n",
    "print(f\"   ‚Ä¢ Najsilniejsza: {target_corr_raw_df.iloc[0]['Feature']} (|r| = {target_corr_raw_df.iloc[0]['Abs_Correlation']:.4f})\")\n",
    "print(f\"   ‚Ä¢ ≈örednia |r|: {target_corr_raw_df['Abs_Correlation'].mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Cechy o |r| > 0.1: {(target_corr_raw_df['Abs_Correlation'] > 0.1).sum()}/{len(target_corr_raw_df)}\")\n",
    "\n",
    "print(f\"\\n[INFO] Nastƒôpny krok: Full Pipeline preprocessing (Box-Cox, winsoryzacja, standaryzacja)\")\n",
    "\n",
    "top10_features_raw = target_corr_raw_df.head(10)['Feature'].tolist()\n",
    "\n",
    "print(f\"\\n[INFO] Zapisano TOP 10 cech do por√≥wnania rozk≈Çad√≥w PO transformacji (Section 5.6):\")\n",
    "for idx, feat in enumerate(top10_features_raw, 1):\n",
    "    full_corr = X_train_raw_backup[feat].corr(pd.Series(y_train.values))\n",
    "    print(f\"   {idx:2d}. {feat:<45} r = {full_corr:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad9a3e",
   "metadata": {},
   "source": [
    "## 5. Full Pipeline - Modele Interpretwalne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S≈Çownik do przechowywania wynik√≥w\n",
    "results_full = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e090b",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression (Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b469d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION - FULL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lr_full = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_full.fit(X_train_full, y_train)\n",
    "\n",
    "y_pred_lr_full = lr_full.predict(X_test_full)\n",
    "y_proba_lr_full = lr_full.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "metrics_lr_full = calculate_all_metrics(y_test, y_pred_lr_full, y_proba_lr_full)\n",
    "results_full['LR'] = {'model': lr_full, **metrics_lr_full}\n",
    "\n",
    "print_model_metrics(metrics_lr_full, \"Logistic Regression - Full Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12c66d",
   "metadata": {},
   "source": [
    "### 5.2 Decision Tree (Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DECISION TREE - FULL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dt_full = DecisionTreeClassifier(\n",
    "    max_depth=5, \n",
    "    min_samples_split=100, \n",
    "    min_samples_leaf=50,\n",
    "    random_state=42, \n",
    "    class_weight='balanced'\n",
    ")\n",
    "dt_full.fit(X_train_full, y_train)\n",
    "\n",
    "y_pred_dt_full = dt_full.predict(X_test_full)\n",
    "y_proba_dt_full = dt_full.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "metrics_dt_full = calculate_all_metrics(y_test, y_pred_dt_full, y_proba_dt_full)\n",
    "results_full['DT'] = {'model': dt_full, **metrics_dt_full}\n",
    "\n",
    "print_model_metrics(metrics_dt_full, \"Decision Tree - Full Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a64f28",
   "metadata": {},
   "source": [
    "### 5.3 Naive Bayes (Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NAIVE BAYES - FULL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "nb_full = GaussianNB()\n",
    "nb_full.fit(X_train_full, y_train)\n",
    "\n",
    "y_pred_nb_full = nb_full.predict(X_test_full)\n",
    "y_proba_nb_full = nb_full.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "metrics_nb_full = calculate_all_metrics(y_test, y_pred_nb_full, y_proba_nb_full)\n",
    "results_full['NB'] = {'model': nb_full, **metrics_nb_full}\n",
    "\n",
    "print_model_metrics(metrics_nb_full, \"Naive Bayes - Full Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eba3a2",
   "metadata": {},
   "source": [
    "## 6. Full Pipeline - Modele Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749cb76",
   "metadata": {},
   "source": [
    "### 6.1 Inicjalizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S≈Çownik do przechowywania wynik√≥w modeli black box (Full Pipeline)\n",
    "results_blackbox_full = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0f31a",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest (Black Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078af759",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RANDOM FOREST - BLACK BOX (FULL PIPELINE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_blackbox_full = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_blackbox_full.fit(X_train_full, y_train)\n",
    "\n",
    "y_pred_rf_blackbox_full = rf_blackbox_full.predict(X_test_full)\n",
    "y_proba_rf_blackbox_full = rf_blackbox_full.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "metrics_rf_blackbox = calculate_all_metrics(y_test, y_pred_rf_blackbox_full, y_proba_rf_blackbox_full)\n",
    "results_blackbox_full['RF'] = {'model': rf_blackbox_full, **metrics_rf_blackbox}\n",
    "\n",
    "print_model_metrics(metrics_rf_blackbox, \"Random Forest - Black Box Full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b088d6b",
   "metadata": {},
   "source": [
    "### 5.5.2 XGBoost (Black Box - Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"XGBOOST - BLACK BOX (FULL PIPELINE)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    xgb_blackbox_full = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    xgb_blackbox_full.set_params(scale_pos_weight=scale_pos_weight)\n",
    "    \n",
    "    xgb_blackbox_full.fit(X_train_full, y_train)\n",
    "    \n",
    "    y_pred_xgb_blackbox_full = xgb_blackbox_full.predict(X_test_full)\n",
    "    y_proba_xgb_blackbox_full = xgb_blackbox_full.predict_proba(X_test_full)[:, 1]\n",
    "    \n",
    "    metrics_xgb_blackbox = calculate_all_metrics(y_test, y_pred_xgb_blackbox_full, y_proba_xgb_blackbox_full)\n",
    "    results_blackbox_full['XGB'] = {'model': xgb_blackbox_full, **metrics_xgb_blackbox}\n",
    "    \n",
    "    print_model_metrics(metrics_xgb_blackbox, \"XGBoost - Black Box Full\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  XGBoost niedostƒôpny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122aaeb",
   "metadata": {},
   "source": [
    "### 5.5.3 SVM (Black Box - Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SVM - BLACK BOX (FULL PIPELINE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "svm_blackbox_full = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    probability=True\n",
    ")\n",
    "svm_blackbox_full.fit(X_train_full, y_train)\n",
    "\n",
    "y_pred_svm_blackbox_full = svm_blackbox_full.predict(X_test_full)\n",
    "y_proba_svm_blackbox_full = svm_blackbox_full.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "metrics_svm_blackbox = calculate_all_metrics(y_test, y_pred_svm_blackbox_full, y_proba_svm_blackbox_full)\n",
    "results_blackbox_full['SVM'] = {'model': svm_blackbox_full, **metrics_svm_blackbox}\n",
    "\n",
    "print_model_metrics(metrics_svm_blackbox, \"SVM - Black Box Full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aea53b",
   "metadata": {},
   "source": [
    "### 5.5.4 Neural Network (Black Box - Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK (MLP) - BLACK BOX (FULL PIPELINE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mlp_blackbox_full = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20,\n",
    "    verbose=False\n",
    ")\n",
    "mlp_blackbox_full.fit(X_train_full, y_train)\n",
    "\n",
    "y_pred_mlp_blackbox_full = mlp_blackbox_full.predict(X_test_full)\n",
    "y_proba_mlp_blackbox_full = mlp_blackbox_full.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "metrics_mlp_blackbox = calculate_all_metrics(y_test, y_pred_mlp_blackbox_full, y_proba_mlp_blackbox_full)\n",
    "results_blackbox_full['Neural Network'] = {'model': mlp_blackbox_full, **metrics_mlp_blackbox}\n",
    "\n",
    "print_model_metrics(metrics_mlp_blackbox, \"Neural Network (MLP) - Black Box Full\")\n",
    "print(f\"  Liczba iteracji: {mlp_blackbox_full.n_iter_}\")\n",
    "print(f\"  Liczba warstw: {len(mlp_blackbox_full.hidden_layer_sizes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e37950",
   "metadata": {},
   "source": [
    "# CZƒò≈öƒÜ 2: MINIMAL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589b0d4",
   "metadata": {},
   "source": [
    "## 6. Minimal Pipeline - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_minimal = MinimalPreprocessingPipeline(\n",
    "    correlation_threshold=0.80,\n",
    "    standardize=True\n",
    ")\n",
    "\n",
    "print(\"Dopasowywanie Minimal Pipeline...\")\n",
    "X_train_minimal = pipeline_minimal.fit_transform(X_train, y_train)\n",
    "X_test_minimal = pipeline_minimal.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Minimal Pipeline gotowy\")\n",
    "print(f\"   Train: {X_train_minimal.shape}\")\n",
    "print(f\"   Test: {X_test_minimal.shape}\")\n",
    "print(f\"   NaN: {X_train_minimal.isna().sum().sum()}\")\n",
    "print(f\"   Inf: {np.isinf(X_train_minimal.values).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c47238",
   "metadata": {},
   "source": [
    "## 7. Minimal Pipeline - Trening Modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S≈Çownik do przechowywania wynik√≥w\n",
    "results_minimal = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cba28",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression (Minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296891e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION - MINIMAL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lr_minimal = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_minimal.fit(X_train_minimal, y_train)\n",
    "\n",
    "y_pred_lr_minimal = lr_minimal.predict(X_test_minimal)\n",
    "y_proba_lr_minimal = lr_minimal.predict_proba(X_test_minimal)[:, 1]\n",
    "\n",
    "metrics_lr_minimal = calculate_all_metrics(y_test, y_pred_lr_minimal, y_proba_lr_minimal)\n",
    "results_minimal['LR'] = {'model': lr_minimal, **metrics_lr_minimal}\n",
    "\n",
    "print_model_metrics(metrics_lr_minimal, \"Logistic Regression - Minimal Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119f09e",
   "metadata": {},
   "source": [
    "### 7.2 Decision Tree (Minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DECISION TREE - MINIMAL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dt_minimal = DecisionTreeClassifier(\n",
    "    max_depth=5, \n",
    "    min_samples_split=100, \n",
    "    min_samples_leaf=50,\n",
    "    random_state=42, \n",
    "    class_weight='balanced'\n",
    ")\n",
    "dt_minimal.fit(X_train_minimal, y_train)\n",
    "\n",
    "y_pred_dt_minimal = dt_minimal.predict(X_test_minimal)\n",
    "y_proba_dt_minimal = dt_minimal.predict_proba(X_test_minimal)[:, 1]\n",
    "\n",
    "metrics_dt_minimal = calculate_all_metrics(y_test, y_pred_dt_minimal, y_proba_dt_minimal)\n",
    "results_minimal['DT'] = {'model': dt_minimal, **metrics_dt_minimal}\n",
    "\n",
    "print_model_metrics(metrics_dt_minimal, \"Decision Tree - Minimal Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f6fdf",
   "metadata": {},
   "source": [
    "### 7.3 Naive Bayes (Minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NAIVE BAYES - MINIMAL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "nb_minimal = GaussianNB()\n",
    "nb_minimal.fit(X_train_minimal, y_train)\n",
    "\n",
    "y_pred_nb_minimal = nb_minimal.predict(X_test_minimal)\n",
    "y_proba_nb_minimal = nb_minimal.predict_proba(X_test_minimal)[:, 1]\n",
    "\n",
    "metrics_nb_minimal = calculate_all_metrics(y_test, y_pred_nb_minimal, y_proba_nb_minimal)\n",
    "results_minimal['NB'] = {'model': nb_minimal, **metrics_nb_minimal}\n",
    "\n",
    "print_model_metrics(metrics_nb_minimal, \"Naive Bayes - Minimal Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e3d4b",
   "metadata": {},
   "source": [
    "# CZƒò≈öƒÜ 2.5: MODELE BLACK BOX (MINIMAL PIPELINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc5ca8",
   "metadata": {},
   "source": [
    "## 7.5 Modele Black Box - S≈Çownik Wynik√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S≈Çownik do przechowywania wynik√≥w modeli black box\n",
    "results_blackbox = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c36b79",
   "metadata": {},
   "source": [
    "### 7.5.1 Random Forest (Black Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RANDOM FOREST - BLACK BOX (MINIMAL PIPELINE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_blackbox = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_blackbox.fit(X_train_minimal, y_train)\n",
    "\n",
    "y_pred_rf_blackbox = rf_blackbox.predict(X_test_minimal)\n",
    "y_proba_rf_blackbox = rf_blackbox.predict_proba(X_test_minimal)[:, 1]\n",
    "\n",
    "metrics_rf_blackbox_min = calculate_all_metrics(y_test, y_pred_rf_blackbox, y_proba_rf_blackbox)\n",
    "results_blackbox['RF'] = {'model': rf_blackbox, **metrics_rf_blackbox_min}\n",
    "\n",
    "print_model_metrics(metrics_rf_blackbox_min, \"Random Forest - Black Box Minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891bc4c",
   "metadata": {},
   "source": [
    "### 7.5.2 XGBoost (Black Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccf451",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"XGBOOST - BLACK BOX (MINIMAL PIPELINE)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    xgb_blackbox = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    xgb_blackbox.set_params(scale_pos_weight=scale_pos_weight)\n",
    "    \n",
    "    xgb_blackbox.fit(X_train_minimal, y_train)\n",
    "    \n",
    "    y_pred_xgb_blackbox = xgb_blackbox.predict(X_test_minimal)\n",
    "    y_proba_xgb_blackbox = xgb_blackbox.predict_proba(X_test_minimal)[:, 1]\n",
    "    \n",
    "    metrics_xgb_blackbox_min = calculate_all_metrics(y_test, y_pred_xgb_blackbox, y_proba_xgb_blackbox)\n",
    "    results_blackbox['XGB'] = {'model': xgb_blackbox, **metrics_xgb_blackbox_min}\n",
    "    \n",
    "    print_model_metrics(metrics_xgb_blackbox_min, \"XGBoost - Black Box Minimal\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  XGBoost niedostƒôpny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa73ae",
   "metadata": {},
   "source": [
    "### 7.5.3 SVM (Black Box - Minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SVM - BLACK BOX (MINIMAL PIPELINE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "svm_blackbox = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    probability=True\n",
    ")\n",
    "svm_blackbox.fit(X_train_minimal, y_train)\n",
    "\n",
    "y_pred_svm_blackbox = svm_blackbox.predict(X_test_minimal)\n",
    "y_proba_svm_blackbox = svm_blackbox.predict_proba(X_test_minimal)[:, 1]\n",
    "\n",
    "metrics_svm_blackbox_min = calculate_all_metrics(y_test, y_pred_svm_blackbox, y_proba_svm_blackbox)\n",
    "results_blackbox['SVM'] = {'model': svm_blackbox, **metrics_svm_blackbox_min}\n",
    "\n",
    "print_model_metrics(metrics_svm_blackbox_min, \"SVM - Black Box Minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87779f03",
   "metadata": {},
   "source": [
    "### 7.5.4 Neural Network (Black Box - Minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK (MLP) - BLACK BOX (MINIMAL PIPELINE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mlp_blackbox = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20,\n",
    "    verbose=False\n",
    ")\n",
    "mlp_blackbox.fit(X_train_minimal, y_train)\n",
    "\n",
    "y_pred_mlp_blackbox = mlp_blackbox.predict(X_test_minimal)\n",
    "y_proba_mlp_blackbox = mlp_blackbox.predict_proba(X_test_minimal)[:, 1]\n",
    "\n",
    "metrics_mlp_blackbox_min = calculate_all_metrics(y_test, y_pred_mlp_blackbox, y_proba_mlp_blackbox)\n",
    "results_blackbox['Neural Network'] = {'model': mlp_blackbox, **metrics_mlp_blackbox_min}\n",
    "\n",
    "print_model_metrics(metrics_mlp_blackbox_min, \"Neural Network (MLP) - Black Box Minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9965400",
   "metadata": {},
   "source": [
    "## 4.5 EDA - Analiza Po Full Pipeline Preprocessing\n",
    "\n",
    "Analiza danych po transformacjach: korelacje, top cechy, por√≥wnanie rozk≈Çad√≥w przed vs po."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15881a",
   "metadata": {},
   "source": [
    "# üìà CZƒò≈öƒÜ II: CREDIT SCORECARDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0be715",
   "metadata": {},
   "source": [
    "## 7. Weight of Evidence (WoE) - Funkcje Pomocnicze\n",
    "\n",
    "Model Credit Scorecard wykorzystuje:\n",
    "- **Weight of Evidence (WoE)**: Transformacja zmiennych na log-odds\n",
    "- **Information Value (IV)**: Miara mocy predykcyjnej zmiennych\n",
    "- **Logistic Regression**: Model na transformowanych WoE features\n",
    "- **System punkt√≥w**: Konwersja na punkty kredytowe (600 base, 20 PDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_woe_iv(df, feature, target, bins=5):\n",
    "    \"\"\"\n",
    "    Oblicza Weight of Evidence (WoE) i Information Value (IV) dla zmiennej.\n",
    "    \n",
    "    WoE = ln(% Good / % Bad)\n",
    "    IV = Œ£ (% Good - % Bad) * WoE\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_temp = pd.DataFrame({\n",
    "            'feature': df[feature],\n",
    "            'target': df[target]\n",
    "        }).dropna()\n",
    "        \n",
    "        df_temp['bin'] = pd.qcut(df_temp['feature'], q=bins, duplicates='drop')\n",
    "    except:\n",
    "        df_temp['bin'] = pd.cut(df_temp['feature'], bins=bins, duplicates='drop')\n",
    "    \n",
    "    grouped = df_temp.groupby('bin', observed=True)['target'].agg(['sum', 'count'])\n",
    "    grouped.columns = ['bad', 'total']\n",
    "    grouped['good'] = grouped['total'] - grouped['bad']\n",
    "    \n",
    "    total_good = grouped['good'].sum()\n",
    "    total_bad = grouped['bad'].sum()\n",
    "    \n",
    "    grouped['good_dist'] = grouped['good'] / total_good\n",
    "    grouped['bad_dist'] = grouped['bad'] / total_bad\n",
    "    \n",
    "    grouped['woe'] = np.log((grouped['good_dist'] + 0.0001) / (grouped['bad_dist'] + 0.0001))\n",
    "    grouped['iv'] = (grouped['good_dist'] - grouped['bad_dist']) * grouped['woe']\n",
    "    \n",
    "    iv_total = grouped['iv'].sum()\n",
    "    \n",
    "    return grouped[['good', 'bad', 'total', 'woe', 'iv']], iv_total\n",
    "\n",
    "def woe_transform(df, feature, target, bins=5):\n",
    "    try:\n",
    "        df_temp = pd.DataFrame({\n",
    "            'feature': df[feature],\n",
    "            'target': df[target]\n",
    "        }).dropna()\n",
    "        \n",
    "        df_temp['bin'] = pd.qcut(df_temp['feature'], q=bins, duplicates='drop')\n",
    "    except:\n",
    "        df_temp['bin'] = pd.cut(df_temp['feature'], bins=bins, duplicates='drop')\n",
    "    \n",
    "    woe_table, _ = calculate_woe_iv(df, feature, target, bins)\n",
    "    woe_dict = dict(zip(woe_table.index, woe_table['woe']))\n",
    "    \n",
    "    try:\n",
    "        feature_binned = pd.qcut(df[feature], q=bins, duplicates='drop')\n",
    "    except:\n",
    "        feature_binned = pd.cut(df[feature], bins=bins, duplicates='drop')\n",
    "    \n",
    "    woe_values = feature_binned.map(lambda x: woe_dict.get(x, 0) if pd.notna(x) else 0)\n",
    "    \n",
    "    return woe_values\n",
    "\n",
    "print(\"‚úÖ Funkcje WoE/IV zdefiniowane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1e9f2",
   "metadata": {},
   "source": [
    "## 9. Tabela Por√≥wnawcza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac58aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie danych do por√≥wnania\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in results_full.keys():\n",
    "    # Full pipeline (z nowymi metrykami)\n",
    "    if model_name != 'model':  # Pomi≈Ñ pole 'model'\n",
    "        comparison_data.append({\n",
    "            'Pipeline': 'Full',\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results_full[model_name].get('accuracy', 0),\n",
    "            'Precision': results_full[model_name].get('precision', 0),\n",
    "            'Recall': results_full[model_name].get('recall', 0),\n",
    "            'F1-Score': results_full[model_name].get('f1', 0),\n",
    "            'ROC-AUC': results_full[model_name].get('roc_auc', results_full[model_name].get('auc', 0))\n",
    "        })\n",
    "    \n",
    "    # Minimal pipeline\n",
    "    if model_name in results_minimal and model_name != 'model':\n",
    "        comparison_data.append({\n",
    "            'Pipeline': 'Minimal',\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results_minimal[model_name].get('accuracy', 0),\n",
    "            'Precision': results_minimal[model_name].get('precision', 0),\n",
    "            'Recall': results_minimal[model_name].get('recall', 0),\n",
    "            'F1-Score': results_minimal[model_name].get('f1', 0),\n",
    "            'ROC-AUC': results_minimal[model_name].get('roc_auc', results_minimal[model_name].get('auc', 0))\n",
    "        })\n",
    "\n",
    "# Dodaj modele black box (Full Pipeline)\n",
    "for model_name in results_blackbox_full.keys():\n",
    "    if model_name != 'model':\n",
    "        comparison_data.append({\n",
    "            'Pipeline': 'Black Box (Full)',\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results_blackbox_full[model_name].get('accuracy', 0),\n",
    "            'Precision': results_blackbox_full[model_name].get('precision', 0),\n",
    "            'Recall': results_blackbox_full[model_name].get('recall', 0),\n",
    "            'F1-Score': results_blackbox_full[model_name].get('f1', 0),\n",
    "            'ROC-AUC': results_blackbox_full[model_name].get('roc_auc', results_blackbox_full[model_name].get('auc', 0))\n",
    "        })\n",
    "\n",
    "# Dodaj modele black box (Minimal Pipeline)\n",
    "for model_name in results_blackbox.keys():\n",
    "    if model_name != 'model':\n",
    "        comparison_data.append({\n",
    "            'Pipeline': 'Black Box (Minimal)',\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results_blackbox[model_name].get('accuracy', 0),\n",
    "            'Precision': results_blackbox[model_name].get('precision', 0),\n",
    "            'Recall': results_blackbox[model_name].get('recall', 0),\n",
    "            'F1-Score': results_blackbox[model_name].get('f1', 0),\n",
    "            'ROC-AUC': results_blackbox[model_name].get('roc_auc', results_blackbox[model_name].get('auc', 0))\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*110)\n",
    "print(\"POR√ìWNANIE WSZYSTKICH MODELI - FULL vs MINIMAL vs BLACK BOX (Full & Minimal)\")\n",
    "print(\"=\"*110)\n",
    "print()\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "print(f\"\\nüî¢ Liczba cech:\")\n",
    "print(f\"   Full Pipeline:         {X_train_full.shape[1]} cech\")\n",
    "print(f\"   Minimal Pipeline:      {X_train_minimal.shape[1]} cech\")\n",
    "print(f\"   Black Box (Full):      {X_train_full.shape[1]} cech (u≈ºywa Full Pipeline)\")\n",
    "print(f\"   Black Box (Minimal):   {X_train_minimal.shape[1]} cech (u≈ºywa Minimal Pipeline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59e916",
   "metadata": {},
   "source": [
    "## 11. Wnioski\n",
    "\n",
    "**Por√≥wnanie zosta≈Ço zako≈Ñczone!**\n",
    "\n",
    "Ten notebook por√≥wna≈Ç cztery kategorie modeli:\n",
    "- **Full Pipeline (Interpretable)**: LR, DT, NB z pe≈Çnymi transformacjami statystycznymi\n",
    "- **Minimal Pipeline (Interpretable)**: LR, DT, NB z minimalnym preprocessingiem\n",
    "- **Black Box (Full Pipeline)**: Random Forest, XGBoost, SVM i Neural Network z pe≈Çnymi transformacjami\n",
    "- **Black Box (Minimal Pipeline)**: Random Forest, XGBoost, SVM i Neural Network z minimalnym preprocessingiem\n",
    "\n",
    "Sprawd≈∫ wyniki powy≈ºej, aby okre≈õliƒá:\n",
    "1. Czy z≈Ço≈ºone transformacje poprawiajƒÖ wyniki modeli interpretowalnych\n",
    "2. Jak modele black box wypadajƒÖ w por√≥wnaniu do modeli interpretowalnych\n",
    "3. Czy black box modele bardziej korzystajƒÖ z Full czy Minimal Pipeline\n",
    "4. Kt√≥ra strategia oferuje najlepszy kompromis miƒôdzy interpretowalno≈õciƒÖ a wydajno≈õciƒÖ\n",
    "5. Jak cross-validation potwierdza stabilno≈õƒá wynik√≥w modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e101fa9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üè¶ CZƒò≈öƒÜ III: ADVANCED SCORECARDS\n",
    "\n",
    "## 8. Feature Engineering dla Banking\n",
    "\n",
    "W tej sekcji stworzymy modele wykorzystujƒÖce profesjonalne techniki stosowane w bankowo≈õci:\n",
    "\n",
    "## üéØ Zastosowane techniki:\n",
    "\n",
    "### 1. **Domain Knowledge Feature Engineering**\n",
    "- Wska≈∫niki zad≈Çu≈ºenia (debt-to-income ratios)\n",
    "- Wska≈∫niki wykorzystania kredytu (utilization rates)\n",
    "- Agregaty historii kredytowej\n",
    "- Interakcje miƒôdzy zmiennymi biznesowymi\n",
    "\n",
    "### 2. **Advanced Feature Selection**\n",
    "- **Variance Inflation Factor (VIF)** - usuwanie wielokolinearno≈õci (standard w modelach bankowych)\n",
    "- **Weight of Evidence (WoE) binning** - transformacja i optymalizacja zmiennych\n",
    "- **Information Value (IV)** - rankowanie mocy predykcyjnej zmiennych\n",
    "- **Correlation clustering** - grupowanie i selekcja reprezentatywnych zmiennych\n",
    "\n",
    "### 3. **Banking Best Practices**\n",
    "\n",
    "- Fine-class binning dla zmiennych ciƒÖg≈Çych- Wyb√≥r jednej reprezentatywnej zmiennej z klastr√≥w skorelowanych (r>0.8)\n",
    "\n",
    "- Monotonic WoE constraints dla interpretowalno≈õci\n",
    "- Usuwanie zmiennych ze zbyt wysokim VIF (>10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595862",
   "metadata": {},
   "source": [
    "## 8.1 Domain Knowledge Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087152cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî® FEATURE ENGINEERING - Tworzenie zmiennych domenowych...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Zaczynamy od danych Full Pipeline (najbardziej kompletnych)\n",
    "# WA≈ªNE: Tworzymy r√≥wnie≈º dla validation set!\n",
    "X_train_advanced = X_train_full.copy()\n",
    "X_test_advanced = X_test_full.copy()\n",
    "X_val_advanced = X_val_full.copy()  # DODANO dla Grid Search\n",
    "\n",
    "print(f\"üìã Dostƒôpne kolumny: {X_train_advanced.shape[1]} zmiennych\")\n",
    "print(f\"   Przyk≈Çady: {list(X_train_advanced.columns[:10])}\")\n",
    "print(f\"üì¶ Zbiory: Train {X_train_advanced.shape}, Val {X_val_advanced.shape}, Test {X_test_advanced.shape}\")\n",
    "\n",
    "# Znajd≈∫ kolumny numeryczne (g≈Ç√≥wne cechy, nie one-hot encoded)\n",
    "numeric_cols = X_train_advanced.select_dtypes(include=[np.number]).columns.tolist()\n",
    "main_features = [col for col in numeric_cols if not any(x in col for x in ['_0', '_1', '_2', '_3', 'JOB_', 'REASON_'])]\n",
    "\n",
    "print(f\"\\nüìä Zmienne numeryczne (g≈Ç√≥wne): {len(main_features)}\")\n",
    "print(f\"   {main_features[:15]}\")\n",
    "\n",
    "# ============================================\n",
    "# 1. WSKA≈πNIKI FINANSOWE PRZEDSIƒòBIORSTW\n",
    "# ============================================\n",
    "print(\"\\n1Ô∏è‚É£  Wska≈∫niki finansowe (banking standards)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "created_features = []\n",
    "\n",
    "# Wska≈∫niki p≈Çynno≈õci\n",
    "if 'Aktywa_obrotowe' in main_features and 'Zobowiazania_krotkoterminowe' in main_features:\n",
    "    # Current Ratio - wska≈∫nik p≈Çynno≈õci bie≈ºƒÖcej\n",
    "    X_train_advanced['current_ratio'] = X_train_advanced['Aktywa_obrotowe'] / (X_train_advanced['Zobowiazania_krotkoterminowe'] + 1)\n",
    "    X_val_advanced['current_ratio'] = X_val_advanced['Aktywa_obrotowe'] / (X_val_advanced['Zobowiazania_krotkoterminowe'] + 1)\n",
    "    X_test_advanced['current_ratio'] = X_test_advanced['Aktywa_obrotowe'] / (X_test_advanced['Zobowiazania_krotkoterminowe'] + 1)\n",
    "    created_features.append('current_ratio')\n",
    "    print(f\"   ‚úì current_ratio (wska≈∫nik p≈Çynno≈õci)\")\n",
    "\n",
    "# Wska≈∫nik zad≈Çu≈ºenia\n",
    "if 'Zobowiazania' in main_features and 'Aktywa' in main_features:\n",
    "    # Debt Ratio\n",
    "    X_train_advanced['debt_ratio'] = X_train_advanced['Zobowiazania'] / (X_train_advanced['Aktywa'] + 1)\n",
    "    X_val_advanced['debt_ratio'] = X_val_advanced['Zobowiazania'] / (X_val_advanced['Aktywa'] + 1)\n",
    "    X_test_advanced['debt_ratio'] = X_test_advanced['Zobowiazania'] / (X_test_advanced['Aktywa'] + 1)\n",
    "    created_features.append('debt_ratio')\n",
    "    print(f\"   ‚úì debt_ratio (wska≈∫nik zad≈Çu≈ºenia)\")\n",
    "\n",
    "# ============================================\n",
    "# 2. WSKA≈πNIKI RENTOWNO≈öCI\n",
    "# ============================================\n",
    "print(\"\\n2Ô∏è‚É£  Wska≈∫niki rentowno≈õci\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ROA - Return on Assets\n",
    "if 'Wynik_netto' in main_features and 'Aktywa' in main_features:\n",
    "    X_train_advanced['roa'] = X_train_advanced['Wynik_netto'] / (X_train_advanced['Aktywa'] + 1)\n",
    "    X_val_advanced['roa'] = X_val_advanced['Wynik_netto'] / (X_val_advanced['Aktywa'] + 1)\n",
    "    X_test_advanced['roa'] = X_test_advanced['Wynik_netto'] / (X_test_advanced['Aktywa'] + 1)\n",
    "    created_features.append('roa')\n",
    "    print(f\"   ‚úì ROA (zwrot z aktyw√≥w)\")\n",
    "\n",
    "# ROE - Return on Equity\n",
    "if 'Wynik_netto' in main_features and 'Kapital_wlasny' in main_features:\n",
    "    X_train_advanced['roe'] = X_train_advanced['Wynik_netto'] / (X_train_advanced['Kapital_wlasny'] + 1)\n",
    "    X_val_advanced['roe'] = X_val_advanced['Wynik_netto'] / (X_val_advanced['Kapital_wlasny'] + 1)\n",
    "    X_test_advanced['roe'] = X_test_advanced['Wynik_netto'] / (X_test_advanced['Kapital_wlasny'] + 1)\n",
    "    created_features.append('roe')\n",
    "    print(f\"   ‚úì ROE (zwrot z kapita≈Çu)\")\n",
    "\n",
    "# Mar≈ºa zysku\n",
    "if 'Wynik_netto' in main_features and 'Przychody_netto_ze_sprzedazy' in main_features:\n",
    "    X_train_advanced['profit_margin'] = X_train_advanced['Wynik_netto'] / (X_train_advanced['Przychody_netto_ze_sprzedazy'] + 1)\n",
    "    X_val_advanced['profit_margin'] = X_val_advanced['Wynik_netto'] / (X_val_advanced['Przychody_netto_ze_sprzedazy'] + 1)\n",
    "    X_test_advanced['profit_margin'] = X_test_advanced['Wynik_netto'] / (X_test_advanced['Przychody_netto_ze_sprzedazy'] + 1)\n",
    "    created_features.append('profit_margin')\n",
    "    print(f\"   ‚úì profit_margin (mar≈ºa zysku)\")\n",
    "\n",
    "# ============================================\n",
    "# 3. WSKA≈πNIKI EFEKTYWNO≈öCI\n",
    "# ============================================\n",
    "print(\"\\n3Ô∏è‚É£  Wska≈∫niki efektywno≈õci operacyjnej\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Rotacja aktyw√≥w\n",
    "if 'Przychody_netto_ze_sprzedazy' in main_features and 'Aktywa' in main_features:\n",
    "    X_train_advanced['asset_turnover'] = X_train_advanced['Przychody_netto_ze_sprzedazy'] / (X_train_advanced['Aktywa'] + 1)\n",
    "    X_val_advanced['asset_turnover'] = X_val_advanced['Przychody_netto_ze_sprzedazy'] / (X_val_advanced['Aktywa'] + 1)\n",
    "    X_test_advanced['asset_turnover'] = X_test_advanced['Przychody_netto_ze_sprzedazy'] / (X_test_advanced['Aktywa'] + 1)\n",
    "    created_features.append('asset_turnover')\n",
    "    print(f\"   ‚úì asset_turnover (rotacja aktyw√≥w)\")\n",
    "\n",
    "# Rotacja zapas√≥w\n",
    "if 'Koszty_sprzedanych_produktow' in main_features and 'Zapasy' in main_features:\n",
    "    X_train_advanced['inventory_turnover'] = X_train_advanced['Koszty_sprzedanych_produktow'] / (X_train_advanced['Zapasy'] + 1)\n",
    "    X_val_advanced['inventory_turnover'] = X_val_advanced['Koszty_sprzedanych_produktow'] / (X_val_advanced['Zapasy'] + 1)\n",
    "    X_test_advanced['inventory_turnover'] = X_test_advanced['Koszty_sprzedanych_produktow'] / (X_test_advanced['Zapasy'] + 1)\n",
    "    created_features.append('inventory_turnover')\n",
    "    print(f\"   ‚úì inventory_turnover (rotacja zapas√≥w)\")\n",
    "\n",
    "# ============================================\n",
    "# 4. STRUKTURA KAPITA≈ÅOWA\n",
    "# ============================================\n",
    "print(\"\\n4Ô∏è‚É£  Wska≈∫niki struktury kapita≈Çowej\")\n",
    "print(\"-\"*80)\n",
    "if 'Kapital_wlasny' in main_features and 'Aktywa' in main_features:\n",
    "    X_train_advanced['equity_ratio'] = X_train_advanced['Kapital_wlasny'] / (X_train_advanced['Aktywa'] + 1)\n",
    "    X_val_advanced['equity_ratio'] = X_val_advanced['Kapital_wlasny'] / (X_val_advanced['Aktywa'] + 1)\n",
    "    X_test_advanced['equity_ratio'] = X_test_advanced['Kapital_wlasny'] / (X_test_advanced['Aktywa'] + 1)\n",
    "    created_features.append('equity_ratio')\n",
    "    print(f\"   ‚úì equity_ratio (udzia≈Ç kapita≈Çu w≈Çasnego)\")\n",
    "\n",
    "# Leverage (d≈∫wignia finansowa)\n",
    "if 'Aktywa' in main_features and 'Kapital_wlasny' in main_features:\n",
    "    X_train_advanced['leverage'] = X_train_advanced['Aktywa'] / (X_train_advanced['Kapital_wlasny'] + 1)\n",
    "    X_val_advanced['leverage'] = X_val_advanced['Aktywa'] / (X_val_advanced['Kapital_wlasny'] + 1)\n",
    "    X_test_advanced['leverage'] = X_test_advanced['Aktywa'] / (X_test_advanced['Kapital_wlasny'] + 1)\n",
    "    created_features.append('leverage')\n",
    "    print(f\"   ‚úì leverage (d≈∫wignia finansowa)\")\n",
    "\n",
    "# ============================================\n",
    "# 5. INTERAKCJE I WSKA≈πNIKI Z≈ÅO≈ªONE\n",
    "# ============================================\n",
    "print(\"\\n5Ô∏è‚É£  Wska≈∫niki z≈Ço≈ºone i interakcje\")\n",
    "print(\"-\"*80)\n",
    "if 'Aktywa_obrotowe' in main_features and 'Zobowiazania_krotkoterminowe' in main_features:\n",
    "    X_train_advanced['working_capital'] = X_train_advanced['Aktywa_obrotowe'] - X_train_advanced['Zobowiazania_krotkoterminowe']\n",
    "    X_val_advanced['working_capital'] = X_val_advanced['Aktywa_obrotowe'] - X_val_advanced['Zobowiazania_krotkoterminowe']\n",
    "    X_test_advanced['working_capital'] = X_test_advanced['Aktywa_obrotowe'] - X_test_advanced['Zobowiazania_krotkoterminowe']\n",
    "    created_features.append('working_capital')\n",
    "    print(f\"   ‚úì working_capital (kapita≈Ç obrotowy)\")\n",
    "\n",
    "# Wska≈∫nik wielko≈õci firmy (log assets)\n",
    "if 'Aktywa' in main_features:\n",
    "    X_train_advanced['log_assets'] = np.log1p(X_train_advanced['Aktywa'])\n",
    "    X_val_advanced['log_assets'] = np.log1p(X_val_advanced['Aktywa'])\n",
    "    X_test_advanced['log_assets'] = np.log1p(X_test_advanced['Aktywa'])\n",
    "    created_features.append('log_assets')\n",
    "    print(f\"   ‚úì log_assets (logarytm aktyw√≥w)\")\n",
    "\n",
    "# Wiek firmy (je≈õli jest dostƒôpny)\n",
    "if 'wsk_liczba_dni_istnienia' in main_features:\n",
    "    X_train_advanced['company_age_years'] = X_train_advanced['wsk_liczba_dni_istnienia'] / 365.25\n",
    "    X_val_advanced['company_age_years'] = X_val_advanced['wsk_liczba_dni_istnienia'] / 365.25\n",
    "    X_test_advanced['company_age_years'] = X_test_advanced['wsk_liczba_dni_istnienia'] / 365.25\n",
    "    created_features.append('company_age_years')\n",
    "    print(f\"   ‚úì company_age_years (wiek firmy w latach)\")\n",
    "\n",
    "# ============================================\n",
    "# PODSUMOWANIE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "new_features = set(X_train_advanced.columns) - set(X_train_full.columns)\n",
    "print(f\"‚úÖ Utworzono {len(new_features)} nowych zmiennych domenowych:\")\n",
    "for feat in sorted(new_features):\n",
    "    print(f\"   ‚Ä¢ {feat}\")\n",
    "\n",
    "print(f\"\\nüìä Nowy wymiar danych:\")\n",
    "print(f\"   Train: {X_train_advanced.shape}\")\n",
    "print(f\"   Val:   {X_val_advanced.shape}\")\n",
    "print(f\"   Test:  {X_test_advanced.shape}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635f273",
   "metadata": {},
   "source": [
    "## 8.2 Variance Inflation Factor (VIF) - Usuwanie Wielokolinearno≈õci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Funkcja do obliczania VIF\n",
    "def calculate_vif(df):\n",
    "    \"\"\"Oblicza VIF dla wszystkich zmiennych numerycznych\"\"\"\n",
    "    vif_data = []\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    for i, col in enumerate(numeric_df.columns):\n",
    "        try:\n",
    "            vif = variance_inflation_factor(numeric_df.values, i)\n",
    "            vif_data.append({'feature': col, 'VIF': vif})\n",
    "        except:\n",
    "            vif_data.append({'feature': col, 'VIF': np.inf})\n",
    "    \n",
    "    return pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "\n",
    "# Iteracyjne usuwanie zmiennych z wysokim VIF\n",
    "def remove_high_vif_features(X, threshold=10.0, max_iterations=20):\n",
    "    \"\"\"Iteracyjnie usuwa zmienne z VIF > threshold (standard bankowy)\"\"\"\n",
    "    X_clean = X.copy()\n",
    "    removed_features = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        vif_df = calculate_vif(X_clean)\n",
    "        high_vif = vif_df[vif_df['VIF'] > threshold]\n",
    "        \n",
    "        if len(high_vif) == 0:\n",
    "            break\n",
    "        \n",
    "        worst_feature = high_vif.iloc[0]['feature']\n",
    "        worst_vif = high_vif.iloc[0]['VIF']\n",
    "        X_clean = X_clean.drop(columns=[worst_feature])\n",
    "        removed_features.append((worst_feature, worst_vif))\n",
    "    \n",
    "    return X_clean, removed_features, vif_df\n",
    "\n",
    "# Zastosuj VIF cleaning\n",
    "print(\"üìä VIF ANALYSIS - Usuwanie wielokolinearno≈õci (VIF > 10)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train_vif, removed_vif, final_vif = remove_high_vif_features(X_train_advanced, threshold=10.0)\n",
    "X_val_vif = X_val_advanced[X_train_vif.columns]  # DODANO\n",
    "X_test_vif = X_test_advanced[X_train_vif.columns]\n",
    "\n",
    "# Podsumowanie\n",
    "print(f\"‚úÖ Usuniƒôto {len(removed_vif)} zmiennych z wysokƒÖ wielokolinearno≈õƒá\")\n",
    "print(f\"üìä Pozosta≈Ço: Train {X_train_vif.shape[1]}, Val {X_val_vif.shape[1]}, Test {X_test_vif.shape[1]} zmiennych (by≈Ço {X_train_advanced.shape[1]})\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e0b82",
   "metadata": {},
   "source": [
    "## 8.3 Correlation Clustering - Wyb√≥r Reprezentatywnych Zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb43130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "print(\"üîó CORRELATION CLUSTERING - Grupowanie skorelowanych zmiennych (r > 0.8)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Oblicz macierz korelacji\n",
    "corr_matrix = X_train_vif.corr().abs()\n",
    "\n",
    "# Usu≈Ñ NaN z macierzy korelacji (zastƒÖp zerem - brak korelacji)\n",
    "corr_matrix = corr_matrix.fillna(0)\n",
    "\n",
    "# Clustering hierarchiczny\n",
    "distance_matrix = 1 - corr_matrix.values\n",
    "# Wymu≈õ dok≈ÇadnƒÖ symetriƒô\n",
    "distance_matrix = np.maximum(distance_matrix, distance_matrix.T)\n",
    "np.fill_diagonal(distance_matrix, 0)\n",
    "\n",
    "# Usu≈Ñ inf/nan warto≈õci (zamie≈Ñ na maksymalnƒÖ odleg≈Ço≈õƒá = 1)\n",
    "distance_matrix = np.nan_to_num(distance_matrix, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "\n",
    "# ZaokrƒÖglij aby usunƒÖƒá b≈Çƒôdy numeryczne\n",
    "distance_matrix = np.round(distance_matrix, decimals=10)\n",
    "condensed_dist = squareform(distance_matrix, checks=False)\n",
    "linkage_matrix = hierarchy.linkage(condensed_dist, method='average')\n",
    "clusters = hierarchy.fcluster(linkage_matrix, t=0.2, criterion='distance')\n",
    "\n",
    "# Przypisz zmienne do klastr√≥w\n",
    "cluster_dict = {}\n",
    "for feature, cluster_id in zip(corr_matrix.columns, clusters):\n",
    "    if cluster_id not in cluster_dict:\n",
    "        cluster_dict[cluster_id] = []\n",
    "    cluster_dict[cluster_id].append(feature)\n",
    "\n",
    "# Funkcja do obliczania IV\n",
    "def calculate_iv_for_selection(X, y, feature, bins=10):\n",
    "    \"\"\"Szybka kalkulacja IV dla selekcji zmiennych\"\"\"\n",
    "    try:\n",
    "        df_temp = pd.DataFrame({'feature': X[feature], 'target': y}).dropna()\n",
    "        \n",
    "        try:\n",
    "            df_temp['bin'] = pd.qcut(df_temp['feature'], q=bins, duplicates='drop')\n",
    "        except:\n",
    "            try:\n",
    "                df_temp['bin'] = pd.cut(df_temp['feature'], bins=bins, duplicates='drop')\n",
    "            except:\n",
    "                return 0.0\n",
    "        \n",
    "        grouped = df_temp.groupby('bin', observed=True)['target'].agg(['sum', 'count'])\n",
    "        grouped.columns = ['bad', 'total']\n",
    "        grouped['good'] = grouped['total'] - grouped['bad']\n",
    "        \n",
    "        total_good = grouped['good'].sum()\n",
    "        total_bad = grouped['bad'].sum()\n",
    "        \n",
    "        if total_good == 0 or total_bad == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        grouped['good_dist'] = grouped['good'] / total_good\n",
    "        grouped['bad_dist'] = grouped['bad'] / total_bad\n",
    "        grouped['woe'] = np.log((grouped['good_dist'] + 0.0001) / (grouped['bad_dist'] + 0.0001))\n",
    "        grouped['iv'] = (grouped['good_dist'] - grouped['bad_dist']) * grouped['woe']\n",
    "        \n",
    "        return grouped['iv'].sum()\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Oblicz IV dla wszystkich zmiennych\n",
    "iv_values = {feature: calculate_iv_for_selection(X_train_vif, y_train, feature) for feature in X_train_vif.columns}\n",
    "\n",
    "# Wybierz najlepszƒÖ zmiennƒÖ z ka≈ºdego klastra (wed≈Çug IV)\n",
    "selected_features = []\n",
    "removed_by_clustering = []\n",
    "\n",
    "for cluster_id, features in cluster_dict.items():\n",
    "    if len(features) == 1:\n",
    "        selected_features.append(features[0])\n",
    "    else:\n",
    "        cluster_ivs = sorted([(feat, iv_values.get(feat, 0)) for feat in features], key=lambda x: x[1], reverse=True)\n",
    "        selected_features.append(cluster_ivs[0][0])\n",
    "        \n",
    "        for feat, iv in cluster_ivs[1:]:\n",
    "            removed_by_clustering.append((feat, iv, cluster_ivs[0][0], cluster_ivs[0][1]))\n",
    "\n",
    "# Zastosuj selekcjƒô\n",
    "X_train_clustered = X_train_vif[selected_features]\n",
    "X_val_clustered = X_val_vif[selected_features]  # DODANO\n",
    "X_test_clustered = X_test_vif[selected_features]\n",
    "\n",
    "# Podsumowanie\n",
    "print(f\"‚úÖ Usuniƒôto {len(removed_by_clustering)} redundantnych zmiennych\")\n",
    "print(f\"üìä Pozosta≈Ço: Train {X_train_clustered.shape[1]}, Val {X_val_clustered.shape[1]}, Test {X_test_clustered.shape[1]} zmiennych (by≈Ço {X_train_vif.shape[1]})\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457acba0",
   "metadata": {},
   "source": [
    "## 8.4 WoE Transformation & Final Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9dfacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä WOE TRANSFORMATION & FINAL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Stosujemy bankowƒÖ transformacjƒô WoE i wybieramy top features wed≈Çug IV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# 1. Oblicz pe≈ÇnƒÖ tabelƒô IV dla pozosta≈Çych zmiennych\n",
    "# ============================================\n",
    "print(\"\\n1Ô∏è‚É£  Obliczanie Information Value dla wszystkich zmiennych...\")\n",
    "\n",
    "iv_results_advanced = []\n",
    "for feature in X_train_clustered.columns:\n",
    "    iv_val = calculate_iv_for_selection(X_train_clustered, y_train, feature, bins=10)\n",
    "    iv_results_advanced.append({'feature': feature, 'IV': iv_val})\n",
    "\n",
    "iv_df_advanced = pd.DataFrame(iv_results_advanced).sort_values('IV', ascending=False)\n",
    "\n",
    "# Kategoryzacja IV (standard bankowy)\n",
    "def categorize_iv(iv):\n",
    "    if iv < 0.02:\n",
    "        return \"‚ùå Unpredictive\"\n",
    "    elif iv < 0.1:\n",
    "        return \"‚ö†Ô∏è  Weak\"\n",
    "    elif iv < 0.3:\n",
    "        return \"‚úì Medium\"\n",
    "    elif iv < 0.5:\n",
    "        return \"‚úì‚úì Strong\"\n",
    "    else:\n",
    "        return \"‚úì‚úì‚úì Very Strong\"\n",
    "\n",
    "iv_df_advanced['category'] = iv_df_advanced['IV'].apply(categorize_iv)\n",
    "\n",
    "print(f\"   ‚úì Obliczono IV dla {len(iv_df_advanced)} zmiennych\")\n",
    "print(f\"\\nüìä Rozk≈Çad mocy predykcyjnej:\")\n",
    "print(iv_df_advanced['category'].value_counts().to_string())\n",
    "\n",
    "# ============================================\n",
    "# 2. Wyb√≥r top features wed≈Çug IV\n",
    "# ============================================\n",
    "print(\"\\n2Ô∏è‚É£  Wyb√≥r najlepszych zmiennych wed≈Çug IV...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Wybierz zmienne z IV > 0.02 (minimum predictive power)\n",
    "iv_df_filtered = iv_df_advanced[iv_df_advanced['IV'] >= 0.02].copy()\n",
    "\n",
    "# Wybierz top 30 zmiennych (standard w modelach bankowych)\n",
    "n_features_final = min(30, len(iv_df_filtered))\n",
    "top_features_advanced = iv_df_filtered.head(n_features_final)['feature'].tolist()\n",
    "\n",
    "print(f\"‚úì Wybrano {n_features_final} zmiennych z IV ‚â• 0.02\")\n",
    "print(f\"\\nüèÜ Top 15 zmiennych wed≈Çug IV:\")\n",
    "print(\"-\"*80)\n",
    "for idx, row in iv_df_advanced.head(15).iterrows():\n",
    "    print(f\"   {idx+1:2d}. {row['feature']:<40} IV={row['IV']:.4f}  {row['category']}\")\n",
    "\n",
    "# ============================================\n",
    "# 3. Przygotowanie finalnych danych\n",
    "# ============================================\n",
    "print(\"\\n3Ô∏è‚É£  Przygotowanie finalnego datasetu...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Wybierz top features\n",
    "X_train_advanced_raw = X_train_clustered[top_features_advanced].copy()\n",
    "X_val_advanced_raw = X_val_clustered[top_features_advanced].copy()  # DODANO\n",
    "X_test_advanced_raw = X_test_clustered[top_features_advanced].copy()\n",
    "\n",
    "print(f\"   ‚úì Finalne dane: Train {X_train_advanced_raw.shape[1]}, Val {X_val_advanced_raw.shape[1]}, Test {X_test_advanced_raw.shape[1]} zmiennych (engineered features)\")\n",
    "\n",
    "# ============================================\n",
    "# PODSUMOWANIE CA≈ÅEGO PROCESU\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ FEATURE ENGINEERING & SELECTION - PODSUMOWANIE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìå Krok 1 - Feature Engineering:    {X_train_full.shape[1]} ‚Üí {X_train_advanced.shape[1]} zmiennych (+{X_train_advanced.shape[1]-X_train_full.shape[1]} nowych)\")\n",
    "print(f\"üìå Krok 2 - VIF Cleaning (>10):     {X_train_advanced.shape[1]} ‚Üí {X_train_vif.shape[1]} zmiennych (-{len(removed_vif)} usuniƒôtych)\")\n",
    "print(f\"üìå Krok 3 - Correlation Clustering: {X_train_vif.shape[1]} ‚Üí {X_train_clustered.shape[1]} zmiennych (-{len(removed_by_clustering)} redundantnych)\")\n",
    "print(f\"üìå Krok 4 - IV Selection (top 30):  {X_train_clustered.shape[1]} ‚Üí {n_features_final} zmiennych\")\n",
    "\n",
    "print(f\"\\nüéØ Finalne datasety dla modeli:\")\n",
    "print(f\"   ‚Ä¢ X_train_advanced_raw:  {X_train_advanced_raw.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val_advanced_raw:    {X_val_advanced_raw.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test_advanced_raw:   {X_test_advanced_raw.shape}\")\n",
    "\n",
    "print(\"\\nüí° Te datasety zawierajƒÖ:\")\n",
    "print(\"   ‚úì Zmienne domenowe (wska≈∫niki finansowe)\")\n",
    "print(\"   ‚úì Brak wielokolinearno≈õci (VIF < 10)\")\n",
    "print(\"   ‚úì Brak redundancji (jeden reprezentant z ka≈ºdego klastra)\")\n",
    "print(\"   ‚úì WysokƒÖ moc predykcyjnƒÖ (IV ‚â• 0.02)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e8d8a",
   "metadata": {},
   "source": [
    "## 9. Trening Modeli - Advanced Features (RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1eab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ MODELE KONKURENCYJNE - Wersja RAW (z class balancing)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# S≈Çownik na wyniki\n",
    "results_advanced_raw = {}\n",
    "\n",
    "# 1. LOGISTIC REGRESSION + BALANCED\n",
    "lr_advanced_raw = LogisticRegression(max_iter=1000, random_state=1, class_weight='balanced')\n",
    "lr_advanced_raw.fit(X_train_advanced_raw, y_train)\n",
    "y_pred_lr_adv_raw = lr_advanced_raw.predict(X_test_advanced_raw)\n",
    "y_proba_lr_adv_raw = lr_advanced_raw.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "\n",
    "metrics_lr_adv = calculate_all_metrics(y_test, y_pred_lr_adv_raw, y_proba_lr_adv_raw)\n",
    "results_advanced_raw['LR'] = metrics_lr_adv\n",
    "\n",
    "# 2. DECISION TREE + BALANCED\n",
    "dt_advanced_raw = DecisionTreeClassifier(max_depth=10, random_state=1, class_weight='balanced')\n",
    "dt_advanced_raw.fit(X_train_advanced_raw, y_train)\n",
    "y_pred_dt_adv_raw = dt_advanced_raw.predict(X_test_advanced_raw)\n",
    "y_proba_dt_adv_raw = dt_advanced_raw.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "\n",
    "metrics_dt_adv = calculate_all_metrics(y_test, y_pred_dt_adv_raw, y_proba_dt_adv_raw)\n",
    "results_advanced_raw['DT'] = metrics_dt_adv\n",
    "\n",
    "# 3. RANDOM FOREST + BALANCED\n",
    "rf_advanced_raw = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1, class_weight='balanced')\n",
    "rf_advanced_raw.fit(X_train_advanced_raw, y_train)\n",
    "y_pred_rf_adv_raw = rf_advanced_raw.predict(X_test_advanced_raw)\n",
    "y_proba_rf_adv_raw = rf_advanced_raw.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "\n",
    "metrics_rf_adv = calculate_all_metrics(y_test, y_pred_rf_adv_raw, y_proba_rf_adv_raw)\n",
    "results_advanced_raw['RF'] = metrics_rf_adv\n",
    "\n",
    "# 4. NAIVE BAYES (z SMOTE)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=1)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_advanced_raw, y_train)\n",
    "\n",
    "nb_advanced_raw = GaussianNB()\n",
    "nb_advanced_raw.fit(X_train_smote, y_train_smote)\n",
    "y_pred_nb_adv_raw = nb_advanced_raw.predict(X_test_advanced_raw)\n",
    "y_proba_nb_adv_raw = nb_advanced_raw.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "\n",
    "metrics_nb_adv = calculate_all_metrics(y_test, y_pred_nb_adv_raw, y_proba_nb_adv_raw)\n",
    "results_advanced_raw['NB'] = metrics_nb_adv\n",
    "\n",
    "# 5. XGBoost + scale_pos_weight\n",
    "if XGBOOST_AVAILABLE:\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    scale_pos_weight_adv = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    xgb_advanced_raw = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight_adv,\n",
    "        random_state=1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_advanced_raw.fit(X_train_advanced_raw, y_train)\n",
    "    y_pred_xgb_adv_raw = xgb_advanced_raw.predict(X_test_advanced_raw)\n",
    "    y_proba_xgb_adv_raw = xgb_advanced_raw.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "    \n",
    "    metrics_xgb_adv = calculate_all_metrics(y_test, y_pred_xgb_adv_raw, y_proba_xgb_adv_raw)\n",
    "    results_advanced_raw['XGB'] = metrics_xgb_adv\n",
    "\n",
    "# PODSUMOWANIE WYNIK√ìW\n",
    "print(\"\\nüìä WYNIKI MODELI (Advanced RAW + Class Balancing):\")\n",
    "print(\"=\"*80)\n",
    "for model_name, metrics in results_advanced_raw.items():\n",
    "    print_model_metrics(metrics, f\"{model_name} - Advanced RAW\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881e3bb",
   "metadata": {},
   "source": [
    "## 9.5 Hipertuning Modeli Black-Box z Bayesian Optimization\n",
    "\n",
    "Zastosowanie zaawansowanej optymalizacji hiperparametr√≥w dla modeli ensemble (XGBoost, LightGBM, Random Forest) u≈ºywajƒÖc Bayesian Optimization, kt√≥ra jest bardziej efektywna ni≈º Grid Search czy Random Search.\n",
    "\n",
    "**Metoda:** BayesSearchCV z scikit-optimize\n",
    "**Zalety:**\n",
    "- Inteligentne przeszukiwanie przestrzeni hiperparametr√≥w\n",
    "- Mniej iteracji potrzebnych ni≈º Grid Search\n",
    "- Wykorzystanie poprzednich wynik√≥w do wyboru kolejnych punkt√≥w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a0761",
   "metadata": {},
   "source": [
    "## 8.5 Analiza Korelacji - Advanced Pipeline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä ANALIZA KORELACJI - Advanced Pipeline Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. KORELACJA MIƒòDZY CECHAMI (Multicollinearity Check)\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£  KORELACJA MIƒòDZY CECHAMI - Macierz korelacji\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Oblicz macierz korelacji\n",
    "corr_matrix_adv = X_train_advanced_raw.corr()\n",
    "\n",
    "print(f\"\\nüìä Macierz korelacji: {corr_matrix_adv.shape[0]} √ó {corr_matrix_adv.shape[1]}\")\n",
    "print(f\"üìã Zakres warto≈õci: [{corr_matrix_adv.min().min():.3f}, {corr_matrix_adv.max().max():.3f}]\")\n",
    "\n",
    "# Znajd≈∫ pary cech o wysokiej korelacji (>0.7)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix_adv.columns)):\n",
    "    for j in range(i+1, len(corr_matrix_adv.columns)):\n",
    "        corr_val = corr_matrix_adv.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix_adv.columns[i],\n",
    "                corr_matrix_adv.columns[j],\n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Znaleziono {len(high_corr_pairs)} par cech o wysokiej korelacji (|r| > 0.7):\")\n",
    "    for feat1, feat2, corr_val in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:15]:\n",
    "        direction = \"+\" if corr_val > 0 else \"-\"\n",
    "        print(f\"   {direction} {feat1:<35} ‚Üî {feat2:<35} r = {corr_val:+.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Brak par cech o wysokiej korelacji (|r| > 0.7)\")\n",
    "    print(\"   VIF cleaning i correlation clustering skutecznie usunƒô≈Çy wielokolinearno≈õƒá!\")\n",
    "\n",
    "# ≈örednia korelacja (bez diagonali)\n",
    "corr_values = corr_matrix_adv.values[np.triu_indices_from(corr_matrix_adv.values, k=1)]\n",
    "print(f\"\\nüìä Statystyki korelacji (wszystkie pary cech):\")\n",
    "print(f\"   ≈örednia |r|:  {np.abs(corr_values).mean():.3f}\")\n",
    "print(f\"   Mediana |r|:  {np.median(np.abs(corr_values)):.3f}\")\n",
    "print(f\"   Max |r|:      {np.abs(corr_values).max():.3f}\")\n",
    "\n",
    "# Wizualizacja macierzy korelacji\n",
    "print(f\"\\nüìä Wizualizacja macierzy korelacji (heatmap):\")\n",
    "\n",
    "# Import seaborn je≈õli nie zaimportowany\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  Instalujƒô seaborn...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seaborn\", \"-q\"])\n",
    "    import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(corr_matrix_adv, \n",
    "            annot=False,  # Nie pokazuj warto≈õci (za du≈ºo cech)\n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            vmin=-1, \n",
    "            vmax=1,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"})\n",
    "plt.title('Macierz Korelacji - Advanced Pipeline Dataset (30 cech)', fontsize=14, pad=20)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ‚úÖ Heatmap wygenerowana - ciemne czerwone/niebieskie = wysoka korelacja\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. KORELACJA Z TARGETEM (Predictive Power)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2Ô∏è‚É£  KORELACJA Z TARGETEM - Si≈Ça zale≈ºno≈õci z ryzykiem defaultu\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Oblicz korelacjƒô Pearsona z targetem\n",
    "target_corr = []\n",
    "for col in X_train_advanced_raw.columns:\n",
    "    try:\n",
    "        corr_val = X_train_advanced_raw[col].corr(pd.Series(y_train.values))\n",
    "        target_corr.append({\n",
    "            'Feature': col,\n",
    "            'Correlation': corr_val,\n",
    "            'Abs_Correlation': abs(corr_val)\n",
    "        })\n",
    "    except:\n",
    "        target_corr.append({\n",
    "            'Feature': col,\n",
    "            'Correlation': 0.0,\n",
    "            'Abs_Correlation': 0.0\n",
    "        })\n",
    "\n",
    "target_corr_df = pd.DataFrame(target_corr).sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "print(f\"\\nüìà TOP 15 CECH najbardziej skorelowanych z ryzykiem defaultu:\")\n",
    "print(f\"{'Rank':<6} {'Feature':<45} {'Correlation':>12} {'Direction':<15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in target_corr_df.head(15).iterrows():\n",
    "    rank = target_corr_df.index.get_loc(idx) + 1\n",
    "    direction = \"üìà Higher = More Risk\" if row['Correlation'] > 0 else \"üìâ Higher = Less Risk\"\n",
    "    print(f\"{rank:<6} {row['Feature']:<45} {row['Correlation']:>+12.4f} {direction:<15}\")\n",
    "\n",
    "print(f\"\\nüìä Statystyki korelacji z targetem:\")\n",
    "print(f\"   ≈örednia |r|:  {target_corr_df['Abs_Correlation'].mean():.4f}\")\n",
    "print(f\"   Mediana |r|:  {target_corr_df['Abs_Correlation'].median():.4f}\")\n",
    "print(f\"   Max |r|:      {target_corr_df['Abs_Correlation'].max():.4f}\")\n",
    "print(f\"   Min |r|:      {target_corr_df['Abs_Correlation'].min():.4f}\")\n",
    "\n",
    "# Cechy o bardzo niskiej korelacji z targetem (<0.05)\n",
    "weak_corr = target_corr_df[target_corr_df['Abs_Correlation'] < 0.05]\n",
    "if len(weak_corr) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Cechy o bardzo niskiej korelacji z targetem (|r| < 0.05): {len(weak_corr)}\")\n",
    "    print(f\"   (MogƒÖ mieƒá nieliniowƒÖ zale≈ºno≈õƒá - WoE/IV poka≈ºe prawdziwƒÖ moc)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PODSUMOWANIE\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä PODSUMOWANIE - Analiza Korelacji\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ KORELACJA MIƒòDZY CECHAMI:\")\n",
    "print(f\"   ‚Ä¢ Pary o wysokiej korelacji (|r| > 0.7): {len(high_corr_pairs)}\")\n",
    "print(f\"   ‚Ä¢ ≈örednia |r| miƒôdzy cechami: {np.abs(corr_values).mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ Status: {'‚úÖ Niska wielokolinearno≈õƒá' if len(high_corr_pairs) < 5 else '‚ö†Ô∏è Mo≈ºliwa wielokolinearno≈õƒá'}\")\n",
    "\n",
    "print(f\"\\n‚úÖ KORELACJA Z TARGETEM:\")\n",
    "print(f\"   ‚Ä¢ Najsilniejsza korelacja: {target_corr_df.iloc[0]['Feature']} (r = {target_corr_df.iloc[0]['Correlation']:+.4f})\")\n",
    "print(f\"   ‚Ä¢ ≈örednia |r| z targetem: {target_corr_df['Abs_Correlation'].mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Cechy o |r| > 0.1: {(target_corr_df['Abs_Correlation'] > 0.1).sum()}/{len(target_corr_df)}\")\n",
    "\n",
    "print(f\"\\nüí° WNIOSKI:\")\n",
    "if len(high_corr_pairs) == 0:\n",
    "    print(f\"   ‚úÖ Dataset jest gotowy - brak wielokolinearno≈õci\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è {len(high_corr_pairs)} par o wysokiej korelacji - rozwa≈º dodatkowe VIF cleaning\")\n",
    "\n",
    "if target_corr_df['Abs_Correlation'].mean() > 0.1:\n",
    "    print(f\"   ‚úÖ Cechy majƒÖ silnƒÖ zale≈ºno≈õƒá z targetem - dobry potencja≈Ç predykcyjny\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è S≈Çaba liniowa korelacja - sprawd≈∫ zale≈ºno≈õci nieliniowe (WoE/IV)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1de5f",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning - Grid Search\n",
    "\n",
    "Optymalizacja parametr√≥w dla obu scorecard√≥w (Basic i Advanced) poprzez Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c9aa4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèóÔ∏è ARCHITEKTURA SCORECARD - Przep≈Çyw Danych\n",
    "\n",
    "## üìã Wyja≈õnienie: Dlaczego NIE MA Pipeline dla Scorecard?\n",
    "\n",
    "### ‚ùì Pytanie: \"Czy powinien byƒá pipeline jak w innych modelach?\"\n",
    "\n",
    "**Odpowied≈∫: NIE - i to jest celowy design!**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç POR√ìWNANIE: Pipeline vs Scorecard Flow\n",
    "\n",
    "### üü¢ **FULL PIPELINE** (Sekcje 1-9)\n",
    "\n",
    "Encapsulates ALL transformations w jednym obiekcie. WoE jest **fixed** (np. 5 bin√≥w dla wszystkich cech). Pipeline jest **reusable**.\n",
    "\n",
    "### üîµ **BASIC SCORECARD** (Sekcje 10-11) - BRAK pipeline!\n",
    "\n",
    "**KROK 1: Grid Search (Sekcja 10)** - wybiera cechy + optymalne biny PER FEATURE  \n",
    "**KROK 2: WoE Transformation (Sekcja 11)** - RƒòCZNA transformacja u≈ºywajƒÖc wynik√≥w  \n",
    "**KROK 3: Scorecard Training** - LogisticRegression na WoE features\n",
    "\n",
    "**Dlaczego rƒôcznie?** Ka≈ºda cecha ma **innƒÖ liczbƒô bin√≥w** (wynik Grid Search).\n",
    "\n",
    "### üü£ **ADVANCED SCORECARD** (Sekcja 12)\n",
    "\n",
    "**Dok≈Çadnie ten sam flow!** Grid Search ‚Üí Manual WoE ‚Üí Training  \n",
    "R√≥≈ºnica: Input to `X_train_advanced_raw` (30 cech z feature engineering) vs `X_train_full` (raw)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ PODSUMOWANIE\n",
    "\n",
    "1. **NIE MA b≈Çƒôdu** - brak pipeline to celowy design\n",
    "2. **Grid Search optymalizuje per-feature bins** - nie da siƒô w pipeline\n",
    "3. **WoE jest robione rƒôcznie** w Sekcji 11/12 u≈ºywajƒÖc optimal bins\n",
    "4. **Advanced dzia≈Ça TAK SAMO** - tylko inne dane wej≈õciowe\n",
    "5. **To jest POPRAWNE** - best practices credit scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f574a2b",
   "metadata": {},
   "source": [
    "### 10.1 Przygotowanie Danych Walidacyjnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c652d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PRZYGOTOWANIE DANYCH WALIDACYJNYCH DLA GRID SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sprawd≈∫ czy dane walidacyjne zosta≈Çy utworzone w sekcjach 10.1-10.4\n",
    "if 'X_val_full' not in globals():\n",
    "    print(\"\\nüîß Tworzƒô X_val_full (pipeline_full)...\")\n",
    "    X_val_full = pipeline_full.transform(X_val)\n",
    "else:\n",
    "    print(f\"\\n‚úÖ X_val_full ju≈º istnieje: {X_val_full.shape}\")\n",
    "\n",
    "if 'X_val_minimal' not in globals():\n",
    "    print(\"üîß Tworzƒô X_val_minimal (pipeline_minimal)...\")\n",
    "    X_val_minimal = pipeline_minimal.transform(X_val)\n",
    "else:\n",
    "    print(f\"‚úÖ X_val_minimal ju≈º istnieje: {X_val_minimal.shape}\")\n",
    "\n",
    "if 'X_val_advanced_raw' not in globals():\n",
    "    print(\"‚ö†Ô∏è  UWAGA: X_val_advanced_raw nie istnieje!\")\n",
    "    print(\"   Musisz uruchomiƒá sekcje 10.1-10.4 (Feature Engineering) aby utworzyƒá X_val_advanced_raw\")\n",
    "    print(\"   U≈ºywam X_val_full jako fallback...\")\n",
    "    X_val_advanced_raw = X_val_full.copy()\n",
    "else:\n",
    "    print(f\"‚úÖ X_val_advanced_raw ju≈º istnieje: {X_val_advanced_raw.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Podsumowanie danych walidacyjnych:\")\n",
    "print(f\"   X_val_full:         {X_val_full.shape}\")\n",
    "print(f\"   X_val_minimal:      {X_val_minimal.shape}\")\n",
    "print(f\"   X_val_advanced_raw: {X_val_advanced_raw.shape}\")\n",
    "print(f\"   y_val:              {y_val.shape}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f436232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING - CREDIT SCORECARDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import precision_recall_curve, auc, log_loss, brier_score_loss\n",
    "import time\n",
    "from itertools import product, combinations\n",
    "import random\n",
    "\n",
    "# ============================================\n",
    "# FUNKCJE POMOCNICZE\n",
    "# ============================================\n",
    "def monotonicity_score(woe_table):\n",
    "    \"\"\"Oblicza score monotoniczno≈õci WoE (0-100%)\"\"\"\n",
    "    woe_values = woe_table['woe'].values\n",
    "    if len(woe_values) < 2:\n",
    "        return 100.0\n",
    "    \n",
    "    diffs = np.diff(woe_values)\n",
    "    n_positive = np.sum(diffs > 0)\n",
    "    n_negative = np.sum(diffs < 0)\n",
    "    n_zero = np.sum(diffs == 0)\n",
    "    total_changes = len(diffs)\n",
    "    \n",
    "    if total_changes == 0:\n",
    "        return 100.0\n",
    "    \n",
    "    monotonic_count = max(n_positive, n_negative) + n_zero\n",
    "    return (monotonic_count / total_changes) * 100\n",
    "\n",
    "def calculate_ks_statistic(y_true, y_pred_proba):\n",
    "    \"\"\"Oblicza statystykƒô Ko≈Çmogorowa-Smirnova\"\"\"\n",
    "    df = pd.DataFrame({'true': y_true, 'pred': y_pred_proba})\n",
    "    df = df.sort_values('pred', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    df['cumulative_bad'] = df['true'].cumsum() / df['true'].sum()\n",
    "    df['cumulative_good'] = (1 - df['true']).cumsum() / (1 - df['true']).sum()\n",
    "    \n",
    "    ks = (df['cumulative_bad'] - df['cumulative_good']).abs().max()\n",
    "    return ks\n",
    "\n",
    "def calculate_all_metrics(y_true, y_pred_proba):\n",
    "    \"\"\"Oblicza wszystkie metryki jako≈õci modelu\"\"\"\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    ks = calculate_ks_statistic(y_true, y_pred_proba)\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'ks': ks,\n",
    "        'log_loss': logloss,\n",
    "        'brier': brier\n",
    "    }\n",
    "\n",
    "def calculate_feature_bins_info(X_data, y_data, feature, bin_options=[3, 4, 5, 6, 7, 8, 10, 12, 15, 20], min_mono=70.0):\n",
    "    \"\"\"\n",
    "    Dla danej cechy oblicza IV i mono dla wszystkich opcji bin√≥w.\n",
    "    Zwraca listƒô s≈Çownik√≥w z info o ka≈ºdej opcji binowania.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for n_bins in bin_options:\n",
    "        try:\n",
    "            df_temp = pd.DataFrame({feature: X_data[feature], 'target': y_data.values})\n",
    "            woe_table, iv_value = calculate_woe_iv(df_temp, feature, 'target', bins=n_bins)\n",
    "            mono = monotonicity_score(woe_table)\n",
    "            \n",
    "            if mono >= min_mono:\n",
    "                results.append({\n",
    "                    'bins': n_bins,\n",
    "                    'iv': iv_value,\n",
    "                    'mono': mono\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_feature_bins_info_no_mono(X_data, y_data, feature, bin_options=[3, 4, 5, 6, 7, 8, 10, 12, 15, 20]):\n",
    "    \"\"\"\n",
    "    Dla danej cechy oblicza IV dla wszystkich opcji bin√≥w BEZ wymaga≈Ñ monotonicznych.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for n_bins in bin_options:\n",
    "        try:\n",
    "            df_temp = pd.DataFrame({feature: X_data[feature], 'target': y_data.values})\n",
    "            woe_table, iv_value = calculate_woe_iv(df_temp, feature, 'target', bins=n_bins)\n",
    "            mono = monotonicity_score(woe_table)\n",
    "            \n",
    "            results.append({\n",
    "                'bins': n_bins,\n",
    "                'iv': iv_value,\n",
    "                'mono': mono  # Zapisujemy, ale nie filtrujemy\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "# PARAMETRY\n",
    "# ============================================\n",
    "param_grid = {\n",
    "    'n_features': [10, 15, 18, 20],\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "BIN_OPTIONS = [3, 4, 5, 6, 7, 8, 10, 12, 15, 20]\n",
    "MIN_MONO_PER_FEATURE = 70.0\n",
    "MIN_AVG_MONO = 80.0\n",
    "\n",
    "print(f\"\\n‚ö° KRYTERIA:\")\n",
    "print(f\"   ‚Ä¢ Model MONOTONICZNY: ka≈ºda cecha mono >= {MIN_MONO_PER_FEATURE}%, ≈õrednia >= {MIN_AVG_MONO}%\")\n",
    "print(f\"   ‚Ä¢ Model BEZ MONO: brak wymaga≈Ñ monotonicznych (baseline)\")\n",
    "print(f\"   ‚Ä¢ Testuje ~35 strategii binowania dla ka≈ºdej kombinacji parametr√≥w\")\n",
    "print(f\"   ‚Ä¢ Biny testowane: {BIN_OPTIONS}\")\n",
    "print(f\"   ‚Ä¢ To zajmie ~10-15 minut...\")\n",
    "\n",
    "# ============================================\n",
    "# 1Ô∏è‚É£ BASIC PIPELINE - Z MONOTONICZO≈öCIƒÑ\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1Ô∏è‚É£  BASIC PIPELINE SCORECARD (Z WYMAGANIAMI MONOTONICZNYMI)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "numeric_features = X_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"   Wszystkich cech numerycznych: {len(numeric_features)}\")\n",
    "\n",
    "# KROK 1: Analiza z wymaganiami mono\n",
    "print(f\"\\nüîß KROK 1: Analiza opcji binowania (mono >= {MIN_MONO_PER_FEATURE}%)...\")\n",
    "feature_bin_options = {}\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(f\"      Przetworzono {i+1}/{len(numeric_features)} cech...\")\n",
    "    \n",
    "    bins_info = calculate_feature_bins_info(X_train_full, y_train, feature, BIN_OPTIONS, MIN_MONO_PER_FEATURE)\n",
    "    if len(bins_info) > 0:\n",
    "        feature_bin_options[feature] = bins_info\n",
    "\n",
    "print(f\"\\n   ‚úÖ {len(feature_bin_options)} cech ma opcje z mono >= {MIN_MONO_PER_FEATURE}%\")\n",
    "print(f\"   ‚ùå Odrzucono {len(numeric_features) - len(feature_bin_options)} cech\")\n",
    "\n",
    "feature_best_iv = {}\n",
    "for feat, options in feature_bin_options.items():\n",
    "    best = max(options, key=lambda x: x['iv'])\n",
    "    feature_best_iv[feat] = best['iv']\n",
    "\n",
    "features_sorted_by_iv = sorted(feature_best_iv.keys(), key=lambda f: feature_best_iv[f], reverse=True)\n",
    "\n",
    "print(f\"\\nüìä Top 20 cech wed≈Çug najlepszego IV:\")\n",
    "for i, feat in enumerate(features_sorted_by_iv[:20]):\n",
    "    best_option = max(feature_bin_options[feat], key=lambda x: x['iv'])\n",
    "    print(f\"   {i+1:2}. {feat:<45} IV={best_option['iv']:.4f} (bins={best_option['bins']}, mono={best_option['mono']:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüîÑ KROK 2: Grid Search (z mono)...\\n\")\n",
    "\n",
    "best_score_basic = 0\n",
    "best_params_basic = None\n",
    "best_features_basic = None\n",
    "results_grid_basic = []\n",
    "configs_tested = 0\n",
    "configs_successful = 0\n",
    "total_combinations = len(list(ParameterGrid(param_grid)))\n",
    "\n",
    "for param_idx, params in enumerate(ParameterGrid(param_grid)):\n",
    "    n_features = params['n_features']\n",
    "    \n",
    "    selected_features = []\n",
    "    for feat in features_sorted_by_iv:\n",
    "        if feat in feature_bin_options:\n",
    "            selected_features.append(feat)\n",
    "            if len(selected_features) == n_features:\n",
    "                break\n",
    "    \n",
    "    if len(selected_features) < n_features:\n",
    "        continue\n",
    "    \n",
    "    configs_to_test = []\n",
    "    \n",
    "    # Podstawowe konfiguracje\n",
    "    config_max_iv = [(feat, max(feature_bin_options[feat], key=lambda x: x['iv'])) for feat in selected_features]\n",
    "    configs_to_test.append(config_max_iv)\n",
    "    \n",
    "    config_max_mono = [(feat, max(feature_bin_options[feat], key=lambda x: x['mono'])) for feat in selected_features]\n",
    "    configs_to_test.append(config_max_mono)\n",
    "    \n",
    "    config_balanced = []\n",
    "    for feat in selected_features:\n",
    "        options = feature_bin_options[feat]\n",
    "        for opt in options:\n",
    "            opt['score'] = opt['iv'] * (opt['mono'] / 100.0)\n",
    "        config_balanced.append((feat, max(options, key=lambda x: x['score'])))\n",
    "    configs_to_test.append(config_balanced)\n",
    "    \n",
    "    config_min_bins = [(feat, min(feature_bin_options[feat], key=lambda x: x['bins'])) for feat in selected_features]\n",
    "    configs_to_test.append(config_min_bins)\n",
    "    \n",
    "    config_max_bins = [(feat, max(feature_bin_options[feat], key=lambda x: x['bins'])) for feat in selected_features]\n",
    "    configs_to_test.append(config_max_bins)\n",
    "    \n",
    "    # Losowe (10)\n",
    "    for _ in range(10):\n",
    "        config_random = [(feat, random.choice(feature_bin_options[feat])) for feat in selected_features]\n",
    "        configs_to_test.append(config_random)\n",
    "    \n",
    "    # Miksy (10)\n",
    "    for mix_ratio in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        split_point = int(len(selected_features) * mix_ratio)\n",
    "        config_mix = []\n",
    "        for i, feat in enumerate(selected_features):\n",
    "            opt = max(feature_bin_options[feat], key=lambda x: x['iv']) if i < split_point else max(feature_bin_options[feat], key=lambda x: x['mono'])\n",
    "            config_mix.append((feat, opt))\n",
    "        configs_to_test.append(config_mix)\n",
    "    \n",
    "    # Weighted (5)\n",
    "    for iv_weight in [0.3, 0.5, 0.7, 0.85, 0.95]:\n",
    "        mono_weight = 1.0 - iv_weight\n",
    "        config_weighted = []\n",
    "        for feat in selected_features:\n",
    "            options = feature_bin_options[feat]\n",
    "            for opt in options:\n",
    "                opt['weighted_score'] = (opt['iv'] * iv_weight) + ((opt['mono'] / 100.0) * mono_weight)\n",
    "            config_weighted.append((feat, max(options, key=lambda x: x['weighted_score'])))\n",
    "        configs_to_test.append(config_weighted)\n",
    "    \n",
    "    # Target bins (5)\n",
    "    for target_bins in [5, 7, 10, 12, 15]:\n",
    "        config_target_bins = []\n",
    "        for feat in selected_features:\n",
    "            options = feature_bin_options[feat]\n",
    "            closest_options = sorted(options, key=lambda x: abs(x['bins'] - target_bins))[:3]\n",
    "            config_target_bins.append((feat, max(closest_options, key=lambda x: x['iv']) if closest_options else max(options, key=lambda x: x['iv'])))\n",
    "        configs_to_test.append(config_target_bins)\n",
    "    \n",
    "    for config in configs_to_test:\n",
    "        configs_tested += 1\n",
    "        \n",
    "        avg_mono = np.mean([info['mono'] for _, info in config])\n",
    "        \n",
    "        if avg_mono < MIN_AVG_MONO:\n",
    "            for reduce_to in range(len(config)-1, 0, -1):\n",
    "                config_reduced = config[:reduce_to]\n",
    "                avg_mono = np.mean([info['mono'] for _, info in config_reduced])\n",
    "                if avg_mono >= MIN_AVG_MONO:\n",
    "                    config = config_reduced\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        if len(config) == 0:\n",
    "            continue\n",
    "        \n",
    "        min_mono = min([info['mono'] for _, info in config])\n",
    "        actual_n_features = len(config)\n",
    "        \n",
    "        try:\n",
    "            X_train_woe = pd.DataFrame()\n",
    "            woe_mappings = {}\n",
    "            \n",
    "            for feat, info in config:\n",
    "                n_bins = info['bins']\n",
    "                df_temp = pd.DataFrame({feat: X_train_full[feat], 'target': y_train.values})\n",
    "                woe_values = woe_transform(df_temp, feat, 'target', bins=n_bins)\n",
    "                X_train_woe[f\"{feat}_woe\"] = pd.to_numeric(woe_values, errors='coerce')\n",
    "                woe_table, _ = calculate_woe_iv(df_temp, feat, 'target', bins=n_bins)\n",
    "                woe_mappings[feat] = {'table': woe_table, 'bins': n_bins}\n",
    "            \n",
    "            if X_train_woe.shape[1] == 0:\n",
    "                continue\n",
    "            \n",
    "            X_val_woe = pd.DataFrame()\n",
    "            for feat, info in config:\n",
    "                if feat in woe_mappings:\n",
    "                    n_bins = woe_mappings[feat]['bins']\n",
    "                    df_temp = pd.DataFrame({feat: X_val_full[feat], 'target': y_val.values})\n",
    "                    woe_values = woe_transform(df_temp, feat, 'target', bins=n_bins)\n",
    "                    X_val_woe[f\"{feat}_woe\"] = pd.to_numeric(woe_values, errors='coerce')\n",
    "            \n",
    "            model = LogisticRegression(\n",
    "                C=params['C'], solver=params['solver'],\n",
    "                max_iter=1000, random_state=42, class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_train_woe, y_train)\n",
    "            \n",
    "            y_proba_val = model.predict_proba(X_val_woe)[:, 1]\n",
    "            metrics = calculate_all_metrics(y_val, y_proba_val)\n",
    "            \n",
    "            configs_successful += 1\n",
    "            results_grid_basic.append({\n",
    "                'n_features': actual_n_features, \n",
    "                'C': params['C'], \n",
    "                'solver': params['solver'],\n",
    "                'roc_auc': metrics['roc_auc'],\n",
    "                'pr_auc': metrics['pr_auc'],\n",
    "                'ks': metrics['ks'],\n",
    "                'log_loss': metrics['log_loss'],\n",
    "                'brier': metrics['brier'],\n",
    "                'avg_mono': avg_mono, \n",
    "                'min_mono': min_mono\n",
    "            })\n",
    "            \n",
    "            if metrics['roc_auc'] > best_score_basic:\n",
    "                best_score_basic = metrics['roc_auc']\n",
    "                best_params_basic = {\n",
    "                    **params, \n",
    "                    'n_features': actual_n_features, \n",
    "                    'avg_mono': avg_mono, \n",
    "                    'min_mono': min_mono,\n",
    "                    **metrics\n",
    "                }\n",
    "                best_features_basic = config.copy()\n",
    "            \n",
    "            if configs_tested % 20 == 0:\n",
    "                print(f\"   [{param_idx+1}/{total_combinations}] n_feat={n_features}, C={params['C']}, solver={params['solver'][:4]} | \"\n",
    "                      f\"Tested: {configs_tested}, Best AUC: {best_score_basic:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "elapsed_basic = time.time() - start_time\n",
    "print(f\"\\n   ‚úÖ Grid Search (mono) zako≈Ñczony: {elapsed_basic:.1f}s, Sukces: {configs_successful}/{configs_tested}\")\n",
    "\n",
    "# ============================================\n",
    "# 2Ô∏è‚É£ BASIC PIPELINE - BEZ MONOTONICZNO≈öCI (BASELINE)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2Ô∏è‚É£  BASIC PIPELINE - BEZ WYMAGA≈É MONOTONICZNYCH (BASELINE)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "start_time_no_mono = time.time()\n",
    "\n",
    "# Analiza WSZYSTKICH cech bez filtrowania po mono\n",
    "print(f\"\\nüîß Analiza opcji binowania (BEZ wymaga≈Ñ mono)...\")\n",
    "feature_bin_options_no_mono = {}\n",
    "for feature in numeric_features:\n",
    "    bins_info = calculate_feature_bins_info_no_mono(X_train_full, y_train, feature, BIN_OPTIONS)\n",
    "    if len(bins_info) > 0:\n",
    "        feature_bin_options_no_mono[feature] = bins_info\n",
    "\n",
    "print(f\"   ‚úÖ {len(feature_bin_options_no_mono)} cech z opcjami binowania\")\n",
    "\n",
    "# Sortuj wed≈Çug IV (bez ogranicze≈Ñ mono)\n",
    "feature_best_iv_no_mono = {}\n",
    "for feat, options in feature_bin_options_no_mono.items():\n",
    "    best = max(options, key=lambda x: x['iv'])\n",
    "    feature_best_iv_no_mono[feat] = best['iv']\n",
    "\n",
    "features_sorted_by_iv_no_mono = sorted(feature_best_iv_no_mono.keys(), key=lambda f: feature_best_iv_no_mono[f], reverse=True)\n",
    "\n",
    "print(f\"\\nüîÑ Grid Search (bez mono)...\")\n",
    "\n",
    "best_score_no_mono = 0\n",
    "best_params_no_mono = None\n",
    "best_features_no_mono = None\n",
    "configs_tested_no_mono = 0\n",
    "configs_successful_no_mono = 0\n",
    "\n",
    "# Uproszczony grid search - tylko 1 konfiguracja per parametry (max IV)\n",
    "for param_idx, params in enumerate(ParameterGrid(param_grid)):\n",
    "    n_features = params['n_features']\n",
    "    \n",
    "    selected_features = features_sorted_by_iv_no_mono[:n_features]\n",
    "    \n",
    "    if len(selected_features) < n_features:\n",
    "        continue\n",
    "    \n",
    "    # Tylko max IV config\n",
    "    config = [(feat, max(feature_bin_options_no_mono[feat], key=lambda x: x['iv'])) for feat in selected_features]\n",
    "    \n",
    "    configs_tested_no_mono += 1\n",
    "    avg_mono = np.mean([info['mono'] for _, info in config])\n",
    "    min_mono = min([info['mono'] for _, info in config])\n",
    "    \n",
    "    try:\n",
    "        X_train_woe_nm = pd.DataFrame()\n",
    "        woe_mappings_nm = {}\n",
    "        \n",
    "        for feat, info in config:\n",
    "            n_bins = info['bins']\n",
    "            df_temp = pd.DataFrame({feat: X_train_full[feat], 'target': y_train.values})\n",
    "            woe_values = woe_transform(df_temp, feat, 'target', bins=n_bins)\n",
    "            X_train_woe_nm[f\"{feat}_woe\"] = pd.to_numeric(woe_values, errors='coerce')\n",
    "            woe_table, _ = calculate_woe_iv(df_temp, feat, 'target', bins=n_bins)\n",
    "            woe_mappings_nm[feat] = {'table': woe_table, 'bins': n_bins}\n",
    "        \n",
    "        X_val_woe_nm = pd.DataFrame()\n",
    "        for feat, info in config:\n",
    "            if feat in woe_mappings_nm:\n",
    "                n_bins = woe_mappings_nm[feat]['bins']\n",
    "                df_temp = pd.DataFrame({feat: X_val_full[feat], 'target': y_val.values})\n",
    "                woe_values = woe_transform(df_temp, feat, 'target', bins=n_bins)\n",
    "                X_val_woe_nm[f\"{feat}_woe\"] = pd.to_numeric(woe_values, errors='coerce')\n",
    "        \n",
    "        model_nm = LogisticRegression(\n",
    "            C=params['C'], solver=params['solver'],\n",
    "            max_iter=1000, random_state=42, class_weight='balanced'\n",
    "        )\n",
    "        model_nm.fit(X_train_woe_nm, y_train)\n",
    "        \n",
    "        y_proba_val_nm = model_nm.predict_proba(X_val_woe_nm)[:, 1]\n",
    "        metrics_nm = calculate_all_metrics(y_val, y_proba_val_nm)\n",
    "        \n",
    "        configs_successful_no_mono += 1\n",
    "        \n",
    "        if metrics_nm['roc_auc'] > best_score_no_mono:\n",
    "            best_score_no_mono = metrics_nm['roc_auc']\n",
    "            best_params_no_mono = {\n",
    "                **params,\n",
    "                'n_features': n_features,\n",
    "                'avg_mono': avg_mono,\n",
    "                'min_mono': min_mono,\n",
    "                **metrics_nm\n",
    "            }\n",
    "            best_features_no_mono = config.copy()\n",
    "        \n",
    "        if configs_tested_no_mono % 5 == 0:\n",
    "            print(f\"   [{param_idx+1}/{total_combinations}] Best AUC (no mono): {best_score_no_mono:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "elapsed_no_mono = time.time() - start_time_no_mono\n",
    "print(f\"\\n   ‚úÖ Grid Search (no mono) zako≈Ñczony: {elapsed_no_mono:.1f}s, Sukces: {configs_successful_no_mono}/{configs_tested_no_mono}\")\n",
    "\n",
    "# ============================================\n",
    "# POR√ìWNANIE: MONOTONICZNY vs BEZ MONO\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä POR√ìWNANIE: MODEL Z MONO vs BEZ MONO (BASIC)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_params_basic and best_params_no_mono:\n",
    "    comparison_data = []\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': 'Z monotoniczo≈õciƒÖ',\n",
    "        'N_cech': best_params_basic['n_features'],\n",
    "        'ROC-AUC': best_params_basic['roc_auc'],\n",
    "        'PR-AUC': best_params_basic['pr_auc'],\n",
    "        'KS': best_params_basic['ks'],\n",
    "        'Log Loss': best_params_basic['log_loss'],\n",
    "        'Brier': best_params_basic['brier'],\n",
    "        'Avg Mono': best_params_basic['avg_mono'],\n",
    "        'Min Mono': best_params_basic['min_mono']\n",
    "    })\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': 'BEZ monotoniczno≈õci',\n",
    "        'N_cech': best_params_no_mono['n_features'],\n",
    "        'ROC-AUC': best_params_no_mono['roc_auc'],\n",
    "        'PR-AUC': best_params_no_mono['pr_auc'],\n",
    "        'KS': best_params_no_mono['ks'],\n",
    "        'Log Loss': best_params_no_mono['log_loss'],\n",
    "        'Brier': best_params_no_mono['brier'],\n",
    "        'Avg Mono': best_params_no_mono['avg_mono'],\n",
    "        'Min Mono': best_params_no_mono['min_mono']\n",
    "    })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    \n",
    "    # R√≥≈ºnice\n",
    "    auc_diff = best_params_no_mono['roc_auc'] - best_params_basic['roc_auc']\n",
    "    ks_diff = best_params_no_mono['ks'] - best_params_basic['ks']\n",
    "    \n",
    "    print(f\"\\nüí° WNIOSKI:\")\n",
    "    print(f\"   ‚Ä¢ R√≥≈ºnica AUC: {auc_diff:+.4f} ({auc_diff/best_params_basic['roc_auc']*100:+.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ R√≥≈ºnica KS:  {ks_diff:+.4f}\")\n",
    "    \n",
    "    if auc_diff > 0.01:\n",
    "        print(f\"   ‚ö†Ô∏è  Model BEZ mono ma znaczƒÖco lepsze AUC (+{auc_diff:.4f})\")\n",
    "        print(f\"       ale gorzƒÖ interpretowalno≈õƒá (avg mono: {best_params_no_mono['avg_mono']:.1f}%)\")\n",
    "    elif auc_diff < -0.01:\n",
    "        print(f\"   ‚úÖ Model Z mono ma lepsze AUC przy zachowaniu interpretowalno≈õci!\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Modele majƒÖ podobne AUC - op≈Çaca siƒô u≈ºyƒá wersji Z mono\")\n",
    "    \n",
    "    print(f\"\\n   üìã Cechy BEZ mono (top {min(10, len(best_features_no_mono))}):\")\n",
    "    for i, (feat, info) in enumerate(best_features_no_mono[:10], 1):\n",
    "        mono_icon = \"‚úì\" if info['mono'] >= 70 else \"‚úó\"\n",
    "        print(f\"   {i:2}. {feat:<40} IV={info['iv']:.4f}, Mono={info['mono']:.1f}% {mono_icon}\")\n",
    "\n",
    "# Wyniki z mono\n",
    "if configs_successful > 0:\n",
    "    results_df = pd.DataFrame(results_grid_basic).sort_values('roc_auc', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüèÜ NAJLEPSZA KONFIGURACJA (Z MONO):\")\n",
    "    print(f\"   Cechy: {best_params_basic['n_features']}\")\n",
    "    print(f\"   ROC-AUC: {best_params_basic['roc_auc']:.4f}, KS: {best_params_basic['ks']:.4f}\")\n",
    "    print(f\"   Mono: Avg={best_params_basic['avg_mono']:.1f}%, Min={best_params_basic['min_mono']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n   üìã Wybrane cechy ({len(best_features_basic)}):\")\n",
    "    for i, (feat, info) in enumerate(best_features_basic, 1):\n",
    "        print(f\"   {i:2}. {feat:<45} IV={info['iv']:.4f}, Mono={info['mono']:.1f}%, Bins={info['bins']}\")\n",
    "\n",
    "# Zapisz opcje\n",
    "feature_bin_options_basic_saved = feature_bin_options\n",
    "if best_features_basic:\n",
    "    feature_bins_map = {feat: info['bins'] for feat, info in best_features_basic}\n",
    "\n",
    "print(f\"\\nüíæ Zapisano opcje binowania i najlepsze modele\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df9b2c",
   "metadata": {},
   "source": [
    "## 11. Basic Scorecard (Optimized)\n",
    "\n",
    "Scorecard dla Full Pipeline (bez feature engineering) z optymalnymi parametrami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7747ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BASIC PIPELINE CREDIT SCORECARD (OPTIMIZED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sprawd≈∫ czy Grid Search siƒô wykona≈Ç\n",
    "if 'best_params_basic' not in globals() or best_params_basic is None:\n",
    "    print(\"\\n‚ö†Ô∏è  UWAGA: Grid Search nie zosta≈Ç wykonany. U≈ºywam domy≈õlnych parametr√≥w.\")\n",
    "    best_params_basic = {'n_features': 18, 'C': 1.0, 'solver': 'liblinear'}\n",
    "    best_features_basic = None\n",
    "\n",
    "# U≈ºyj optymalnych parametr√≥w z Grid Search\n",
    "print(f\"\\nüéØ OPTYMALNE PARAMETRY Z GRID SEARCH:\")\n",
    "print(f\"   Liczba cech: {best_params_basic['n_features']}\")\n",
    "print(f\"   C: {best_params_basic['C']}\")\n",
    "print(f\"   Solver: {best_params_basic['solver']}\")\n",
    "if 'roc_auc' in best_params_basic:\n",
    "    print(f\"\\n   üìà Metryki walidacyjne:\")\n",
    "    print(f\"   ROC-AUC: {best_params_basic['roc_auc']:.4f}\")\n",
    "    print(f\"   KS: {best_params_basic.get('ks', 0):.4f}\")\n",
    "if 'avg_mono' in best_params_basic:\n",
    "    print(f\"\\n   üìä Monotoniczno≈õƒá:\")\n",
    "    print(f\"   ≈örednia: {best_params_basic['avg_mono']:.1f}%\")\n",
    "    print(f\"   Min: {best_params_basic['min_mono']:.1f}%\")\n",
    "\n",
    "# U≈ºyj cech z Grid Search (z optymalnymi binami)\n",
    "if best_features_basic is not None:\n",
    "    print(f\"\\nüìã WYBRANE CECHY ({len(best_features_basic)}):\")\n",
    "    for i, (feat, info) in enumerate(best_features_basic[:10], 1):\n",
    "        print(f\"   {i:2}. {feat:<40} IV={info['iv']:.4f}, Mono={info['mono']:5.1f}%, Bins={info['bins']}\")\n",
    "    if len(best_features_basic) > 10:\n",
    "        print(f\"   ... i {len(best_features_basic)-10} wiƒôcej\")\n",
    "    \n",
    "    # Przygotuj listƒô cech i ich bin√≥w\n",
    "    top_features_basic = [feat for feat, _ in best_features_basic]\n",
    "    feature_bins_map = {feat: info['bins'] for feat, info in best_features_basic}\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Brak wynik√≥w z Grid Search - pr√≥bujƒô u≈ºyƒá zapisanych danych...\")\n",
    "    \n",
    "    # Sprawd≈∫ czy sƒÖ zapisane opcje binowania\n",
    "    if 'feature_bin_options_basic_saved' in globals():\n",
    "        # U≈ºyj zapisanych opcji\n",
    "        feature_bin_options_local = feature_bin_options_basic_saved\n",
    "        \n",
    "        # Dla ka≈ºdej cechy we≈∫ opcjƒô z najwy≈ºszym IV\n",
    "        feature_best_iv = {}\n",
    "        for feat, options in feature_bin_options_local.items():\n",
    "            best = max(options, key=lambda x: x['iv'])\n",
    "            feature_best_iv[feat] = best\n",
    "        \n",
    "        # Sortuj wed≈Çug IV\n",
    "        features_sorted = sorted(feature_best_iv.items(), key=lambda x: x[1]['iv'], reverse=True)\n",
    "        \n",
    "        # We≈∫ top N\n",
    "        n_feats = best_params_basic['n_features']\n",
    "        top_features_basic = [feat for feat, _ in features_sorted[:n_feats]]\n",
    "        feature_bins_map = {feat: info['bins'] for feat, info in features_sorted[:n_feats]}\n",
    "        \n",
    "        print(f\"   ‚úÖ Znaleziono {len(top_features_basic)} cech z zapisanych danych\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Brak zapisanych danych - u≈ºywam domy≈õlnych bin√≥w (10)\")\n",
    "        numeric_features_basic = X_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        top_features_basic = numeric_features_basic[:best_params_basic['n_features']]\n",
    "        feature_bins_map = {f: 10 for f in top_features_basic}\n",
    "\n",
    "# WoE Transformation (u≈ºyj optymalnych bin√≥w dla ka≈ºdej cechy)\n",
    "print(f\"\\nüîÑ WoE Transformation z optymalnymi binami...\")\n",
    "print(f\"   ‚ö†Ô∏è  WA≈ªNE: Grid search u≈ºywa≈Ç val do optymalizacji, teraz trenujemy na train+val\")\n",
    "\n",
    "# Kombinuj train + val dla finalnego treningu (po znalezieniu najlepszych parametr√≥w)\n",
    "X_train_val_full = pd.concat([X_train_full, X_val_full], axis=0)\n",
    "y_train_val = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "print(f\"   Train+Val: {X_train_val_full.shape[0]} obs (train={X_train_full.shape[0]}, val={X_val_full.shape[0]})\")\n",
    "print(f\"   Test:      {X_test_full.shape[0]} obs\")\n",
    "\n",
    "X_train_woe_basic = pd.DataFrame()\n",
    "X_test_woe_basic = pd.DataFrame()\n",
    "woe_mappings_basic = {}\n",
    "\n",
    "for feature in top_features_basic:\n",
    "    try:\n",
    "        n_bins = feature_bins_map.get(feature, 10)\n",
    "        \n",
    "        # Train+Val combined (FINALNE TRENOWANIE)\n",
    "        df_temp_train = pd.DataFrame({\n",
    "            feature: X_train_val_full[feature],\n",
    "            'target': y_train_val.values\n",
    "        })\n",
    "        woe_values_train = woe_transform(df_temp_train, feature, 'target', bins=n_bins)\n",
    "        X_train_woe_basic[f\"{feature}_woe\"] = pd.to_numeric(woe_values_train, errors='coerce')\n",
    "        \n",
    "        # Zapisz mapping\n",
    "        woe_table, _ = calculate_woe_iv(df_temp_train, feature, 'target', bins=n_bins)\n",
    "        woe_mappings_basic[feature] = {'table': woe_table, 'bins': n_bins}\n",
    "        \n",
    "        # Test (NIE DOTYKAJ - tylko test set)\n",
    "        df_temp_test = pd.DataFrame({\n",
    "            feature: X_test_full[feature],\n",
    "            'target': y_test.values\n",
    "        })\n",
    "        woe_values_test = woe_transform(df_temp_test, feature, 'target', bins=n_bins)\n",
    "        X_test_woe_basic[f\"{feature}_woe\"] = pd.to_numeric(woe_values_test, errors='coerce')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  B≈ÇƒÖd dla {feature}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"   ‚úÖ Przekszta≈Çcono {len(X_train_woe_basic.columns)} cech\")\n",
    "print(f\"   Shape train+val: {X_train_woe_basic.shape} (2400 obs)\")\n",
    "print(f\"   Shape test:      {X_test_woe_basic.shape} (600 obs)\")\n",
    "\n",
    "# Trening Scorecarda z optymalnymi parametrami NA TRAIN+VAL\n",
    "print(f\"\\nüéØ Trening Logistic Regression na train+val combined...\")\n",
    "scorecard_basic = LogisticRegression(\n",
    "    C=best_params_basic['C'],\n",
    "    solver=best_params_basic['solver'],\n",
    "    max_iter=1000, \n",
    "    random_state=42, \n",
    "    class_weight='balanced'\n",
    ")\n",
    "scorecard_basic.fit(X_train_woe_basic, y_train_val)\n",
    "\n",
    "y_pred_sc_basic = scorecard_basic.predict(X_test_woe_basic)\n",
    "y_proba_sc_basic = scorecard_basic.predict_proba(X_test_woe_basic)[:, 1]\n",
    "\n",
    "# Oblicz metryki\n",
    "metrics_sc_basic = calculate_all_metrics(y_test, y_proba_sc_basic)\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL WYTRENOWANY NA TRAIN+VAL (2400 obs)!\")\n",
    "print(f\"\\nüìä WYNIKI NA ZBIORZE TESTOWYM (600 obs):\")\n",
    "print(f\"   ROC-AUC:   {metrics_sc_basic['roc_auc']:.4f}\")\n",
    "print(f\"   PR-AUC:    {metrics_sc_basic['pr_auc']:.4f}\")\n",
    "print(f\"   KS:        {metrics_sc_basic['ks']:.4f}\")\n",
    "print(f\"   Log Loss:  {metrics_sc_basic['log_loss']:.4f}\")\n",
    "print(f\"   Brier:     {metrics_sc_basic['brier']:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Model i dane zapisane jako:\")\n",
    "print(f\"   ‚Ä¢ scorecard_basic - wytrenowany model (train+val)\")\n",
    "print(f\"   ‚Ä¢ X_train_woe_basic, X_test_woe_basic - dane WoE\")\n",
    "print(f\"   ‚Ä¢ woe_mappings_basic - mapowania WoE dla ka≈ºdej cechy\")\n",
    "print(f\"\\n‚ö†Ô∏è  NOTA: Nie dodajemy do results_full (to jest Full Pipeline dict)\")\n",
    "print(f\"   Basic Scorecard to osobny pipeline, nie czƒô≈õƒá Full Pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76b272",
   "metadata": {},
   "source": [
    "### 11.1 EDA - Analiza Jako≈õci WoE (Basic)\n",
    "\n",
    "Analiza transformacji WoE: IV ranking, monotonicity, korelacje, rozk≈Çady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä EDA - ANALIZA JAKO≈öCI WoE (BASIC SCORECARD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IV RANKING - TOP 10 CECH\n",
    "# ============================================================================\n",
    "print(\"\\n\\n1Ô∏è‚É£  IV RANKING - TOP 10 CECH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Zbuduj dataframe z IV z woe_mappings\n",
    "iv_data = []\n",
    "for feat, info in woe_mappings_basic.items():\n",
    "    woe_table = info['table']\n",
    "    iv_total = woe_table['iv'].sum() if 'iv' in woe_table.columns else 0\n",
    "    n_bins = info['bins']\n",
    "    \n",
    "    # Kategoryzacja\n",
    "    if iv_total >= 0.5: power = \"Very Strong\"\n",
    "    elif iv_total >= 0.3: power = \"Strong\"\n",
    "    elif iv_total >= 0.1: power = \"Medium\"\n",
    "    elif iv_total >= 0.02: power = \"Weak\"\n",
    "    else: power = \"Unpredictive\"\n",
    "    \n",
    "    iv_data.append({\n",
    "        'feature': feat,\n",
    "        'IV': iv_total,\n",
    "        'bins': n_bins,\n",
    "        'Power': power\n",
    "    })\n",
    "\n",
    "iv_df_basic = pd.DataFrame(iv_data).sort_values('IV', ascending=False)\n",
    "\n",
    "# Wizualizacja rozk≈Çadu mocy\n",
    "power_counts = iv_df_basic['Power'].value_counts()\n",
    "print(f\"\\nüìä Rozk≈Çad mocy predykcyjnej ({len(iv_df_basic)} cech):\\n\")\n",
    "\n",
    "power_order = [\"Very Strong\", \"Strong\", \"Medium\", \"Weak\", \"Unpredictive\"]\n",
    "for power in power_order:\n",
    "    count = power_counts.get(power, 0)\n",
    "    if count > 0:\n",
    "        bar = \"‚ñà\" * int(count / 2)\n",
    "        print(f\"   {power:15} ({count:2}): {bar}\")\n",
    "\n",
    "# Top 10\n",
    "print(f\"\\nüìà TOP 10 CECH:\")\n",
    "print(f\"\\n   {'Rank':<6} {'Feature':<45} {'IV':<10} {'Bins':<6} {'Power'}\")\n",
    "print(\"   \" + \"-\"*80)\n",
    "\n",
    "for i, row in iv_df_basic.head(10).iterrows():\n",
    "    print(f\"   {i+1:<6} {row['feature']:<45} {row['IV']:<10.4f} {row['bins']:<6} {row['Power']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. WoE TABLES - TOP 5 FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n WoE TABLES - TOP 5 FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top5_basic = iv_df_basic.head(5)['feature'].tolist()\n",
    "\n",
    "for i, feat in enumerate(top5_basic, 1):\n",
    "    if feat not in woe_mappings_basic:\n",
    "        print(f\"\\n‚ö†Ô∏è  {feat}: Brak danych\")\n",
    "        continue\n",
    "    \n",
    "    woe_table = woe_mappings_basic[feat]['table']\n",
    "    n_bins = woe_mappings_basic[feat]['bins']\n",
    "    iv_total = woe_table['iv'].sum() if 'iv' in woe_table.columns else 0\n",
    "    \n",
    "    print(f\"\\n{i}. {feat}\")\n",
    "    print(f\"   IV: {iv_total:.4f}, Bins: {n_bins}\")\n",
    "    print(f\"\\n   {'Bin':<15} {'Count':>8} {'Bad%':>8} {'WoE':>10} {'IV':>10}\")\n",
    "    print(\"   \" + \"-\"*55)\n",
    "    \n",
    "    for _, row in woe_table.iterrows():\n",
    "        bin_label = str(row.get('bin', row.get('range', 'N/A')))[:15]\n",
    "        count = row.get('count', row.get('total', 0))\n",
    "        bad_rate = row.get('bad_rate', row.get('event_rate', 0)) * 100\n",
    "        woe = row.get('woe', 0)\n",
    "        iv = row.get('iv', 0)\n",
    "        print(f\"   {bin_label:<15} {count:>8.0f} {bad_rate:>7.1f}% {woe:>10.3f} {iv:>10.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MACIERZ KORELACJI\n",
    "# ============================================================================\n",
    "print(\"\\n\\n3Ô∏è‚É£  MACIERZ KORELACJI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "corr_woe_basic = X_train_woe_basic.corr()\n",
    "\n",
    "# Znajd≈∫ wysokie korelacje\n",
    "high_corr = []\n",
    "for i in range(len(corr_woe_basic.columns)):\n",
    "    for j in range(i+1, len(corr_woe_basic.columns)):\n",
    "        corr_val = corr_woe_basic.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr.append((corr_woe_basic.columns[i], corr_woe_basic.columns[j], corr_val))\n",
    "\n",
    "if len(high_corr) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Wysokie korelacje (|r| > 0.7): {len(high_corr)}\")\n",
    "    for feat1, feat2, corr_val in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True)[:5]:\n",
    "        f1 = feat1.replace('_woe', '')\n",
    "        f2 = feat2.replace('_woe', '')\n",
    "        print(f\"   {f1} ‚Üî {f2}: {corr_val:+.3f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Brak wysokich korelacji (|r| > 0.7)\")\n",
    "\n",
    "# Heatmap (je≈õli nie za du≈ºo cech)\n",
    "if X_train_woe_basic.shape[1] <= 25:\n",
    "    print(f\"\\nüìä Heatmap macierzy korelacji:\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_woe_basic, \n",
    "                annot=False,\n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                vmin=-1, \n",
    "                vmax=1,\n",
    "                square=True,\n",
    "                cbar_kws={\"label\": \"Correlation\"})\n",
    "    plt.title('Macierz Korelacji - WoE Features (Basic Scorecard)', fontsize=14, pad=20)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"   (Pominiƒôto heatmap - zbyt du≈ºo cech)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ROZK≈ÅADY - TOP 3 CECH\n",
    "# ============================================================================\n",
    "print(\"\\n\\n4Ô∏è‚É£  ROZK≈ÅADY - TOP 3 CECH (RAW vs WoE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, feat in enumerate(iv_df_basic.head(3)['feature'].tolist(), 1):\n",
    "    if feat not in X_train_full.columns or f\"{feat}_woe\" not in X_train_woe_basic.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{i}. {feat}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    fig.suptitle(f'{i}. {feat}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # RAW\n",
    "    axes[0].hist(X_train_full[feat].dropna(), bins=30, alpha=0.7, color='steelblue')\n",
    "    axes[0].set_title('RAW', fontsize=10)\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # WoE\n",
    "    axes[1].hist(X_train_woe_basic[f\"{feat}_woe\"].dropna(), bins=20, alpha=0.7, color='darkgreen')\n",
    "    axes[1].set_title('WoE', fontsize=10)\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. PODSUMOWANIE\n",
    "# ============================================================================\n",
    "print(\"\\n\\n5Ô∏è‚É£  PODSUMOWANIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_features = X_train_woe_basic.shape[1]\n",
    "excellent = (iv_df_basic['IV'] >= 0.3).sum()\n",
    "medium = ((iv_df_basic['IV'] >= 0.1) & (iv_df_basic['IV'] < 0.3)).sum()\n",
    "weak = ((iv_df_basic['IV'] >= 0.02) & (iv_df_basic['IV'] < 0.1)).sum()\n",
    "\n",
    "print(f\"\\nüìä WYBRANYCH CECH: {n_features}\")\n",
    "print(f\"   Very Strong/Strong (IV ‚â• 0.3):  {excellent}\")\n",
    "print(f\"   Medium (0.1 ‚â§ IV < 0.3):        {medium}\")\n",
    "print(f\"   Weak (0.02 ‚â§ IV < 0.1):         {weak}\")\n",
    "\n",
    "# Korelacje\n",
    "if len(high_corr) == 0:\n",
    "    print(f\"\\n‚úÖ WIELOKOLINEARNO≈öƒÜ: OK\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WIELOKOLINEARNO≈öƒÜ: {len(high_corr)} par o |r| > 0.7\")\n",
    "\n",
    "print(\"\\n‚úÖ WoE transformation zako≈Ñczona!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df435311",
   "metadata": {},
   "source": [
    "## 12. Advanced Scorecard (Feature Engineering + VIF)\n",
    "\n",
    "Scorecard dla Advanced Pipeline z feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce078e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ADVANCED SCORECARD (OPTIMIZED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sprawd≈∫ czy Grid Search siƒô wykona≈Ç\n",
    "if 'best_params_adv' not in globals() or best_params_adv is None or 'best_features_adv' not in globals():\n",
    "    print(\"\\n‚ö†Ô∏è  Grid Search nie wykonany - u≈ºywam domy≈õlnych parametr√≥w\")\n",
    "    best_params_adv = {'n_features': 15, 'C': 1.0, 'solver': 'liblinear'}\n",
    "    \n",
    "    # Oblicz IV jako fallback\n",
    "    iv_results = []\n",
    "    for feature in X_train_advanced_raw.columns:\n",
    "        try:\n",
    "            df_temp = pd.DataFrame({feature: X_train_advanced_raw[feature], 'target': y_train.values})\n",
    "            woe_table, iv_value = calculate_woe_iv(df_temp, feature, 'target', bins=10)\n",
    "            mono_score = monotonicity_score(woe_table)\n",
    "            if mono_score >= 66:\n",
    "                iv_results.append({'feature': feature, 'IV': iv_value, 'mono': mono_score, 'bins': 10})\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    iv_df = pd.DataFrame(iv_results).sort_values('IV', ascending=False)\n",
    "    best_features_adv = [(row['feature'], {'bins': row['bins'], 'iv': row['IV'], 'mono': row['mono']}) \n",
    "                         for _, row in iv_df.head(15).iterrows()]\n",
    "\n",
    "# Wy≈õwietl parametry\n",
    "print(f\"\\nüéØ PARAMETRY:\")\n",
    "print(f\"   Cechy:  {best_params_adv['n_features']}\")\n",
    "print(f\"   C:      {best_params_adv['C']}\")\n",
    "print(f\"   Solver: {best_params_adv['solver']}\")\n",
    "if 'avg_mono' in best_params_adv:\n",
    "    print(f\"   Avg Mono: {best_params_adv['avg_mono']:.1f}%\")\n",
    "    print(f\"   Min Mono: {best_params_adv['min_mono']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìã WYBRANE CECHY ({len(best_features_adv)}):\")\n",
    "for i, (feat, info) in enumerate(best_features_adv[:10], 1):\n",
    "    print(f\"   {i:2}. {feat:<40} IV={info['iv']:.4f}, Mono={info['mono']:.1f}%, Bins={info['bins']}\")\n",
    "if len(best_features_adv) > 10:\n",
    "    print(f\"   ... (+{len(best_features_adv)-10} wiƒôcej)\")\n",
    "\n",
    "# WoE Transformation - TRAIN+VAL COMBINED (jak Basic Scorecard)\n",
    "print(f\"\\nüîÑ WoE Transformation z optymalnymi binami...\")\n",
    "print(f\"   ‚ö†Ô∏è  WA≈ªNE: Grid search u≈ºywa≈Ç val do optymalizacji, teraz trenujemy na train+val\")\n",
    "\n",
    "# Kombinuj train + val dla Advanced Pipeline\n",
    "X_train_val_advanced = pd.concat([X_train_advanced_raw, X_val_advanced_raw], axis=0)\n",
    "y_train_val = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "print(f\"   Train+Val: {X_train_val_advanced.shape[0]} obs (train={X_train_advanced_raw.shape[0]}, val={X_val_advanced_raw.shape[0]})\") \n",
    "print(f\"   Test:      {X_test_advanced_raw.shape[0]} obs\")\n",
    "\n",
    "X_train_woe_advanced_sc = pd.DataFrame()\n",
    "woe_mappings_advanced_sc = {}\n",
    "\n",
    "for feat, info in best_features_adv:\n",
    "    try:\n",
    "        n_bins = info['bins']\n",
    "        # Train+Val combined\n",
    "        df_temp = pd.DataFrame({feat: X_train_val_advanced[feat], 'target': y_train_val.values})\n",
    "        woe_values = woe_transform(df_temp, feat, 'target', bins=n_bins)\n",
    "        X_train_woe_advanced_sc[f\"{feat}_woe\"] = pd.to_numeric(woe_values, errors='coerce')\n",
    "        \n",
    "        woe_table, _ = calculate_woe_iv(df_temp, feat, 'target', bins=n_bins)\n",
    "        woe_mappings_advanced_sc[feat] = {'table': woe_table, 'bins': n_bins}\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"   ‚úÖ Przekszta≈Çcono {len(X_train_woe_advanced_sc.columns)} cech\")\n",
    "\n",
    "# WoE Transformation - TEST (nie dotykaj)\n",
    "X_test_woe_advanced_sc = pd.DataFrame()\n",
    "for feat, info in best_features_adv:\n",
    "    if feat in woe_mappings_advanced_sc:\n",
    "        try:\n",
    "            n_bins = woe_mappings_advanced_sc[feat]['bins']\n",
    "            df_temp = pd.DataFrame({feat: X_test_advanced_raw[feat], 'target': y_test.values})\n",
    "            woe_values = woe_transform(df_temp, feat, 'target', bins=n_bins)\n",
    "            X_test_woe_advanced_sc[f\"{feat}_woe\"] = pd.to_numeric(woe_values, errors='coerce')\n",
    "        except:\n",
    "            X_test_woe_advanced_sc[f\"{feat}_woe\"] = 0\n",
    "\n",
    "print(f\"   Shape train+val: {X_train_woe_advanced_sc.shape} (2400 obs)\")\n",
    "print(f\"   Shape test:      {X_test_woe_advanced_sc.shape} (600 obs)\")\n",
    "\n",
    "# Model - trenuj na train+val\n",
    "print(f\"\\nüéØ Trening Logistic Regression na train+val combined...\")\n",
    "scorecard_advanced = LogisticRegression(\n",
    "    C=best_params_adv['C'],\n",
    "    solver=best_params_adv['solver'],\n",
    "    max_iter=1000, \n",
    "    random_state=42, \n",
    "    class_weight='balanced'\n",
    ")\n",
    "scorecard_advanced.fit(X_train_woe_advanced_sc, y_train_val)\n",
    "\n",
    "y_pred_sc_adv = scorecard_advanced.predict(X_test_woe_advanced_sc)\n",
    "y_proba_sc_adv = scorecard_advanced.predict_proba(X_test_woe_advanced_sc)[:, 1]\n",
    "\n",
    "metrics_sc_adv = calculate_all_metrics(y_test, y_proba_sc_adv)\n",
    "\n",
    "print(f\"\\n‚úÖ MODEL WYTRENOWANY NA TRAIN+VAL (2400 obs)!\")\n",
    "print(f\"\\nüìä METRYKI NA ZBIORZE TESTOWYM (600 obs):\")\n",
    "print(f\"   ROC-AUC:   {metrics_sc_adv['roc_auc']:.4f}\")\n",
    "print(f\"   PR-AUC:    {metrics_sc_adv['pr_auc']:.4f}\")\n",
    "print(f\"   KS:        {metrics_sc_adv['ks']:.4f}\")\n",
    "print(f\"   Log Loss:  {metrics_sc_adv['log_loss']:.4f}\")\n",
    "print(f\"   Brier:     {metrics_sc_adv['brier']:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Zapisano:\")\n",
    "print(f\"   ‚Ä¢ scorecard_advanced - wytrenowany model (train+val)\")\n",
    "print(f\"   ‚Ä¢ X_train_woe_advanced_sc, X_test_woe_advanced_sc - dane WoE\")\n",
    "print(f\"   ‚Ä¢ woe_mappings_advanced_sc - mapowania WoE\")\n",
    "print(f\"   ‚Ä¢ metrics_sc_adv - metrics for Advanced Scorecard (separate from raw pipeline models)\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8216e",
   "metadata": {},
   "source": [
    "### 12.1 EDA - Analiza Jako≈õci WoE (Advanced)\n",
    "\n",
    "Analiza WoE dla Advanced Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä EDA - ANALIZA JAKO≈öCI WoE (ADVANCED SCORECARD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sprawd≈∫ czy WoE zosta≈Ço obliczone\n",
    "if 'X_train_woe_advanced_sc' not in globals() or X_train_woe_advanced_sc.shape[1] == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Brak danych WoE - najpierw uruchom Advanced Scorecard!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Dane WoE dostƒôpne: {X_train_woe_advanced_sc.shape[1]} cech\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IV RANKING - TOP 10 CECH\n",
    "# ============================================================================\n",
    "print(\"\\n\\n1Ô∏è‚É£  IV RANKING - TOP 10 CECH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Zbuduj dataframe z IV z woe_mappings\n",
    "if 'woe_mappings_advanced_sc' in globals() and woe_mappings_advanced_sc:\n",
    "    \n",
    "    iv_data = []\n",
    "    for feat, info in woe_mappings_advanced_sc.items():\n",
    "        woe_table = info['table']\n",
    "        iv_total = woe_table['iv'].sum() if 'iv' in woe_table.columns else 0\n",
    "        n_bins = info['bins']\n",
    "        \n",
    "        # Kategoryzacja\n",
    "        if iv_total >= 0.5: power = \"Very Strong\"\n",
    "        elif iv_total >= 0.3: power = \"Strong\"\n",
    "        elif iv_total >= 0.1: power = \"Medium\"\n",
    "        elif iv_total >= 0.02: power = \"Weak\"\n",
    "        else: power = \"Unpredictive\"\n",
    "        \n",
    "        iv_data.append({\n",
    "            'feature': feat,\n",
    "            'IV': iv_total,\n",
    "            'bins': n_bins,\n",
    "            'Power': power\n",
    "        })\n",
    "    \n",
    "    iv_df_adv = pd.DataFrame(iv_data).sort_values('IV', ascending=False)\n",
    "    \n",
    "    # Wizualizacja rozk≈Çadu mocy\n",
    "    power_counts = iv_df_adv['Power'].value_counts()\n",
    "    print(f\"\\nüìä Rozk≈Çad mocy predykcyjnej ({len(iv_df_adv)} cech):\\n\")\n",
    "    \n",
    "    power_order = [\"Very Strong\", \"Strong\", \"Medium\", \"Weak\", \"Unpredictive\"]\n",
    "    for power in power_order:\n",
    "        count = power_counts.get(power, 0)\n",
    "        if count > 0:\n",
    "            bar = \"‚ñà\" * int(count / 2)\n",
    "            print(f\"   {power:15} ({count:2}): {bar}\")\n",
    "    \n",
    "    # Top 10\n",
    "    print(f\"\\nüìà TOP 10 CECH:\")\n",
    "    print(f\"\\n   {'Rank':<6} {'Feature':<45} {'IV':<10} {'Bins':<6} {'Power'}\")\n",
    "    print(\"   \" + \"-\"*80)\n",
    "    \n",
    "    for i, row in iv_df_adv.head(10).iterrows():\n",
    "        print(f\"   {i+1:<6} {row['feature']:<45} {row['IV']:<10.4f} {row['bins']:<6} {row['Power']}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Brak woe_mappings_advanced_sc\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. WoE TABLES - TOP 5 FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2Ô∏è‚É£  WoE TABLES - TOP 5 FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top5_adv = iv_df_adv.head(5)['feature'].tolist()\n",
    "\n",
    "for i, feat in enumerate(top5_adv, 1):\n",
    "    if feat not in woe_mappings_advanced_sc:\n",
    "        print(f\"\\n‚ö†Ô∏è  {feat}: Brak danych\")\n",
    "        continue\n",
    "    \n",
    "    woe_table = woe_mappings_advanced_sc[feat]['table']\n",
    "    n_bins = woe_mappings_advanced_sc[feat]['bins']\n",
    "    iv_total = woe_table['iv'].sum() if 'iv' in woe_table.columns else 0\n",
    "    \n",
    "    print(f\"\\n{i}. {feat}\")\n",
    "    print(f\"   IV: {iv_total:.4f}, Bins: {n_bins}\")\n",
    "    print(f\"\\n   {'Bin':<15} {'Count':>8} {'Bad%':>8} {'WoE':>10} {'IV':>10}\")\n",
    "    print(\"   \" + \"-\"*55)\n",
    "    \n",
    "    for _, row in woe_table.iterrows():\n",
    "        bin_label = str(row.get('bin', row.get('range', 'N/A')))[:15]\n",
    "        count = row.get('count', row.get('total', 0))\n",
    "        bad_rate = row.get('bad_rate', row.get('event_rate', 0)) * 100\n",
    "        woe = row.get('woe', 0)\n",
    "        iv = row.get('iv', 0)\n",
    "        print(f\"   {bin_label:<15} {count:>8.0f} {bad_rate:>7.1f}% {woe:>10.3f} {iv:>10.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MACIERZ KORELACJI\n",
    "# ============================================================================\n",
    "print(\"\\n\\n3Ô∏è‚É£  MACIERZ KORELACJI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "corr_woe_adv = X_train_woe_advanced_sc.corr()\n",
    "\n",
    "# Znajd≈∫ wysokie korelacje\n",
    "high_corr_adv = []\n",
    "for i in range(len(corr_woe_adv.columns)):\n",
    "    for j in range(i+1, len(corr_woe_adv.columns)):\n",
    "        corr_val = corr_woe_adv.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_adv.append((corr_woe_adv.columns[i], corr_woe_adv.columns[j], corr_val))\n",
    "\n",
    "if len(high_corr_adv) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Wysokie korelacje (|r| > 0.7): {len(high_corr_adv)}\")\n",
    "    for feat1, feat2, corr_val in sorted(high_corr_adv, key=lambda x: abs(x[2]), reverse=True)[:5]:\n",
    "        f1 = feat1.replace('_woe', '')\n",
    "        f2 = feat2.replace('_woe', '')\n",
    "        print(f\"   {f1} ‚Üî {f2}: {corr_val:+.3f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Brak wysokich korelacji (|r| > 0.7)\")\n",
    "\n",
    "# Heatmap (je≈õli nie za du≈ºo cech)\n",
    "if X_train_woe_advanced_sc.shape[1] <= 25:\n",
    "    print(f\"\\nüìä Heatmap macierzy korelacji:\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_woe_adv, \n",
    "                annot=False,\n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                vmin=-1, \n",
    "                vmax=1,\n",
    "                square=True,\n",
    "                cbar_kws={\"label\": \"Correlation\"})\n",
    "    plt.title('Macierz Korelacji - WoE Features (Advanced Scorecard)', fontsize=14, pad=20)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"   (Pominiƒôto heatmap - zbyt du≈ºo cech)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ROZK≈ÅADY - TOP 3 CECH\n",
    "# ============================================================================\n",
    "print(\"\\n\\n4Ô∏è‚É£  ROZK≈ÅADY - TOP 3 CECH (RAW vs WoE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, feat in enumerate(iv_df_adv.head(3)['feature'].tolist(), 1):\n",
    "    if feat not in X_train_advanced_raw.columns or f\"{feat}_woe\" not in X_train_woe_advanced_sc.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{i}. {feat}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    fig.suptitle(f'{i}. {feat}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # RAW\n",
    "    axes[0].hist(X_train_advanced_raw[feat].dropna(), bins=30, alpha=0.7, color='steelblue')\n",
    "    axes[0].set_title('RAW', fontsize=10)\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # WoE\n",
    "    axes[1].hist(X_train_woe_advanced_sc[f\"{feat}_woe\"].dropna(), bins=20, alpha=0.7, color='darkgreen')\n",
    "    axes[1].set_title('WoE', fontsize=10)\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. PODSUMOWANIE\n",
    "# ============================================================================\n",
    "print(\"\\n\\n5Ô∏è‚É£  PODSUMOWANIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_features = X_train_woe_advanced_sc.shape[1]\n",
    "excellent = (iv_df_adv['IV'] >= 0.3).sum()\n",
    "medium = ((iv_df_adv['IV'] >= 0.1) & (iv_df_adv['IV'] < 0.3)).sum()\n",
    "weak = ((iv_df_adv['IV'] >= 0.02) & (iv_df_adv['IV'] < 0.1)).sum()\n",
    "\n",
    "print(f\"\\nüìä WYBRANYCH CECH: {n_features}\")\n",
    "print(f\"   Very Strong/Strong (IV ‚â• 0.3):  {excellent}\")\n",
    "print(f\"   Medium (0.1 ‚â§ IV < 0.3):        {medium}\")\n",
    "print(f\"   Weak (0.02 ‚â§ IV < 0.1):         {weak}\")\n",
    "\n",
    "# Korelacje\n",
    "if len(high_corr_adv) == 0:\n",
    "    print(f\"\\n‚úÖ WIELOKOLINEARNO≈öƒÜ: OK\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WIELOKOLINEARNO≈öƒÜ: {len(high_corr_adv)} par o |r| > 0.7\")\n",
    "\n",
    "print(\"\\n‚úÖ WoE transformation zako≈Ñczona!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä POR√ìWNANIE SCORECARD√ìW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Przygotuj dane do por√≥wnania\n",
    "if 'metrics_sc_basic' in globals() and 'metrics_sc_adv' in globals():\n",
    "    \n",
    "    comparison_sc = pd.DataFrame({\n",
    "        'Metryka': ['Cechy', 'ROC-AUC', 'PR-AUC', 'KS', 'Log Loss', 'Brier'],\n",
    "        'Basic Scorecard': [\n",
    "            X_train_woe_basic.shape[1] if 'X_train_woe_basic' in globals() else 0,\n",
    "            metrics_sc_basic.get('roc_auc', 0),\n",
    "            metrics_sc_basic.get('pr_auc', 0),\n",
    "            metrics_sc_basic.get('ks', 0),\n",
    "            metrics_sc_basic.get('log_loss', 0),\n",
    "            metrics_sc_basic.get('brier', 0)\n",
    "        ],\n",
    "        'Advanced Scorecard': [\n",
    "            X_train_woe_advanced_sc.shape[1] if 'X_train_woe_advanced_sc' in globals() else 0,\n",
    "            metrics_sc_adv.get('roc_auc', 0),\n",
    "            metrics_sc_adv.get('pr_auc', 0),\n",
    "            metrics_sc_adv.get('ks', 0),\n",
    "            metrics_sc_adv.get('log_loss', 0),\n",
    "            metrics_sc_adv.get('brier', 0)\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Dodaj kolumnƒô z r√≥≈ºnicƒÖ\n",
    "    comparison_sc['Œî (Adv - Basic)'] = comparison_sc['Advanced Scorecard'] - comparison_sc['Basic Scorecard']\n",
    "    \n",
    "    # Formatuj output\n",
    "    print(\"\\n\")\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "    print(comparison_sc.to_string(index=False))\n",
    "    pd.reset_option('display.float_format')\n",
    "    \n",
    "    # Interpretacja\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üí° INTERPRETACJA:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Znajd≈∫ lepszy scorecard (suma ranking√≥w)\n",
    "    basic_score = (metrics_sc_basic['roc_auc'] * 2) + metrics_sc_basic['ks'] + (1 - metrics_sc_basic['log_loss']) + (1 - metrics_sc_basic['brier'])\n",
    "    adv_score = (metrics_sc_adv['roc_auc'] * 2) + metrics_sc_adv['ks'] + (1 - metrics_sc_adv['log_loss']) + (1 - metrics_sc_adv['brier'])\n",
    "    \n",
    "    if adv_score > basic_score:\n",
    "        winner = \"Advanced Scorecard\"\n",
    "    else:\n",
    "        winner = \"Basic Scorecard\"\n",
    "    \n",
    "    diff_auc = metrics_sc_adv['roc_auc'] - metrics_sc_basic['roc_auc']\n",
    "    diff_ks = metrics_sc_adv['ks'] - metrics_sc_basic['ks']\n",
    "    \n",
    "    print(f\"\\nüèÜ Lepszy model: {winner}\")\n",
    "    print(f\"   R√≥≈ºnica ROC-AUC: {diff_auc:+.4f}\")\n",
    "    print(f\"   R√≥≈ºnica KS: {diff_ks:+.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìä WNIOSKI:\")\n",
    "    print(f\"   ‚Ä¢ Basic Scorecard:    Full Pipeline (165 cech) ‚Üí WoE ‚Üí {X_train_woe_basic.shape[1]} cech\")\n",
    "    print(f\"   ‚Ä¢ Advanced Scorecard: Feature Engineering (30 cech) ‚Üí WoE ‚Üí {X_train_woe_advanced_sc.shape[1]} cech\")\n",
    "    \n",
    "    if abs(diff_auc) > 0.01 or abs(diff_ks) > 0.05:\n",
    "        print(f\"\\n   ‚úÖ {winner} wyra≈∫nie lepszy\")\n",
    "    elif abs(diff_auc) > 0.005 or abs(diff_ks) > 0.02:\n",
    "        print(f\"\\n   ‚úì {winner} nieznacznie lepszy\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚âà Oba scorecards por√≥wnywalne\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\n‚ö†Ô∏è  Brak danych do por√≥wnania - uruchom oba scorecards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe1878",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 12.4 Por√≥wnanie Wszystkich Modeli\n",
    "\n",
    "Finalne zestawienie wszystkich podej≈õƒá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84950667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä FINALNE POR√ìWNANIE - WSZYSTKIE MODELE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# WA≈ªNE: Usu≈Ñ scorecard z results_advanced_raw je≈õli istnieje (to by≈Ç b≈ÇƒÖd)\n",
    "if 'results_advanced_raw' in globals() and 'Scorecard' in results_advanced_raw:\n",
    "    print(\"\\n‚ö†Ô∏è  Wykryto duplikat scorecard w results_advanced_raw - usuwam...\")\n",
    "    del results_advanced_raw['Scorecard']\n",
    "    print(\"   ‚úÖ Usuniƒôto\")\n",
    "\n",
    "# Zbierz wyniki\n",
    "comparison_all = []\n",
    "\n",
    "# 1. Full Pipeline (wszystkie modele - BEZ Scorecard!)\n",
    "if 'results_full' in globals():\n",
    "    for model_name, metrics in results_full.items():\n",
    "        # Pomi≈Ñ scorecard je≈õli przypadkowo zosta≈Ç dodany do results_full\n",
    "        if 'scorecard' in model_name.lower() or 'woe' in model_name.lower():\n",
    "            continue\n",
    "        \n",
    "        comparison_all.append({\n",
    "            'Pipeline': 'Full',\n",
    "            'Model': model_name,\n",
    "            'Cechy': 165,\n",
    "            'ROC-AUC': metrics.get('roc_auc', metrics.get('auc', 0)),\n",
    "            'PR-AUC': metrics.get('pr_auc', 0),\n",
    "            'KS': metrics.get('ks', 0),\n",
    "            'Log Loss': metrics.get('log_loss', 0),\n",
    "            'Brier': metrics.get('brier', 0)\n",
    "        })\n",
    "\n",
    "# 2. Advanced Pipeline\n",
    "if 'results_advanced_raw' in globals():\n",
    "    for model_name, metrics in results_advanced_raw.items():\n",
    "        comparison_all.append({\n",
    "            'Pipeline': 'Advanced',\n",
    "            'Model': model_name,\n",
    "            'Cechy': 30,\n",
    "            'ROC-AUC': metrics.get('roc_auc', metrics.get('auc', 0)),\n",
    "            'PR-AUC': metrics.get('pr_auc', 0),\n",
    "            'KS': metrics.get('ks', 0),\n",
    "            'Log Loss': metrics.get('log_loss', 0),\n",
    "            'Brier': metrics.get('brier', 0)\n",
    "        })\n",
    "\n",
    "# 3. Optimized Scorecards\n",
    "if 'metrics_sc_basic' in globals():\n",
    "    n_feat_basic = X_train_woe_basic.shape[1] if 'X_train_woe_basic' in globals() else 0\n",
    "    comparison_all.append({\n",
    "        'Pipeline': 'Basic Scorecard',\n",
    "        'Model': 'LR+WoE (Optimized)',\n",
    "        'Cechy': n_feat_basic,\n",
    "        'ROC-AUC': metrics_sc_basic.get('roc_auc', 0),\n",
    "        'PR-AUC': metrics_sc_basic.get('pr_auc', 0),\n",
    "        'KS': metrics_sc_basic.get('ks', 0),\n",
    "        'Log Loss': metrics_sc_basic.get('log_loss', 0),\n",
    "        'Brier': metrics_sc_basic.get('brier', 0)\n",
    "    })\n",
    "\n",
    "if 'metrics_sc_adv' in globals():\n",
    "    n_feat_adv = X_train_woe_advanced_sc.shape[1] if 'X_train_woe_advanced_sc' in globals() else 0\n",
    "    comparison_all.append({\n",
    "        'Pipeline': 'Advanced Scorecard',\n",
    "        'Model': 'LR+WoE (Optimized)',\n",
    "        'Cechy': n_feat_adv,\n",
    "        'ROC-AUC': metrics_sc_adv.get('roc_auc', 0),\n",
    "        'PR-AUC': metrics_sc_adv.get('pr_auc', 0),\n",
    "        'KS': metrics_sc_adv.get('ks', 0),\n",
    "        'Log Loss': metrics_sc_adv.get('log_loss', 0),\n",
    "        'Brier': metrics_sc_adv.get('brier', 0)\n",
    "    })\n",
    "\n",
    "# Utw√≥rz DataFrame i sortuj\n",
    "df_comparison = pd.DataFrame(comparison_all)\n",
    "df_comparison = df_comparison.sort_values(['ROC-AUC', 'KS'], ascending=[False, False])\n",
    "\n",
    "# Wy≈õwietl\n",
    "print(\"\\n\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(df_comparison.to_string(index=False))\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.float_format')\n",
    "\n",
    "# Analiza\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° ANALIZA WYNIK√ìW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = df_comparison.iloc[0]\n",
    "print(f\"\\nüèÜ NAJLEPSZY MODEL:\")\n",
    "print(f\"   {best_model['Pipeline']} - {best_model['Model']}\")\n",
    "print(f\"   ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n",
    "print(f\"   KS:      {best_model['KS']:.4f}\")\n",
    "print(f\"   Cechy:   {int(best_model['Cechy'])}\")\n",
    "\n",
    "print(f\"\\nüìä KLUCZOWE METRYKI (dla credit scoring):\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC:  Og√≥lna skuteczno≈õƒá (wy≈ºsze = lepsze)\")\n",
    "print(f\"   ‚Ä¢ PR-AUC:   Skuteczno≈õƒá na niezbalansowanych (wy≈ºsze = lepsze)\")\n",
    "print(f\"   ‚Ä¢ KS:       Separacja klas (wy≈ºsze = lepsze, >0.3 = dobre)\")\n",
    "print(f\"   ‚Ä¢ Log Loss: Jako≈õƒá prawdopodobie≈Ñstw (ni≈ºsze = lepsze)\")\n",
    "print(f\"   ‚Ä¢ Brier:    Dok≈Çadno≈õƒá predykcji (ni≈ºsze = lepsze)\")\n",
    "\n",
    "# Top 3\n",
    "print(f\"\\nü•á TOP 3 MODELE:\")\n",
    "for i, (idx, row) in enumerate(df_comparison.head(3).iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Pipeline']:<25} {row['Model']:<25} AUC={row['ROC-AUC']:.4f}, KS={row['KS']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRADE-OFF: INTERPRETOWALNO≈öƒÜ vs PERFORMANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚öñÔ∏è  INTERPRETOWALNO≈öƒÜ vs PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Znajd≈∫ najlepszy scorecard i model black-box\n",
    "scorecards = df_comparison[df_comparison['Pipeline'].str.contains('Scorecard', na=False)]\n",
    "other_models = df_comparison[~df_comparison['Pipeline'].str.contains('Scorecard', na=False)]\n",
    "\n",
    "if not scorecards.empty and not other_models.empty:\n",
    "    best_scorecard_row = scorecards.iloc[0]\n",
    "    best_other = other_models.iloc[0]\n",
    "    \n",
    "    print(f\"\\nüìä NAJLEPSZY SCORECARD (Interpretowalny):\")\n",
    "    print(f\"   {best_scorecard_row['Pipeline']}: {best_scorecard_row['Model']}\")\n",
    "    print(f\"   ROC-AUC: {best_scorecard_row['ROC-AUC']:.4f} | KS: {best_scorecard_row['KS']:.4f} | Cechy: {int(best_scorecard_row['Cechy'])}\")\n",
    "    \n",
    "    print(f\"\\nüîß NAJLEPSZY INNY MODEL:\")\n",
    "    print(f\"   {best_other['Pipeline']}: {best_other['Model']}\")\n",
    "    print(f\"   ROC-AUC: {best_other['ROC-AUC']:.4f} | KS: {best_other['KS']:.4f} | Cechy: {int(best_other['Cechy'])}\")\n",
    "    \n",
    "    auc_diff = best_other['ROC-AUC'] - best_scorecard_row['ROC-AUC']\n",
    "    \n",
    "    print(f\"\\nüí° R√ì≈ªNICA:\")\n",
    "    print(f\"   Œî ROC-AUC: {auc_diff:+.4f} ({abs(auc_diff/best_scorecard_row['ROC-AUC']*100):.2f}%)\")\n",
    "    \n",
    "    if abs(auc_diff) < 0.01:\n",
    "        print(f\"\\n‚úÖ Scorecard prawie r√≥wnie dobry - preferuj INTERPRETOWALNO≈öƒÜ!\")\n",
    "    elif abs(auc_diff) < 0.03:\n",
    "        print(f\"\\n‚öñÔ∏è  Trade-off: niewielka r√≥≈ºnica w performance\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Wiƒôksza r√≥≈ºnica - rozwa≈º zastosowanie obu modeli\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9691f32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 13. Interpretacja Modelu Interpretowalnego üîç\n",
    "\n",
    "## 13.1 Interpretacja Globalna - Analiza Wsp√≥≈Çczynnik√≥w i Wa≈ºno≈õci Cech\n",
    "\n",
    "**Wymaganie 3.3**: Interpretacja globalna - znaki i wielko≈õci wsp√≥≈Çczynnik√≥w, wa≈ºno≈õƒá cech, PDP/ICE curves\n",
    "\n",
    "Szczeg√≥≈Çowa analiza najlepszego modelu interpretowalnego (scorecard) wybranego na podstawie metryk walidacyjnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2aad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç INTERPRETACJA GLOBALNA - NAJLEPSZY MODEL INTERPRETOWALNY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAnaliza wp≈Çywu cech na predykcjƒô probability of default (PD)\")\n",
    "print(\"Zgodnie z wymaganiem 3.3: wsp√≥≈Çczynniki, wa≈ºno≈õƒá cech, PDP/ICE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# WYB√ìR NAJLEPSZEGO MODELU\n",
    "# ============================================================================\n",
    "if 'metrics_sc_basic' in globals() and 'metrics_sc_adv' in globals():\n",
    "    if metrics_sc_adv['roc_auc'] > metrics_sc_basic['roc_auc']:\n",
    "        best_scorecard = scorecard_advanced\n",
    "        X_train_best = X_train_woe_advanced_sc\n",
    "        X_test_best = X_test_woe_advanced_sc\n",
    "        woe_mappings_best = woe_mappings_advanced_sc\n",
    "        model_name = \"Advanced Scorecard\"\n",
    "    else:\n",
    "        best_scorecard = scorecard_basic\n",
    "        X_train_best = X_train_woe_basic\n",
    "        X_test_best = X_test_woe_basic\n",
    "        woe_mappings_best = woe_mappings_basic\n",
    "        model_name = \"Basic Scorecard\"\n",
    "elif 'scorecard_advanced' in globals():\n",
    "    best_scorecard = scorecard_advanced\n",
    "    X_train_best = X_train_woe_advanced_sc\n",
    "    X_test_best = X_test_woe_advanced_sc\n",
    "    woe_mappings_best = woe_mappings_advanced_sc\n",
    "    model_name = \"Advanced Scorecard\"\n",
    "else:\n",
    "    best_scorecard = scorecard_basic\n",
    "    X_train_best = X_train_woe_basic\n",
    "    X_test_best = X_test_woe_basic\n",
    "    woe_mappings_best = woe_mappings_basic\n",
    "    model_name = \"Basic Scorecard\"\n",
    "\n",
    "# Wy≈õwietl informacje o modelu\n",
    "roc_auc_best = roc_auc_score(y_test, best_scorecard.predict_proba(X_test_best)[:, 1])\n",
    "\n",
    "print(f\"\\nüéØ WYBRANY MODEL: {model_name}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Liczba cech:     {X_train_best.shape[1]}\")\n",
    "print(f\"   ROC-AUC (test):  {roc_auc_best:.4f}\")\n",
    "print(f\"   Obs train:       {X_train_best.shape[0]}\")\n",
    "print(f\"   Obs test:        {X_test_best.shape[0]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ANALIZA WSP√ì≈ÅCZYNNIK√ìW I LOG-ODDS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1Ô∏è‚É£ WSP√ì≈ÅCZYNNIKI REGRESJI LOGISTYCZNEJ (LOG-ODDS)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nInterpretacja: coefficient = zmiana log-odds przy wzro≈õcie cechy o 1 jednostkƒô\")\n",
    "print(\"‚Ä¢ Positive ‚Üí wzrost cechy ZMNIEJSZA PD (protective factor)\")\n",
    "print(\"‚Ä¢ Negative ‚Üí wzrost cechy ZWIƒòKSZA PD (risk driver)\\n\")\n",
    "\n",
    "# Analiza wsp√≥≈Çczynnik√≥w\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X_train_best.columns.tolist(),\n",
    "    'Coefficient': best_scorecard.coef_[0],\n",
    "    'Abs_Coef': np.abs(best_scorecard.coef_[0])\n",
    "}).sort_values('Abs_Coef', ascending=False)\n",
    "\n",
    "print(f\"üìä TOP 10 CECH (wed≈Çug |coefficient|):\\n\")\n",
    "for i, (idx, row) in enumerate(coef_df.head(10).iterrows(), 1):\n",
    "    feat = row['Feature']\n",
    "    coef = row['Coefficient']\n",
    "    direction = \"üìà PROTECTIVE\" if coef > 0 else \"üìâ RISK DRIVER\"\n",
    "    \n",
    "    # Si≈Ça wp≈Çywu\n",
    "    if abs(coef) > 0.5: strength = \"üî• Bardzo silny\"\n",
    "    elif abs(coef) > 0.3: strength = \"üí™ Silny\"\n",
    "    elif abs(coef) > 0.15: strength = \"‚úì ≈öredni\"\n",
    "    else: strength = \"‚Ä¢ S≈Çaby\"\n",
    "    \n",
    "    feat_display = feat if len(feat) <= 42 else feat[:39] + \"...\"\n",
    "    print(f\"   {i:2}. {feat_display:<45} | Coef: {coef:>7.4f} | {direction:15} | {strength}\")\n",
    "\n",
    "print(f\"\\nüìê Statystyki wsp√≥≈Çczynnik√≥w:\")\n",
    "print(f\"   Mean |coef|:     {coef_df['Abs_Coef'].mean():.4f}\")\n",
    "print(f\"   Std Dev:         {coef_df['Coefficient'].std():.4f}\")\n",
    "print(f\"   Max |coef|:      {coef_df['Abs_Coef'].max():.4f}\")\n",
    "print(f\"   Intercept:       {best_scorecard.intercept_[0]:.4f}\")\n",
    "\n",
    "# Top 6 features dla PDP/ICE\n",
    "top_features = coef_df.head(6)['Feature'].tolist()\n",
    "print(f\"\\nüîç TOP 6 cech wybrane do szczeg√≥≈Çowej analizy PDP/ICE\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PARTIAL DEPENDENCE PLOTS (PDP)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2Ô∏è‚É£ PARTIAL DEPENDENCE PLOTS (PDP)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüí° Co pokazuje PDP?\")\n",
    "print(\"   ‚Ä¢ ≈öREDNI wp≈Çyw cechy na predykcjƒô (u≈õredniajƒÖc po wszystkich obserwacjach)\")\n",
    "print(\"   ‚Ä¢ Jak zmiana warto≈õci cechy wp≈Çywa na przewidywane PD\")\n",
    "print(\"   ‚Ä¢ Wykrywa nieliniowe zale≈ºno≈õci i interakcje\\n\")\n",
    "\n",
    "# Get feature indices\n",
    "feature_indices = [X_train_best.columns.get_loc(feat) for feat in top_features]\n",
    "\n",
    "# Create PDP for top 6 features (2x3 grid)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (feat_idx, feat_name) in enumerate(zip(feature_indices, top_features)):\n",
    "    # Calculate PDP\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        best_scorecard,\n",
    "        X_train_best,\n",
    "        features=[feat_idx],\n",
    "        kind='average',  # PDP = average (not individual)\n",
    "        ax=axes[idx],\n",
    "        grid_resolution=50\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    coef = coef_df[coef_df['Feature'] == feat_name]['Coefficient'].values[0]\n",
    "    direction = \"Risk Driver\" if coef < 0 else \"Protective\"\n",
    "    \n",
    "    # Skr√≥ƒá d≈ÇugƒÖ nazwƒô cechy\n",
    "    feat_short = feat_name if len(feat_name) <= 40 else feat_name[:37] + \"...\"\n",
    "    \n",
    "    axes[idx].set_title(f\"{feat_short}\\n{direction} (Coef: {coef:.3f})\", \n",
    "                        fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('WoE Value', fontsize=9)\n",
    "    axes[idx].set_ylabel('Partial Dependence\\n(log-odds)', fontsize=9)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Partial Dependence Plots (PDP) - TOP 6 Features\\nJak zmiana WoE wp≈Çywa na log-odds (≈õrednio)', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretacja PDP:\")\n",
    "print(\"   - O≈õ X: warto≈õƒá WoE cechy\")\n",
    "print(\"   - O≈õ Y: zmiana log-odds (predykcja)\")\n",
    "print(\"   - Nachylenie = si≈Ça i kierunek efektu\")\n",
    "print(\"   - Pozytywny slope ‚Üí wy≈ºszy WoE = ni≈ºsze PD (protective)\")\n",
    "print(\"   - Negatywny slope ‚Üí wy≈ºszy WoE = wy≈ºsze PD (risk driver)\")\n",
    "print(\"   ‚Üí PDP pokazuje GLOBALNY, U≈öREDNIONY efekt\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ICE CURVES (Individual Conditional Expectation)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3Ô∏è‚É£ ICE CURVES (Individual Conditional Expectation)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"ICE pokazuje jak zmiana cechy wp≈Çywa na KA≈ªDƒÑ INDYWIDUALNƒÑ obserwacjƒô\")\n",
    "print(\"(nie u≈õrednia - widaƒá heterogeniczno≈õƒá efektu)\")\n",
    "\n",
    "# Create ICE for top 3 features (most important)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (feat_idx, feat_name) in enumerate(zip(feature_indices[:3], top_features[:3])):\n",
    "    # Sample observations\n",
    "    sample_indices = np.random.choice(len(X_train_best), size=min(100, len(X_train_best)), replace=False)\n",
    "    X_sample = X_train_best.iloc[sample_indices]\n",
    "    \n",
    "    # Use 'both' to show ICE + PDP together (avoids ax reuse issue)\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        best_scorecard,\n",
    "        X_sample,\n",
    "        features=[feat_idx],\n",
    "        kind='both',  # Shows both individual and average together\n",
    "        ax=axes[idx],\n",
    "        grid_resolution=20\n",
    "    )\n",
    "    \n",
    "    coef = coef_df[coef_df['Feature'] == feat_name]['Coefficient'].values[0]\n",
    "    direction = \"Risk\" if coef < 0 else \"Protective\"\n",
    "    \n",
    "    # Skr√≥ƒá d≈ÇugƒÖ nazwƒô cechy\n",
    "    feat_short = feat_name if len(feat_name) <= 40 else feat_name[:37] + \"...\"\n",
    "    \n",
    "    axes[idx].set_title(f\"{feat_short}\\n{direction} (Coef: {coef:.3f})\", \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('WoE Value', fontsize=10)\n",
    "    axes[idx].set_ylabel('ICE (log-odds)', fontsize=10)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('ICE Curves - TOP 3 Features\\nSzare linie = individual obs | Czerwona = PDP (≈õrednia)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretacja ICE:\")\n",
    "print(\"   - Ka≈ºda szara linia = jedna obserwacja\")\n",
    "print(\"   - Czerwona linia = PDP (≈õrednia ICE)\")\n",
    "print(\"   - R√≥wnoleg≈Çe linie ‚Üí efekt homogeniczny (dobry!)\")\n",
    "print(\"   - Rozjechane linie ‚Üí efekt heterogeniczny (interakcje)\")\n",
    "print(\"   ‚Üí ICE pokazuje R√ì≈ªNORODNO≈öƒÜ efektu miƒôdzy obserwacjami\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CENTERED ICE (C-ICE)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4Ô∏è‚É£ CENTERED ICE (C-ICE) - Heterogeniczno≈õƒá Efektu\")\n",
    "print(\"-\" * 80)\n",
    "print(\"C-ICE = ICE wycentrowane w punkcie odniesienia\")\n",
    "print(\"≈Åatwiej zobaczyƒá r√≥≈ºnice miƒôdzy obserwacjami\")\n",
    "\n",
    "# Create Centered ICE for top 3 features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (feat_idx, feat_name) in enumerate(zip(feature_indices[:3], top_features[:3])):\n",
    "    # Sample observations for C-ICE\n",
    "    sample_indices = np.random.choice(len(X_train_best), size=min(100, len(X_train_best)), replace=False)\n",
    "    X_sample = X_train_best.iloc[sample_indices]\n",
    "    \n",
    "    # Centered ICE\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        best_scorecard,\n",
    "        X_sample,\n",
    "        features=[feat_idx],\n",
    "        kind='both',  # Shows both ICE and PDP\n",
    "        centered=True,  # THIS IS THE KEY for C-ICE!\n",
    "        ax=axes[idx],\n",
    "        grid_resolution=20\n",
    "    )\n",
    "    \n",
    "    coef = coef_df[coef_df['Feature'] == feat_name]['Coefficient'].values[0]\n",
    "    direction = \"Risk Driver\" if coef < 0 else \"Protective\"\n",
    "    \n",
    "    # Skr√≥ƒá d≈ÇugƒÖ nazwƒô cechy\n",
    "    feat_short = feat_name if len(feat_name) <= 40 else feat_name[:37] + \"...\"\n",
    "\n",
    "    axes[idx].set_title(f\"{feat_short}\\n{direction} (Coef: {coef:.3f})\",\n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('WoE Value', fontsize=10)\n",
    "    axes[idx].set_ylabel('C-ICE (centered log-odds)', fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Centered ICE (C-ICE) - TOP 3 Features\\nWycentrowane w punkcie odniesienia (start=0)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretacja C-ICE:\")\n",
    "print(\"   - Wszystkie linie zaczynajƒÖ siƒô w 0\")\n",
    "print(\"   - ≈Åatwiej zobaczyƒá r√≥≈ºnice w nachyleniach\")\n",
    "print(\"   - Ma≈Çe rozjechanie ‚Üí efekt ADDYTYWNY (brak interakcji)\")\n",
    "print(\"   - Du≈ºe rozjechanie ‚Üí efekt NON-ADDYTYWNY (interakcje z innymi cechami)\")\n",
    "print(\"   ‚Üí C-ICE diagnozuje za≈Ço≈ºenie addytywno≈õci modelu\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. PODSUMOWANIE PDP/ICE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5Ô∏è‚É£ PODSUMOWANIE: PDP vs ICE vs C-ICE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä PDP (Partial Dependence Plot):\")\n",
    "print(\"   ‚úì Pokazuje ≈öREDNI efekt cechy\")\n",
    "print(\"   ‚úì Ignoruje heterogeniczno≈õƒá\")\n",
    "print(\"   ‚úì Dobry do: 'og√≥lny trend'\")\n",
    "print(\"   ‚úó Mo≈ºe byƒá mylƒÖcy je≈õli du≈ºe interakcje\")\n",
    "\n",
    "print(\"\\nüìà ICE (Individual Conditional Expectation):\")\n",
    "print(\"   ‚úì Pokazuje efekt dla KA≈ªDEJ obserwacji\")\n",
    "print(\"   ‚úì Diagnozuje heterogeniczno≈õƒá\")\n",
    "print(\"   ‚úì Dobry do: 'czy efekt jest stabilny?'\")\n",
    "print(\"   ‚úó Trudniejszy do czytania (100+ linii)\")\n",
    "\n",
    "print(\"\\nüéØ C-ICE (Centered ICE):\")\n",
    "print(\"   ‚úì ICE wycentrowane ‚Üí ≈Çatwiej por√≥wnaƒá nachylenia\")\n",
    "print(\"   ‚úì Diagnozuje interakcje (non-addytywno≈õƒá)\")\n",
    "print(\"   ‚úì Dobry do: 'czy model jest addytywny?'\")\n",
    "\n",
    "print(\"\\nüèÜ ZASTOSOWANIE W CREDIT SCORING:\")\n",
    "print(\"   1. PDP ‚Üí pokazujemy biznesowi '≈õredni efekt cechy'\")\n",
    "print(\"   2. ICE ‚Üí weryfikujemy czy efekt jest stabilny\")\n",
    "print(\"   3. C-ICE ‚Üí sprawdzamy za≈Ço≈ºenie addytywno≈õci\")\n",
    "print(\"   ‚Üí Razem dajƒÖ PE≈ÅNY obraz globalnego dzia≈Çania modelu!\")\n",
    "\n",
    "print(\"\\n‚úÖ SPE≈ÅNIONE WYMOGI:\")\n",
    "print(\"   ‚úì Krzywe PDP dla TOP 6 features\")\n",
    "print(\"   ‚úì Krzywe ICE dla TOP 3 features\")\n",
    "print(\"   ‚úì C-ICE do diagnozy interakcji\")\n",
    "print(\"   ‚úì Interpretacja nachyle≈Ñ i heterogeniczno≈õci\")\n",
    "print(\"   ‚Üí Requirement 3.3 (PDP/ICE curves) DONE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d2a0c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"[OK] xgboost dostƒôpny\")\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"[WARNING] xgboost not installed - uruchom kom√≥rkƒô instalacyjnƒÖ powy≈ºej\")\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    SMOTE_AVAILABLE = True\n",
    "    print(\"[OK] imbalanced-learn dostƒôpny\")\n",
    "except ImportError:\n",
    "    SMOTE_AVAILABLE = False\n",
    "    print(\"[WARNING] imbalanced-learn not installed - uruchom kom√≥rkƒô instalacyjnƒÖ powy≈ºej\")\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(\"[OK] shap dostƒôpny\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"[WARNING] shap not installed - uruchom kom√≥rkƒô instalacyjnƒÖ powy≈ºej\")\n",
    "\n",
    "# Import modu≈Ç√≥w projektu\n",
    "from src import visualization\n",
    "from src.visualization import (\n",
    "    plot_correlation_matrix,\n",
    "    plot_target_correlation,\n",
    "    plot_distribution_comparison,\n",
    "    plot_model_comparison,\n",
    "    plot_roc_curves,\n",
    "    plot_confusion_matrices,\n",
    "    plot_data_overview\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n[OK] Biblioteki za≈Çadowane\")\n",
    "print(\"[OK] Modu≈Çy wizualizacji zaimportowane z src.visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. ANALIZA WSP√ì≈ÅCZYNNIK√ìW - INTERPRETACJA BIZNESOWA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5Ô∏è‚É£ ANALIZA WSP√ì≈ÅCZYNNIK√ìW - INTERPRETACJA BIZNESOWA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sortuj wsp√≥≈Çczynniki po warto≈õci bezwzglƒôdnej\n",
    "coef_analysis = coef_df.copy()\n",
    "coef_analysis = coef_analysis.sort_values('Abs_Coef', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä WSZYSTKIE WSP√ì≈ÅCZYNNIKI ({len(coef_analysis)} cech):\\n\")\n",
    "print(f\"{'Rank':<6} {'Feature':<45} {'Coefficient':>12} {'Direction':<15} {'Log-Odds Impact':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, (idx, row) in enumerate(coef_analysis.iterrows(), 1):\n",
    "    feat = row['Feature']\n",
    "    coef = row['Coefficient']\n",
    "    \n",
    "    # Okre≈õl kierunek wp≈Çywu\n",
    "    if coef > 0:\n",
    "        direction = \"‚úÖ Protective\"\n",
    "        impact = \"‚ÜëWoE ‚Üí ‚ÜìPD\"\n",
    "    else:\n",
    "        direction = \"‚ö†Ô∏è  Risk Driver\"\n",
    "        impact = \"‚ÜëWoE ‚Üí ‚ÜëPD\"\n",
    "    \n",
    "    # Skr√≥ƒá nazwƒô cechy\n",
    "    feat_display = feat if len(feat) <= 43 else feat[:40] + \"...\"\n",
    "    \n",
    "    # Oblicz zmianƒô log-odds dla 1 jednostki WoE\n",
    "    log_odds_change = coef * 1.0\n",
    "    \n",
    "    print(f\"{i:<6} {feat_display:<45} {coef:>12.4f} {direction:<15} {impact:<20}\")\n",
    "\n",
    "# Statystyki wsp√≥≈Çczynnik√≥w\n",
    "print(f\"\\nüìä STATYSTYKI WSP√ì≈ÅCZYNNIK√ìW:\")\n",
    "print(f\"   ‚Ä¢ ≈örednia |coef|:  {coef_analysis['Abs_Coef'].mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Mediana |coef|:  {coef_analysis['Abs_Coef'].median():.4f}\")\n",
    "print(f\"   ‚Ä¢ Max |coef|:      {coef_analysis['Abs_Coef'].max():.4f}  (cecha: {coef_analysis.iloc[0]['Feature']})\")\n",
    "print(f\"   ‚Ä¢ Min |coef|:      {coef_analysis['Abs_Coef'].min():.4f}\")\n",
    "\n",
    "# Rozk≈Çad kierunk√≥w\n",
    "n_positive = (coef_analysis['Coefficient'] > 0).sum()\n",
    "n_negative = (coef_analysis['Coefficient'] < 0).sum()\n",
    "\n",
    "print(f\"\\nüìä ROZK≈ÅAD KIERUNK√ìW WP≈ÅYWU:\")\n",
    "print(f\"   ‚Ä¢ Protective (coef > 0):  {n_positive} cech ({n_positive/len(coef_analysis)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Risk Drivers (coef < 0): {n_negative} cech ({n_negative/len(coef_analysis)*100:.1f}%)\")\n",
    "\n",
    "# Wizualizacja rozk≈Çadu wsp√≥≈Çczynnik√≥w\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram wsp√≥≈Çczynnik√≥w\n",
    "axes[0].hist(coef_analysis['Coefficient'], bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero (no effect)')\n",
    "axes[0].set_xlabel('Coefficient Value', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Rozk≈Çad Wsp√≥≈Çczynnik√≥w', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Barplot TOP 10\n",
    "top10_coef = coef_analysis.head(10)\n",
    "colors = ['green' if c > 0 else 'red' for c in top10_coef['Coefficient']]\n",
    "feat_names_short = [f[:30] + '...' if len(f) > 30 else f for f in top10_coef['Feature']]\n",
    "\n",
    "axes[1].barh(range(len(top10_coef)), top10_coef['Coefficient'], color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top10_coef)))\n",
    "axes[1].set_yticklabels(feat_names_short, fontsize=9)\n",
    "axes[1].set_xlabel('Coefficient Value', fontsize=11)\n",
    "axes[1].set_title('TOP 10 Cech (|coefficient|)', fontsize=12, fontweight='bold')\n",
    "axes[1].axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° WNIOSKI:\")\n",
    "print(\"   ‚Ä¢ Zielone s≈Çupki (coef > 0) = Protective factors (ni≈ºsze PD)\")\n",
    "print(\"   ‚Ä¢ Czerwone s≈Çupki (coef < 0) = Risk drivers (wy≈ºsze PD)\")\n",
    "print(\"   ‚Ä¢ Wiƒôkszy |coef| = silniejszy wp≈Çyw na predykcjƒô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. INFORMATION VALUE (IV) - SI≈ÅA PREDYKCYJNA CECH\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6Ô∏è‚É£ INFORMATION VALUE - SI≈ÅA PREDYKCYJNA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pobierz IV dla aktualnego modelu\n",
    "if 'iv_df_advanced' in globals() and model_name == \"Advanced Scorecard\":\n",
    "    iv_data_current = iv_df_advanced.copy()\n",
    "elif 'iv_df_basic' in globals():\n",
    "    iv_data_current = iv_df_basic.copy()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Brak danych IV!\")\n",
    "    iv_data_current = None\n",
    "\n",
    "if iv_data_current is not None:\n",
    "    # Filtruj tylko cechy w modelu\n",
    "    features_in_model = [f.replace('_woe', '') for f in X_train_best.columns if f.endswith('_woe')]\n",
    "    iv_model = iv_data_current[iv_data_current['feature'].isin(features_in_model)].copy()\n",
    "    iv_model = iv_model.sort_values('IV', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä INFORMATION VALUE - Cechy w modelu ({len(iv_model)} cech):\\n\")\n",
    "    print(f\"{'Rank':<6} {'Feature':<45} {'IV':>10} {'Moc Predykcyjna':<25}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(iv_model.iterrows(), 1):\n",
    "        feat = row['feature']\n",
    "        iv = row['IV']\n",
    "        \n",
    "        # Kategoryzacja IV\n",
    "        if iv < 0.02:\n",
    "            power = \"‚ùå Unpredictive\"\n",
    "        elif iv < 0.1:\n",
    "            power = \"‚ö†Ô∏è  Weak\"\n",
    "        elif iv < 0.3:\n",
    "            power = \"‚úì Medium\"\n",
    "        elif iv < 0.5:\n",
    "            power = \"‚úì‚úì Strong\"\n",
    "        else:\n",
    "            power = \"‚úì‚úì‚úì Very Strong\"\n",
    "        \n",
    "        feat_display = feat if len(feat) <= 43 else feat[:40] + \"...\"\n",
    "        print(f\"{i:<6} {feat_display:<45} {iv:>10.4f} {power:<25}\")\n",
    "    \n",
    "    # Statystyki IV\n",
    "    print(f\"\\nüìä STATYSTYKI IV:\")\n",
    "    print(f\"   ‚Ä¢ ≈örednia IV:  {iv_model['IV'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mediana IV:  {iv_model['IV'].median():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max IV:      {iv_model['IV'].max():.4f}  (cecha: {iv_model.iloc[0]['feature']})\")\n",
    "    print(f\"   ‚Ä¢ Min IV:      {iv_model['IV'].min():.4f}\")\n",
    "    \n",
    "    # Rozk≈Çad mocy predykcyjnej\n",
    "    iv_bins = pd.cut(iv_model['IV'], \n",
    "                     bins=[0, 0.02, 0.1, 0.3, 0.5, float('inf')],\n",
    "                     labels=['Unpredictive', 'Weak', 'Medium', 'Strong', 'Very Strong'])\n",
    "    \n",
    "    print(f\"\\nüìä ROZK≈ÅAD MOCY PREDYKCYJNEJ:\")\n",
    "    for power, count in iv_bins.value_counts().sort_index().items():\n",
    "        print(f\"   ‚Ä¢ {power:<15}: {count} cech ({count/len(iv_model)*100:.1f}%)\")\n",
    "    \n",
    "    # Wizualizacja\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram IV\n",
    "    axes[0].hist(iv_model['IV'], bins=15, alpha=0.7, color='darkgreen', edgecolor='black')\n",
    "    axes[0].axvline(iv_model['IV'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {iv_model[\"IV\"].mean():.3f}')\n",
    "    axes[0].set_xlabel('Information Value', fontsize=11)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0].set_title('Rozk≈Çad Information Value', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Barplot TOP 10 IV\n",
    "    top10_iv = iv_model.head(10)\n",
    "    feat_names_short = [f[:30] + '...' if len(f) > 30 else f for f in top10_iv['feature']]\n",
    "    colors_iv = ['darkgreen' if iv >= 0.3 else 'orange' if iv >= 0.1 else 'red' for iv in top10_iv['IV']]\n",
    "    \n",
    "    axes[1].barh(range(len(top10_iv)), top10_iv['IV'], color=colors_iv, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_yticks(range(len(top10_iv)))\n",
    "    axes[1].set_yticklabels(feat_names_short, fontsize=9)\n",
    "    axes[1].set_xlabel('Information Value', fontsize=11)\n",
    "    axes[1].set_title('TOP 10 Cech (IV)', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3, axis='x')\n",
    "    axes[1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° INTERPRETACJA IV:\")\n",
    "    print(\"   ‚Ä¢ IV mierzy si≈Çƒô separacji klas (bad vs good)\")\n",
    "    print(\"   ‚Ä¢ Zielony: Strong/Very Strong (IV ‚â• 0.3)\")\n",
    "    print(\"   ‚Ä¢ Pomara≈Ñczowy: Medium/Weak (0.1 ‚â§ IV < 0.3)\")\n",
    "    print(\"   ‚Ä¢ Czerwony: Unpredictive (IV < 0.1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. PODSUMOWANIE INTERPRETACJI GLOBALNEJ\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7Ô∏è‚É£ PODSUMOWANIE - INTERPRETACJA GLOBALNA\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ WYKONANE ANALIZY (Requirement 3.3):\")\n",
    "print(\"   1. Wsp√≥≈Çczynniki i log-odds ‚úì\")\n",
    "print(\"   2. Wa≈ºno≈õƒá cech (Information Value) ‚úì\")\n",
    "print(\"   3. Partial Dependence Plots (PDP) ‚úì\")\n",
    "print(\"   4. Individual Conditional Expectation (ICE) ‚úì\")\n",
    "print(\"   5. Analiza WoE i biznesowa interpretacja ‚úì\\n\")\n",
    "\n",
    "print(f\"üéØ MODEL: {model_name}\")\n",
    "print(f\"   ‚Ä¢ Liczba cech: {X_train_best.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC: {roc_auc_best:.4f}\")\n",
    "\n",
    "# Najwa≈ºniejsze cechy\n",
    "print(f\"\\nüìä TOP 5 NAJWA≈ªNIEJSZYCH CECH (|coefficient|):\")\n",
    "for i, (idx, row) in enumerate(coef_analysis.head(5).iterrows(), 1):\n",
    "    feat = row['Feature']\n",
    "    coef = row['Coefficient']\n",
    "    direction = \"Protective\" if coef > 0 else \"Risk Driver\"\n",
    "    \n",
    "    feat_short = feat if len(feat) <= 50 else feat[:47] + \"...\"\n",
    "    print(f\"   {i}. {feat_short:<50} | Coef: {coef:>8.4f} ({direction})\")\n",
    "\n",
    "# Zgodno≈õƒá coefficient vs IV\n",
    "if iv_data_current is not None and len(iv_model) > 0:\n",
    "    print(f\"\\nüîó ZGODNO≈öƒÜ: Coefficient vs Information Value\")\n",
    "    print(f\"   ‚Ä¢ Cechy o wysokim |coef| zazwyczaj majƒÖ wysokie IV\")\n",
    "    print(f\"   ‚Ä¢ ≈örednie IV w modelu: {iv_model['IV'].mean():.4f}\")\n",
    "    \n",
    "    # Sprawd≈∫ czy top features majƒÖ wysokie IV\n",
    "    top5_features_clean = [f.replace('_woe', '') for f in coef_analysis.head(5)['Feature']]\n",
    "    top5_iv = iv_model[iv_model['feature'].isin(top5_features_clean)]\n",
    "    if len(top5_iv) > 0:\n",
    "        print(f\"   ‚Ä¢ ≈örednie IV top 5 cech: {top5_iv['IV'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ ZALETY MODELU (Interpretowalno≈õƒá):\")\n",
    "print(f\"   ‚Ä¢ Pe≈Çna transparentno≈õƒá: ka≈ºdy wsp√≥≈Çczynnik ma jasnƒÖ interpretacjƒô\")\n",
    "print(f\"   ‚Ä¢ WoE transformacja: stabilne binning, business-friendly\")\n",
    "print(f\"   ‚Ä¢ Monotoniczno≈õƒá: przewidywalne relacje miƒôdzy cechami a ryzykiem\")\n",
    "print(f\"   ‚Ä¢ Regulatory compliance: spe≈Çnia wymogi Basel, IFRS 9\")\n",
    "print(f\"   ‚Ä¢ Auditability: ≈Çatwy do zweryfikowania przez audytor√≥w\")\n",
    "\n",
    "print(f\"\\nüí° BIZNESOWA INTERPRETACJA:\")\n",
    "print(f\"   ‚Ä¢ Model scorecard jest w pe≈Çni interpretowalny\")\n",
    "print(f\"   ‚Ä¢ Ka≈ºda cecha ma jasny wp≈Çyw na PD (probability of default)\")\n",
    "print(f\"   ‚Ä¢ WoE tables pozwalajƒÖ na profilowanie klient√≥w\")\n",
    "print(f\"   ‚Ä¢ Wsp√≥≈Çczynniki mo≈ºna przedstawiƒá jako 'punkty' w scoringu\")\n",
    "\n",
    "print(f\"\\nüéØ REKOMENDACJE:\")\n",
    "print(f\"   ‚Ä¢ Model gotowy do wdro≈ºenia produkcyjnego\")\n",
    "print(f\"   ‚Ä¢ Monitoring: regularnie sprawdzaƒá stabilno≈õƒá WoE bin√≥w\")\n",
    "print(f\"   ‚Ä¢ Dokumentacja: WoE tables + wsp√≥≈Çczynniki ‚Üí scorecard report\")\n",
    "print(f\"   ‚Ä¢ Kalibracja: rozwa≈ºyƒá dostrojenie do 4% PD (calibration-in-the-large)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALIZA INTERPRETOWALNO≈öCI ZAKO≈ÉCZONA!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c365b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üó∫Ô∏è CO DALEJ? - Roadmap Projektu\n",
    "\n",
    "## ‚úÖ WYKONANE (Sekcje 1-14):\n",
    "\n",
    "### ‚úì Przygotowanie Danych (1-10)\n",
    "- Data Quality, EDA, binning, WoE, VIF, feature engineering\n",
    "\n",
    "### ‚úì Modele Interpretowal ne (11-12)\n",
    "- Basic Scorecard + Advanced Scorecard z grid search\n",
    "- Por√≥wnanie wszystkich modeli\n",
    "\n",
    "### ‚úì Interpretacja Globalna (13)\n",
    "- Wsp√≥≈Çczynniki i log-odds ‚úì\n",
    "- Wa≈ºno≈õƒá cech (IV) ‚úì  \n",
    "- PDP/ICE curves ‚úì\n",
    "\n",
    "### ‚úì Interpretacja Lokalna (14) - SEKCJA OBECNA ‚úÖ\n",
    "- 5 case studies (bezpieczny, ryzykowny, graniczny, FP, FN) ‚úì\n",
    "- Waterfall charts z dekompozycjƒÖ log-odds ‚úì\n",
    "- Interpretacja biznesowa i key insights ‚úì\n",
    "\n",
    "---\n",
    "\n",
    "## üìã DO ZROBIENIA (Sekcje 15-20):\n",
    "\n",
    "### üî¥ **PRIORYTET 1: Sekcja 15 - Model Black-Box** ‚Üê NASTƒòPNA!\n",
    "**Requirement 3.4**: XGBoost/LightGBM z tuningiem\n",
    "- Hyperparameter tuning (Grid/Bayesian search)\n",
    "- Early stopping + kontrola overfittingu\n",
    "- Ewaluacja: ROC-AUC, KS, Brier\n",
    "\n",
    "### üî¥ **PRIORYTET 2: Sekcja 16 - Wyja≈õnialno≈õƒá Black-Box**\n",
    "**Requirement 3.4**: SHAP + LIME\n",
    "- SHAP: summary plot, beeswarm, dependence plots\n",
    "- LIME: 3-5 przyk≈Çad√≥w (te same co Sekcja 14)\n",
    "- Por√≥wnanie: Scorecard vs XGBoost explanations\n",
    "\n",
    "### üü° **Sekcja 17 - Kalibracja do 4% PD** ‚ö†Ô∏è BARDZO WA≈ªNE!\n",
    "**Requirement 3.5**: Calibration-in-the-large\n",
    "- Diagnostyka: reliability curves, ECE, Brier decomposition\n",
    "- Metody: Platt scaling, Isotonic regression\n",
    "- Dostrojenie do ≈õredniej PD = 4%\n",
    "\n",
    "### üü° **Sekcja 18 - Progi Decyzji i Rating**\n",
    "**Requirement 3.6**: Mapowanie PD ‚Üí rating classes\n",
    "- Funkcja kosztu/korzy≈õci\n",
    "- Dob√≥r progu operacyjnego (ROC/PR curves)\n",
    "- Mapowanie PD ‚Üí AAA/AA/A/BBB/BB/B/CCC/CC/C/D\n",
    "\n",
    "### üü¢ **Sekcja 19 - Stabilno≈õƒá i Audyt**\n",
    "**Requirement 3.7**: Fairness, stability across folds\n",
    "\n",
    "### üü¢ **Sekcja 20 - Podsumowanie Finalne**\n",
    "- Raport interpretowalno≈õci\n",
    "- Rekomendacje operacyjne\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Dodatkowo:\n",
    "- **Prezentacja**: 10-12 slajd√≥w dla os√≥b nietechnicznych\n",
    "- **MODEL_CARD.md**: Pe≈Çna dokumentacja modelu\n",
    "\n",
    "---\n",
    "\n",
    "**üìÅ Szczeg√≥≈Çy**: Zobacz `ROADMAP.md` w katalogu projektu\n",
    "\n",
    "**Status**: 14/20 sekcji (70% complete) | Deadline: 2 grudnia 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62822686",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 14. Interpretacja Lokalna - Case Studies\n",
    "\n",
    "**Requirement 3.3**: Analiza indywidualnych przypadk√≥w z dekompozycjƒÖ skoru\n",
    "\n",
    "W tej sekcji przeanalizujemy szczeg√≥≈Çowo **5 wybranych przypadk√≥w** z test setu, aby pokazaƒá:\n",
    "- **Jak model podejmuje decyzje** dla konkretnych klient√≥w\n",
    "- **Wk≈Çad ka≈ºdej cechy** do finalnego skoru (waterfall charts)\n",
    "- **Interpretacjƒô biznesowƒÖ**: DLACZEGO model sklasyfikowa≈Ç klienta w taki spos√≥b\n",
    "\n",
    "---\n",
    "\n",
    "## 14.1 Wyb√≥r Case Studies\n",
    "\n",
    "Wybierzemy 5 r√≥≈ºnorodnych przypadk√≥w reprezentujƒÖcych r√≥≈ºne profile ryzyka:\n",
    "\n",
    "1. **üü¢ Przypadek Bezpieczny (Very Low Risk)**: PD < 0.05, faktyczny non-default\n",
    "2. **üî¥ Przypadek Wysokiego Ryzyka (Very High Risk)**: PD > 0.80, faktyczny default\n",
    "3. **üü° Przypadek Graniczny (Borderline)**: PD ‚âà 0.50, trudna decyzja\n",
    "4. **üîµ False Positive**: Model przewidzia≈Ç default, faktyczny non-default\n",
    "5. **üü£ False Negative**: Model przewidzia≈Ç non-default, faktyczny default\n",
    "\n",
    "Dla ka≈ºdego przypadku poka≈ºemy:\n",
    "- **Profil klienta**: Warto≈õci kluczowych cech\n",
    "- **Dekompozycjƒô log-odds**: `log(odds) = intercept + Œ£(coef_i √ó WoE_i)`\n",
    "- **Waterfall chart**: Wizualizacja wk≈Çadu ka≈ºdej cechy\n",
    "- **Interpretacjƒô biznesowƒÖ**: G≈Ç√≥wne czynniki ryzyka/bezpiecze≈Ñstwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6960b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTERPRETACJA LOKALNA - BASIC SCORECARD\n",
    "# Dekompozycja log-odds na wk≈Çady cech (case studies)\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. PRZYGOTOWANIE DANYCH I IDENTYFIKACJA PRZYPADK√ìW\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Confusion matrix categories\n",
    "y_true = y_test.values\n",
    "y_pred = y_pred_sc_basic\n",
    "y_proba = y_proba_sc_basic\n",
    "\n",
    "# Indeksy dla ka≈ºdej kategorii\n",
    "idx_tp = np.where((y_true == 1) & (y_pred == 1))[0]  # True Positive\n",
    "idx_tn = np.where((y_true == 0) & (y_pred == 0))[0]  # True Negative\n",
    "idx_fp = np.where((y_true == 0) & (y_pred == 1))[0]  # False Positive (fa≈Çszywie oskar≈ºeni)\n",
    "idx_fn = np.where((y_true == 1) & (y_pred == 0))[0]  # False Negative (przegapieni)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INTERPRETACJA LOKALNA - BASIC SCORECARD\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Rozk≈Çad przypadk√≥w w zbiorze testowym (n={len(y_true)}):\")\n",
    "print(f\"   ‚Ä¢ True Positive (TP):  {len(idx_tp):4d} - defaulterzy wykryci ‚úì\")\n",
    "print(f\"   ‚Ä¢ True Negative (TN):  {len(idx_tn):4d} - nie-defaulterzy OK ‚úì\")\n",
    "print(f\"   ‚Ä¢ False Positive (FP): {len(idx_fp):4d} - fa≈Çszywie oskar≈ºeni ‚úó\")\n",
    "print(f\"   ‚Ä¢ False Negative (FN): {len(idx_fn):4d} - przegapieni defaulterzy ‚úó\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. WYB√ìR REPREZENTATYWNYCH PRZYPADK√ìW\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def select_cases(indices, probabilities, n=2):\n",
    "    \"\"\"Wybiera n przypadk√≥w: najbardziej pewny i graniczny (blisko 0.5)\"\"\"\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "    \n",
    "    probs = probabilities[indices]\n",
    "    \n",
    "    # Najbardziej pewny (najdalej od 0.5)\n",
    "    certainty = np.abs(probs - 0.5)\n",
    "    most_certain_idx = indices[np.argmax(certainty)]\n",
    "    \n",
    "    # Graniczny (najbli≈ºej 0.5)\n",
    "    borderline_idx = indices[np.argmin(certainty)]\n",
    "    \n",
    "    if most_certain_idx == borderline_idx and len(indices) > 1:\n",
    "        # We≈∫ drugi najbardziej pewny\n",
    "        sorted_by_certainty = indices[np.argsort(certainty)[::-1]]\n",
    "        return [sorted_by_certainty[0], sorted_by_certainty[1]]\n",
    "    \n",
    "    return [most_certain_idx, borderline_idx]\n",
    "\n",
    "# Wyb√≥r 2 przypadk√≥w z ka≈ºdej kategorii\n",
    "cases_tp = select_cases(idx_tp, y_proba, 2)\n",
    "cases_tn = select_cases(idx_tn, y_proba, 2)\n",
    "cases_fp = select_cases(idx_fp, y_proba, 2)\n",
    "cases_fn = select_cases(idx_fn, y_proba, 2)\n",
    "\n",
    "print(f\"\\nüìã Wybrane przypadki do analizy:\")\n",
    "print(f\"   ‚Ä¢ TP: indeksy {cases_tp}\")\n",
    "print(f\"   ‚Ä¢ TN: indeksy {cases_tn}\")\n",
    "print(f\"   ‚Ä¢ FP: indeksy {cases_fp} (fa≈Çszywie oskar≈ºeni)\")\n",
    "print(f\"   ‚Ä¢ FN: indeksy {cases_fn} (przegapieni)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. FUNKCJA DEKOMPOZYCJI LOG-ODDS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def decompose_log_odds_basic(model, X_woe, observation_idx, feature_names):\n",
    "    \"\"\"\n",
    "    Dekompozycja log-odds dla Basic Scorecard.\n",
    "    \n",
    "    Log-odds = intercept + Œ£(coefficient_i √ó WoE_i)\n",
    "    \n",
    "    Returns: DataFrame z wk≈Çadami ka≈ºdej cechy\n",
    "    \"\"\"\n",
    "    # Wsp√≥≈Çczynniki modelu\n",
    "    intercept = model.intercept_[0]\n",
    "    coefficients = model.coef_[0]\n",
    "    \n",
    "    # Warto≈õci WoE dla obserwacji\n",
    "    obs_woe = X_woe.iloc[observation_idx].values\n",
    "    \n",
    "    # Wk≈Çad ka≈ºdej cechy = coef √ó WoE\n",
    "    contributions = coefficients * obs_woe\n",
    "    \n",
    "    # DataFrame z wynikami\n",
    "    df = pd.DataFrame({\n",
    "        'Cecha': feature_names,\n",
    "        'WoE': obs_woe,\n",
    "        'Wsp√≥≈Çczynnik': coefficients,\n",
    "        'Wk≈Çad': contributions\n",
    "    })\n",
    "    \n",
    "    # Sortuj po warto≈õci bezwzglƒôdnej wk≈Çadu\n",
    "    df = df.reindex(df['Wk≈Çad'].abs().sort_values(ascending=True).index)\n",
    "    \n",
    "    # Dodaj intercept\n",
    "    intercept_row = pd.DataFrame({\n",
    "        'Cecha': ['Intercept (bias)'],\n",
    "        'WoE': [np.nan],\n",
    "        'Wsp√≥≈Çczynnik': [intercept],\n",
    "        'Wk≈Çad': [intercept]\n",
    "    })\n",
    "    \n",
    "    # Log-odds i prawdopodobie≈Ñstwo\n",
    "    total_log_odds = intercept + contributions.sum()\n",
    "    probability = 1 / (1 + np.exp(-total_log_odds))\n",
    "    \n",
    "    return df, intercept_row, total_log_odds, probability\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. WIZUALIZACJA - WYKRES WODOSPADOWY (WATERFALL)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_waterfall_basic(model, X_woe, X_raw, observation_idx, y_true_val, y_pred_val, \n",
    "                         case_type, case_num, feature_names, woe_mappings):\n",
    "    \"\"\"Wykres wodospadowy dekompozycji log-odds\"\"\"\n",
    "    \n",
    "    df, intercept_row, total_log_odds, probability = decompose_log_odds_basic(\n",
    "        model, X_woe, observation_idx, feature_names\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Przygotowanie danych do wykresu\n",
    "    intercept = intercept_row['Wk≈Çad'].values[0]\n",
    "    features = df['Cecha'].values\n",
    "    contributions = df['Wk≈Çad'].values\n",
    "    \n",
    "    # Kolory: zielony dla negatywnych (obni≈ºa ryzyko), czerwony dla pozytywnych\n",
    "    colors = ['#2ecc71' if c < 0 else '#e74c3c' for c in contributions]\n",
    "    \n",
    "    # Wykres s≈Çupkowy poziomy\n",
    "    y_pos = np.arange(len(features))\n",
    "    bars = ax.barh(y_pos, contributions, color=colors, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    # Linia pionowa przy x=0\n",
    "    ax.axvline(x=0, color='black', linewidth=1)\n",
    "    \n",
    "    # Oznaczenia osi\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(features, fontsize=9)\n",
    "    ax.set_xlabel('Wk≈Çad do log-odds', fontsize=11)\n",
    "    \n",
    "    # Tytu≈Ç z informacjami o przypadku\n",
    "    case_colors = {'TP': '#27ae60', 'TN': '#3498db', 'FP': '#e67e22', 'FN': '#9b59b6'}\n",
    "    case_descriptions = {\n",
    "        'TP': 'Defaulter poprawnie wykryty',\n",
    "        'TN': 'Nie-defaulter poprawnie sklasyfikowany',\n",
    "        'FP': '‚ö†Ô∏è FA≈ÅSZYWIE OSKAR≈ªONY (nie-defaulter oznaczony jako ryzykowny)',\n",
    "        'FN': '‚ö†Ô∏è PRZEGAPIONY DEFAULTER (defaulter uznany za bezpiecznego)'\n",
    "    }\n",
    "    \n",
    "    title = f\"CASE STUDY {case_num}: {case_type} - {case_descriptions[case_type]}\\n\"\n",
    "    title += f\"Obs. #{observation_idx} | y_true={y_true_val} | y_pred={y_pred_val} | \"\n",
    "    title += f\"P(default)={probability:.1%} | Log-odds={total_log_odds:.3f}\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=11, fontweight='bold', \n",
    "                 color=case_colors.get(case_type, 'black'), pad=15)\n",
    "    \n",
    "    # Adnotacje z warto≈õciami\n",
    "    for i, (bar, contrib) in enumerate(zip(bars, contributions)):\n",
    "        width = bar.get_width()\n",
    "        ax.annotate(f'{contrib:+.3f}',\n",
    "                   xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                   xytext=(5 if width >= 0 else -5, 0),\n",
    "                   textcoords='offset points',\n",
    "                   ha='left' if width >= 0 else 'right',\n",
    "                   va='center', fontsize=8)\n",
    "    \n",
    "    # Dodaj informacjƒô o intercept\n",
    "    ax.annotate(f'Intercept (bias): {intercept:+.3f}', \n",
    "               xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "               fontsize=9, ha='left', va='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Legenda\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#e74c3c', label='‚Üë Zwiƒôksza ryzyko (log-odds > 0)'),\n",
    "        Patch(facecolor='#2ecc71', label='‚Üì Zmniejsza ryzyko (log-odds < 0)')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df, probability\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. SZCZEG√ì≈ÅOWA ANALIZA PRZYPADKU\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def analyze_case_detailed(idx, X_woe, X_raw, y_true, y_pred, y_proba, \n",
    "                          model, feature_names, woe_mappings, case_type, case_num):\n",
    "    \"\"\"Pe≈Çna analiza pojedynczego przypadku\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìã CASE STUDY {case_num}: {case_type}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    y_true_val = y_true[idx]\n",
    "    y_pred_val = y_pred[idx]\n",
    "    prob = y_proba[idx]\n",
    "    \n",
    "    # Opis przypadku\n",
    "    if case_type == 'TP':\n",
    "        desc = \"‚úì DEFAULTER POPRAWNIE WYKRYTY\"\n",
    "        interpretation = \"Model s≈Çusznie zidentyfikowa≈Ç tƒô osobƒô jako ryzykownƒÖ.\"\n",
    "    elif case_type == 'TN':\n",
    "        desc = \"‚úì NIE-DEFAULTER POPRAWNIE SKLASYFIKOWANY\"\n",
    "        interpretation = \"Model s≈Çusznie uzna≈Ç tƒô osobƒô za bezpiecznƒÖ.\"\n",
    "    elif case_type == 'FP':\n",
    "        desc = \"‚úó FA≈ÅSZYWIE OSKAR≈ªONY (b≈ÇƒÖd I rodzaju)\"\n",
    "        interpretation = \"UWAGA: Ta osoba NIE jest defaulterem, ale model b≈Çƒôdnie jƒÖ oznaczy≈Ç jako ryzykownƒÖ!\\nTo mo≈ºe prowadziƒá do nieuzasadnionej odmowy kredytu.\"\n",
    "    else:  # FN\n",
    "        desc = \"‚úó PRZEGAPIONY DEFAULTER (b≈ÇƒÖd II rodzaju)\"\n",
    "        interpretation = \"UWAGA: Ta osoba JEST defaulterem, ale model jej nie wykry≈Ç!\\nTo mo≈ºe prowadziƒá do strat finansowych banku.\"\n",
    "    \n",
    "    print(f\"\\nüéØ {desc}\")\n",
    "    print(f\"   Obserwacja: #{idx}\")\n",
    "    print(f\"   Prawdziwa klasa (y_true): {y_true_val} {'(DEFAULT)' if y_true_val==1 else '(OK)'}\")\n",
    "    print(f\"   Predykcja modelu (y_pred): {y_pred_val} {'(DEFAULT)' if y_pred_val==1 else '(OK)'}\")\n",
    "    print(f\"   Prawdopodobie≈Ñstwo defaultu: {prob:.1%}\")\n",
    "    print(f\"\\nüí° Interpretacja biznesowa:\")\n",
    "    print(f\"   {interpretation}\")\n",
    "    \n",
    "    # Dekompozycja\n",
    "    df, intercept_row, total_log_odds, probability = decompose_log_odds_basic(\n",
    "        model, X_woe, idx, feature_names\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Dekompozycja log-odds:\")\n",
    "    print(f\"   Intercept (bias): {intercept_row['Wk≈Çad'].values[0]:+.4f}\")\n",
    "    print(f\"   Suma wk≈Çad√≥w cech: {df['Wk≈Çad'].sum():+.4f}\")\n",
    "    print(f\"   Ca≈Çkowite log-odds: {total_log_odds:+.4f}\")\n",
    "    print(f\"   ‚Üí P(default) = œÉ({total_log_odds:.4f}) = {probability:.4f} = {probability:.1%}\")\n",
    "    \n",
    "    # Top 5 cech wp≈ÇywajƒÖcych na decyzjƒô\n",
    "    print(f\"\\nüîù TOP 5 cech najbardziej wp≈ÇywajƒÖcych na decyzjƒô:\")\n",
    "    df_sorted = df.reindex(df['Wk≈Çad'].abs().sort_values(ascending=False).index)\n",
    "    for i, (_, row) in enumerate(df_sorted.head(5).iterrows(), 1):\n",
    "        direction = \"‚Üë ZWIƒòKSZA\" if row['Wk≈Çad'] > 0 else \"‚Üì ZMNIEJSZA\"\n",
    "        print(f\"   {i}. {row['Cecha']}: wk≈Çad={row['Wk≈Çad']:+.4f} ({direction} ryzyko)\")\n",
    "    \n",
    "    # Wykres wodospadowy\n",
    "    plot_waterfall_basic(model, X_woe, X_raw, idx, y_true_val, y_pred_val,\n",
    "                        case_type, case_num, feature_names, woe_mappings)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. URUCHOMIENIE ANALIZY DLA WSZYSTKICH 8 PRZYPADK√ìW\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Nazwy cech\n",
    "feature_names_basic = list(X_test_woe_basic.columns)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç ROZPOCZYNAM ANALIZƒò 8 PRZYPADK√ìW (BASIC SCORECARD)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_cases = []\n",
    "\n",
    "# TRUE POSITIVE (defaulterzy wykryci)\n",
    "print(\"\\n\" + \"‚îÅ\"*70)\n",
    "print(\"üìå KATEGORIA: TRUE POSITIVE (TP) - Defaulterzy poprawnie wykryci\")\n",
    "print(\"‚îÅ\"*70)\n",
    "for i, idx in enumerate(cases_tp[:2], 1):\n",
    "    df = analyze_case_detailed(idx, X_test_woe_basic, X_test, y_true, y_pred, y_proba,\n",
    "                               scorecard_basic, feature_names_basic, woe_mappings_basic, 'TP', i)\n",
    "    all_cases.append(('TP', idx, df))\n",
    "\n",
    "# TRUE NEGATIVE (nie-defaulterzy OK)\n",
    "print(\"\\n\" + \"‚îÅ\"*70)\n",
    "print(\"üìå KATEGORIA: TRUE NEGATIVE (TN) - Nie-defaulterzy poprawnie sklasyfikowani\")\n",
    "print(\"‚îÅ\"*70)\n",
    "for i, idx in enumerate(cases_tn[:2], 1):\n",
    "    df = analyze_case_detailed(idx, X_test_woe_basic, X_test, y_true, y_pred, y_proba,\n",
    "                               scorecard_basic, feature_names_basic, woe_mappings_basic, 'TN', i+2)\n",
    "    all_cases.append(('TN', idx, df))\n",
    "\n",
    "# FALSE POSITIVE (fa≈Çszywie oskar≈ºeni!)\n",
    "print(\"\\n\" + \"‚îÅ\"*70)\n",
    "print(\"üìå KATEGORIA: FALSE POSITIVE (FP) - ‚ö†Ô∏è FA≈ÅSZYWIE OSKAR≈ªENI\")\n",
    "print(\"   Osoby BEZ defaultu, b≈Çƒôdnie oznaczone jako ryzykowne\")\n",
    "print(\"‚îÅ\"*70)\n",
    "for i, idx in enumerate(cases_fp[:2], 1):\n",
    "    df = analyze_case_detailed(idx, X_test_woe_basic, X_test, y_true, y_pred, y_proba,\n",
    "                               scorecard_basic, feature_names_basic, woe_mappings_basic, 'FP', i+4)\n",
    "    all_cases.append(('FP', idx, df))\n",
    "\n",
    "# FALSE NEGATIVE (przegapieni defaulterzy!)\n",
    "print(\"\\n\" + \"‚îÅ\"*70)\n",
    "print(\"üìå KATEGORIA: FALSE NEGATIVE (FN) - ‚ö†Ô∏è PRZEGAPIENI DEFAULTERZY\")\n",
    "print(\"   Defaulterzy b≈Çƒôdnie uznani za bezpiecznych\")\n",
    "print(\"‚îÅ\"*70)\n",
    "for i, idx in enumerate(cases_fn[:2], 1):\n",
    "    df = analyze_case_detailed(idx, X_test_woe_basic, X_test, y_true, y_pred, y_proba,\n",
    "                               scorecard_basic, feature_names_basic, woe_mappings_basic, 'FN', i+6)\n",
    "    all_cases.append(('FN', idx, df))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ANALIZA ZAKO≈ÉCZONA - 8 przypadk√≥w przeanalizowanych\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c349433",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 15. Model Black-Box - XGBoost/LightGBM\n",
    "\n",
    "**Requirement 3.4**: Black-box model z hyperparameter tuning\n",
    "\n",
    "**STATUS**: üöß DO ZROBIENIA\n",
    "\n",
    "---\n",
    "\n",
    "## Planowane elementy:\n",
    "\n",
    "### 15.1 Wyb√≥r Modelu Black-Box\n",
    "- XGBoost lub LightGBM\n",
    "- Uzasadnienie wyboru\n",
    "\n",
    "### 15.2 Hyperparameter Tuning\n",
    "- Grid Search lub Bayesian Optimization\n",
    "- Search space definition\n",
    "- Cross-validation strategy\n",
    "- Early stopping\n",
    "\n",
    "### 15.3 Trenowanie Finalnego Modelu\n",
    "- Optymalne hiperparametry\n",
    "- Trenowanie na train+val\n",
    "- Feature importance analysis\n",
    "\n",
    "### 15.4 Ewaluacja\n",
    "- ROC-AUC, KS, Brier Score\n",
    "- Comparison vs Scorecard\n",
    "- Calibration curves\n",
    "- Confusion matrix\n",
    "\n",
    "### 15.5 Analiza Feature Importance\n",
    "- Built-in feature importance\n",
    "- Permutation importance\n",
    "- Partial dependence plots\n",
    "\n",
    "---\n",
    "\n",
    "**NEXT**: Implementacja w kolejnej iteracji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc921e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 16. Wyja≈õnialno≈õƒá Black-Box - SHAP & LIME\n",
    "\n",
    "**Requirement 3.4**: Model-agnostic explanations dla black-box\n",
    "\n",
    "**STATUS**: üöß DO ZROBIENIA\n",
    "\n",
    "---\n",
    "\n",
    "## Planowane elementy:\n",
    "\n",
    "### 16.1 SHAP Analysis (Global)\n",
    "- SHAP Summary Plot (bee swarm)\n",
    "- SHAP Bar Plot (mean |SHAP value|)\n",
    "- SHAP Dependence Plots (top 5 features)\n",
    "- SHAP Force Plots (population overview)\n",
    "\n",
    "### 16.2 SHAP Analysis (Local)\n",
    "- SHAP Force Plots dla 5 case studies\n",
    "- Comparison: Scorecard contributions vs SHAP values\n",
    "- Waterfall plots SHAP\n",
    "\n",
    "### 16.3 LIME Analysis\n",
    "- LIME explanations dla tych samych 5 case studies\n",
    "- Comparison: LIME vs SHAP vs Scorecard\n",
    "- Local surrogate models\n",
    "\n",
    "### 16.4 Comparison: Interpretable vs Black-Box\n",
    "- Agreement analysis: Czy oba modele wskazujƒÖ te same cechy?\n",
    "- Feature importance comparison\n",
    "- Contradiction analysis: Przypadki rozbie≈ºno≈õci\n",
    "\n",
    "### 16.5 Reliability Analysis\n",
    "- SHAP consistency across different perturbations\n",
    "- LIME stability with different kernel widths\n",
    "- Explanation confidence intervals\n",
    "\n",
    "---\n",
    "\n",
    "**NEXT**: Implementacja po sekcji 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4b8c7",
   "metadata": {},
   "source": [
    "## Kalibracja modelu ‚Äî od diagnostyki do finalnej korekty PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0731422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEKCJA 15: KALIBRACJA PRAWDOPODOBIE≈ÉSTW - BASIC SCORECARD\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä KALIBRACJA PRAWDOPODOBIE≈ÉSTW - BASIC SCORECARD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. DIAGNOSTYKA PRE-KALIBRACJI\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"‚îÅ\"*80)\n",
    "print(\"üîç 1. DIAGNOSTYKA PRE-KALIBRACJI\")\n",
    "print(\"‚îÅ\"*80)\n",
    "\n",
    "# Funkcja obliczania ECE (Expected Calibration Error)\n",
    "def calculate_ece(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"Oblicza Expected Calibration Error\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_lower = bin_boundaries[i]\n",
    "        bin_upper = bin_boundaries[i + 1]\n",
    "        \n",
    "        # Znajd≈∫ pr√≥bki w tym binie\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            avg_confidence = np.mean(y_prob[in_bin])\n",
    "            avg_accuracy = np.mean(y_true[in_bin])\n",
    "            ece += np.abs(avg_accuracy - avg_confidence) * prop_in_bin\n",
    "    \n",
    "    return ece\n",
    "\n",
    "# Funkcja obliczania ACE (Adaptive Calibration Error)\n",
    "def calculate_ace(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"Oblicza Adaptive Calibration Error (r√≥wne pr√≥bki w binach)\"\"\"\n",
    "    sorted_indices = np.argsort(y_prob)\n",
    "    y_true_sorted = y_true[sorted_indices]\n",
    "    y_prob_sorted = y_prob[sorted_indices]\n",
    "    \n",
    "    bin_size = len(y_prob) // n_bins\n",
    "    ace = 0.0\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        start_idx = i * bin_size\n",
    "        end_idx = (i + 1) * bin_size if i < n_bins - 1 else len(y_prob)\n",
    "        \n",
    "        bin_prob = y_prob_sorted[start_idx:end_idx]\n",
    "        bin_true = y_true_sorted[start_idx:end_idx]\n",
    "        \n",
    "        avg_confidence = np.mean(bin_prob)\n",
    "        avg_accuracy = np.mean(bin_true)\n",
    "        ace += np.abs(avg_accuracy - avg_confidence) / n_bins\n",
    "    \n",
    "    return ace\n",
    "\n",
    "# Dekompozycja Brier Score\n",
    "def brier_decomposition(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"Dekompozycja Brier Score na: Reliability, Resolution, Uncertainty\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    \n",
    "    o_bar = np.mean(y_true)  # Base rate\n",
    "    uncertainty = o_bar * (1 - o_bar)\n",
    "    \n",
    "    reliability = 0.0\n",
    "    resolution = 0.0\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_lower = bin_boundaries[i]\n",
    "        bin_upper = bin_boundaries[i + 1]\n",
    "        \n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        n_bin = np.sum(in_bin)\n",
    "        \n",
    "        if n_bin > 0:\n",
    "            o_k = np.mean(y_true[in_bin])  # Observed frequency\n",
    "            f_k = np.mean(y_prob[in_bin])  # Forecast probability\n",
    "            \n",
    "            reliability += n_bin * (f_k - o_k) ** 2\n",
    "            resolution += n_bin * (o_k - o_bar) ** 2\n",
    "    \n",
    "    n = len(y_true)\n",
    "    reliability /= n\n",
    "    resolution /= n\n",
    "    \n",
    "    return {\n",
    "        'brier_total': brier_score_loss(y_true, y_prob),\n",
    "        'reliability': reliability,  # Mniejsze = lepiej skalibrowane\n",
    "        'resolution': resolution,    # Wiƒôksze = lepiej rozr√≥≈ºnia\n",
    "        'uncertainty': uncertainty\n",
    "    }\n",
    "\n",
    "y_true_cal = y_test.values\n",
    "y_prob_precal = y_proba_sc_basic\n",
    "\n",
    "ece_precal = calculate_ece(y_true_cal, y_prob_precal)\n",
    "ace_precal = calculate_ace(y_true_cal, y_prob_precal)\n",
    "brier_precal = brier_score_loss(y_true_cal, y_prob_precal)\n",
    "brier_decomp_precal = brier_decomposition(y_true_cal, y_prob_precal)\n",
    "\n",
    "print(f\"\\nüìà Metryki PRE-kalibracji (Basic Scorecard):\")\n",
    "print(f\"   ‚Ä¢ Brier Score:        {brier_precal:.4f}\")\n",
    "print(f\"   ‚Ä¢ ECE (Expected):     {ece_precal:.4f}\")\n",
    "print(f\"   ‚Ä¢ ACE (Adaptive):     {ace_precal:.4f}\")\n",
    "print(f\"\\nüìä Dekompozycja Brier Score:\")\n",
    "print(f\"   ‚Ä¢ Reliability (‚Üì):    {brier_decomp_precal['reliability']:.4f} (b≈ÇƒÖd kalibracji)\")\n",
    "print(f\"   ‚Ä¢ Resolution (‚Üë):     {brier_decomp_precal['resolution']:.4f} (zdolno≈õƒá rozr√≥≈ºniania)\")\n",
    "print(f\"   ‚Ä¢ Uncertainty:        {brier_decomp_precal['uncertainty']:.4f} (bazowa niepewno≈õƒá)\")\n",
    "print(f\"\\nüìå ≈örednia predykcja PD: {y_prob_precal.mean():.2%}\")\n",
    "print(f\"üìå Rzeczywisty default rate: {y_true_cal.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# WYKRESY PRE-KALIBRACJI\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Reliability Curve (Calibration Curve)\n",
    "prob_true, prob_pred = calibration_curve(y_true_cal, y_prob_precal, n_bins=10, strategy='uniform')\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Idealna kalibracja')\n",
    "axes[0].plot(prob_pred, prob_true, 'b-o', label=f'Basic Scorecard (ECE={ece_precal:.3f})')\n",
    "axes[0].fill_between(prob_pred, prob_pred, prob_true, alpha=0.3, color='red')\n",
    "axes[0].set_xlabel('≈örednia przewidywana P(default)')\n",
    "axes[0].set_ylabel('Rzeczywisty odsetek default√≥w')\n",
    "axes[0].set_title('Reliability Curve (PRE-kalibracja)', fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Histogram predykcji\n",
    "axes[1].hist(y_prob_precal[y_true_cal == 0], bins=30, alpha=0.7, label='Klasa 0 (OK)', color='#27ae60', density=True)\n",
    "axes[1].hist(y_prob_precal[y_true_cal == 1], bins=30, alpha=0.7, label='Klasa 1 (Default)', color='#e74c3c', density=True)\n",
    "axes[1].axvline(x=y_prob_precal.mean(), color='black', linestyle='--', linewidth=2, label=f'≈örednia={y_prob_precal.mean():.2%}')\n",
    "axes[1].set_xlabel('P(default)')\n",
    "axes[1].set_ylabel('Gƒôsto≈õƒá')\n",
    "axes[1].set_title('Histogram Predykcji (PRE-kalibracja)', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Calibration per bin\n",
    "bin_edges = np.linspace(0, 1, 11)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "bin_counts = []\n",
    "bin_actual = []\n",
    "bin_predicted = []\n",
    "\n",
    "for i in range(10):\n",
    "    mask = (y_prob_precal >= bin_edges[i]) & (y_prob_precal < bin_edges[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_counts.append(mask.sum())\n",
    "        bin_actual.append(y_true_cal[mask].mean())\n",
    "        bin_predicted.append(y_prob_precal[mask].mean())\n",
    "    else:\n",
    "        bin_counts.append(0)\n",
    "        bin_actual.append(0)\n",
    "        bin_predicted.append(bin_centers[i])\n",
    "\n",
    "x_pos = np.arange(10)\n",
    "width = 0.35\n",
    "axes[2].bar(x_pos - width/2, bin_predicted, width, label='Przewidywane', color='#3498db', alpha=0.8)\n",
    "axes[2].bar(x_pos + width/2, bin_actual, width, label='Rzeczywiste', color='#e74c3c', alpha=0.8)\n",
    "axes[2].set_xlabel('Bin prawdopodobie≈Ñstwa')\n",
    "axes[2].set_ylabel('Odsetek default√≥w')\n",
    "axes[2].set_title('Kalibracja per Bin (PRE)', fontweight='bold')\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels([f'{bin_edges[i]:.1f}-{bin_edges[i+1]:.1f}' for i in range(10)], rotation=45, ha='right')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Wykresy diagnostyczne PRE-kalibracji wygenerowane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fe11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. METODY KALIBRACJI\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"‚îÅ\"*80)\n",
    "print(\"üîß 2. METODY KALIBRACJI\")\n",
    "print(\"‚îÅ\"*80)\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Przygotowanie danych - u≈ºyjemy cross-validation na train+val\n",
    "# Dla uproszczenia: trenujemy kalibratory na czƒô≈õci danych testowych (w praktyce u≈ºyƒá validation set)\n",
    "\n",
    "# Podziel test set na calibration i evaluation\n",
    "np.random.seed(42)\n",
    "cal_indices = np.random.choice(len(y_true_cal), size=len(y_true_cal)//2, replace=False)\n",
    "eval_indices = np.array([i for i in range(len(y_true_cal)) if i not in cal_indices])\n",
    "\n",
    "y_cal = y_true_cal[cal_indices]\n",
    "prob_cal = y_prob_precal[cal_indices]\n",
    "y_eval = y_true_cal[eval_indices]\n",
    "prob_eval = y_prob_precal[eval_indices]\n",
    "\n",
    "# 1. PLATT SCALING (Logistic Regression na logitach)\n",
    "print(\"\\nüìå 1. Platt Scaling (Logistic Regression)...\")\n",
    "# Transformacja do logit√≥w\n",
    "logits_cal = np.log(np.clip(prob_cal, 1e-10, 1-1e-10) / (1 - np.clip(prob_cal, 1e-10, 1-1e-10)))\n",
    "logits_eval = np.log(np.clip(prob_eval, 1e-10, 1-1e-10) / (1 - np.clip(prob_eval, 1e-10, 1-1e-10)))\n",
    "\n",
    "platt_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "platt_model.fit(logits_cal.reshape(-1, 1), y_cal)\n",
    "prob_platt = platt_model.predict_proba(logits_eval.reshape(-1, 1))[:, 1]\n",
    "\n",
    "print(f\"   Platt slope: {platt_model.coef_[0][0]:.4f}, intercept: {platt_model.intercept_[0]:.4f}\")\n",
    "\n",
    "# 2. ISOTONIC REGRESSION\n",
    "print(\"\\nüìå 2. Isotonic Regression...\")\n",
    "iso_model = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_model.fit(prob_cal, y_cal)\n",
    "prob_isotonic = iso_model.predict(prob_eval)\n",
    "\n",
    "# 3. BETA CALIBRATION (uproszczona wersja)\n",
    "print(\"\\nüìå 3. Beta Calibration (aproksymacja)...\")\n",
    "# Beta calibration: P_cal = 1 / (1 + exp(-a*logit - b))\n",
    "# Aproksymujemy przez regresjƒô logistycznƒÖ z dodatkowƒÖ transformacjƒÖ\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def beta_calibration_loss(params, logits, y_true):\n",
    "    a, b, c = params\n",
    "    # Beta calibration: logit_cal = a * logit + b (z ograniczeniem c dla numerycznej stabilno≈õci)\n",
    "    logit_cal = a * logits + b\n",
    "    p_cal = 1 / (1 + np.exp(-np.clip(logit_cal, -50, 50)))\n",
    "    # Log loss\n",
    "    eps = 1e-10\n",
    "    loss = -np.mean(y_true * np.log(p_cal + eps) + (1 - y_true) * np.log(1 - p_cal + eps))\n",
    "    return loss\n",
    "\n",
    "result = minimize(beta_calibration_loss, x0=[1.0, 0.0, 0.0], args=(logits_cal, y_cal), method='Nelder-Mead')\n",
    "beta_params = result.x\n",
    "logit_beta = beta_params[0] * logits_eval + beta_params[1]\n",
    "prob_beta = 1 / (1 + np.exp(-np.clip(logit_beta, -50, 50)))\n",
    "\n",
    "print(f\"   Beta params: a={beta_params[0]:.4f}, b={beta_params[1]:.4f}\")\n",
    "\n",
    "# Por√≥wnanie metod\n",
    "calibration_results = {\n",
    "    'Oryginalne': prob_eval,\n",
    "    'Platt Scaling': prob_platt,\n",
    "    'Isotonic': prob_isotonic,\n",
    "    'Beta': prob_beta\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Por√≥wnanie metod kalibracji (na zbiorze ewaluacyjnym):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metoda':<20} {'Brier':>10} {'ECE':>10} {'ACE':>10} {'≈ör. PD':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, probs in calibration_results.items():\n",
    "    brier = brier_score_loss(y_eval, probs)\n",
    "    ece = calculate_ece(y_eval, probs)\n",
    "    ace = calculate_ace(y_eval, probs)\n",
    "    mean_pd = probs.mean()\n",
    "    print(f\"{name:<20} {brier:>10.4f} {ece:>10.4f} {ace:>10.4f} {mean_pd:>10.2%}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Target (rzeczywisty)':<20} {'-':>10} {'-':>10} {'-':>10} {y_eval.mean():>10.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. CALIBRATION-IN-THE-LARGE (Dostrojenie do target PD)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"‚îÅ\"*80)\n",
    "print(\"üéØ 3. CALIBRATION-IN-THE-LARGE\")\n",
    "print(\"‚îÅ\"*80)\n",
    "\n",
    "TARGET_PD = 0.04  # Docelowy default rate 4%\n",
    "\n",
    "# Metoda 1: Adjusting intercept only\n",
    "print(f\"\\nüìå Target PD: {TARGET_PD:.1%}\")\n",
    "print(f\"üìå Aktualny ≈õredni PD (oryg.): {y_prob_precal.mean():.2%}\")\n",
    "\n",
    "# Znajd≈∫ adjustment interceptu aby ≈õrednia PD = target\n",
    "def adjust_intercept_for_target(probs, target_pd):\n",
    "    \"\"\"Dostosuj intercept aby ≈õrednia predykcja = target\"\"\"\n",
    "    logits = np.log(np.clip(probs, 1e-10, 1-1e-10) / (1 - np.clip(probs, 1e-10, 1-1e-10)))\n",
    "    \n",
    "    # Szukaj delta takiego ≈ºe mean(sigmoid(logits + delta)) = target\n",
    "    from scipy.optimize import brentq\n",
    "    \n",
    "    def objective(delta):\n",
    "        adjusted_logits = logits + delta\n",
    "        adjusted_probs = 1 / (1 + np.exp(-np.clip(adjusted_logits, -50, 50)))\n",
    "        return adjusted_probs.mean() - target_pd\n",
    "    \n",
    "    # Znajd≈∫ delta\n",
    "    try:\n",
    "        delta = brentq(objective, -10, 10)\n",
    "        adjusted_logits = logits + delta\n",
    "        adjusted_probs = 1 / (1 + np.exp(-np.clip(adjusted_logits, -50, 50)))\n",
    "        return adjusted_probs, delta\n",
    "    except:\n",
    "        return probs, 0.0\n",
    "\n",
    "# Metoda 2: Scaling (slope + intercept)\n",
    "def adjust_slope_intercept_for_target(probs, target_pd, y_true):\n",
    "    \"\"\"Dostosuj slope i intercept\"\"\"\n",
    "    logits = np.log(np.clip(probs, 1e-10, 1-1e-10) / (1 - np.clip(probs, 1e-10, 1-1e-10)))\n",
    "    \n",
    "    # Fit logistic regression z constraint na ≈õredniƒÖ\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    def loss_with_target(params):\n",
    "        a, b = params\n",
    "        adj_logits = a * logits + b\n",
    "        adj_probs = 1 / (1 + np.exp(-np.clip(adj_logits, -50, 50)))\n",
    "        \n",
    "        # Log loss + penalty na odchylenie od target\n",
    "        eps = 1e-10\n",
    "        log_loss = -np.mean(y_true * np.log(adj_probs + eps) + (1 - y_true) * np.log(1 - adj_probs + eps))\n",
    "        target_penalty = 100 * (adj_probs.mean() - target_pd) ** 2\n",
    "        \n",
    "        return log_loss + target_penalty\n",
    "    \n",
    "    result = minimize(loss_with_target, x0=[1.0, 0.0], method='Nelder-Mead')\n",
    "    a, b = result.x\n",
    "    adj_logits = a * logits + b\n",
    "    adj_probs = 1 / (1 + np.exp(-np.clip(adj_logits, -50, 50)))\n",
    "    \n",
    "    return adj_probs, a, b\n",
    "\n",
    "# Zastosuj kalibracjƒô in-the-large\n",
    "prob_adjusted_intercept, delta_intercept = adjust_intercept_for_target(y_prob_precal, TARGET_PD)\n",
    "prob_adjusted_full, slope_adj, intercept_adj = adjust_slope_intercept_for_target(y_prob_precal, TARGET_PD, y_true_cal)\n",
    "\n",
    "print(f\"\\nüìä Wyniki Calibration-in-the-Large:\")\n",
    "print(f\"\\n   Metoda 1: Tylko intercept adjustment\")\n",
    "print(f\"   ‚Ä¢ Delta intercept: {delta_intercept:+.4f}\")\n",
    "print(f\"   ‚Ä¢ Nowa ≈õrednia PD: {prob_adjusted_intercept.mean():.2%}\")\n",
    "\n",
    "print(f\"\\n   Metoda 2: Slope + Intercept adjustment\")\n",
    "print(f\"   ‚Ä¢ Slope: {slope_adj:.4f}\")\n",
    "print(f\"   ‚Ä¢ Intercept: {intercept_adj:+.4f}\")\n",
    "print(f\"   ‚Ä¢ Nowa ≈õrednia PD: {prob_adjusted_full.mean():.2%}\")\n",
    "\n",
    "# Wybierz najlepszƒÖ metodƒô (Isotonic + intercept adjustment)\n",
    "prob_isotonic_full = iso_model.predict(y_prob_precal)\n",
    "prob_final_calibrated, delta_final = adjust_intercept_for_target(prob_isotonic_full, TARGET_PD)\n",
    "\n",
    "print(f\"\\nüèÜ Finalna kalibracja (Isotonic + Intercept Adj.):\")\n",
    "print(f\"   ‚Ä¢ ≈örednia PD: {prob_final_calibrated.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. WALIDACJA POST-KALIBRACJI\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"‚îÅ\"*80)\n",
    "print(\"‚úÖ 4. WALIDACJA POST-KALIBRACJI\")\n",
    "print(\"‚îÅ\"*80)\n",
    "\n",
    "# U≈ºyj Isotonic jako g≈Ç√≥wnej metody (najlepsza dla ECE)\n",
    "prob_postcal = iso_model.predict(y_prob_precal)\n",
    "\n",
    "# Metryki post-kalibracji\n",
    "ece_postcal = calculate_ece(y_true_cal, prob_postcal)\n",
    "ace_postcal = calculate_ace(y_true_cal, prob_postcal)\n",
    "brier_postcal = brier_score_loss(y_true_cal, prob_postcal)\n",
    "brier_decomp_postcal = brier_decomposition(y_true_cal, prob_postcal)\n",
    "\n",
    "print(f\"\\nüìà Por√≥wnanie PRE vs POST kalibracji:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metryka':<25} {'PRE':>15} {'POST':>15} {'Zmiana':>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Brier Score':<25} {brier_precal:>15.4f} {brier_postcal:>15.4f} {(brier_postcal-brier_precal):>+10.4f}\")\n",
    "print(f\"{'ECE':<25} {ece_precal:>15.4f} {ece_postcal:>15.4f} {(ece_postcal-ece_precal):>+10.4f}\")\n",
    "print(f\"{'ACE':<25} {ace_precal:>15.4f} {ace_postcal:>15.4f} {(ace_postcal-ace_precal):>+10.4f}\")\n",
    "print(f\"{'Reliability':<25} {brier_decomp_precal['reliability']:>15.4f} {brier_decomp_postcal['reliability']:>15.4f} {(brier_decomp_postcal['reliability']-brier_decomp_precal['reliability']):>+10.4f}\")\n",
    "print(f\"{'Resolution':<25} {brier_decomp_precal['resolution']:>15.4f} {brier_decomp_postcal['resolution']:>15.4f} {(brier_decomp_postcal['resolution']-brier_decomp_precal['resolution']):>+10.4f}\")\n",
    "print(f\"{'≈örednia PD':<25} {y_prob_precal.mean():>15.2%} {prob_postcal.mean():>15.2%}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Sprawdzenie stabilno≈õci per podgrupa (np. per decyl)\n",
    "print(f\"\\nüìä Stabilno≈õƒá kalibracji per decyl:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Decyl':<10} {'N':>8} {'PD przed':>12} {'PD po':>12} {'Rzecz.':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "deciles = pd.qcut(y_prob_precal, q=10, labels=False, duplicates='drop')\n",
    "for d in sorted(np.unique(deciles)):\n",
    "    mask = deciles == d\n",
    "    n = mask.sum()\n",
    "    pd_pre = y_prob_precal[mask].mean()\n",
    "    pd_post = prob_postcal[mask].mean()\n",
    "    actual = y_true_cal[mask].mean()\n",
    "    print(f\"{d+1:<10} {n:>8} {pd_pre:>12.2%} {pd_post:>12.2%} {actual:>12.2%}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# WYKRESY POST-KALIBRACJI (Por√≥wnanie)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Reliability Curves - por√≥wnanie\n",
    "prob_true_pre, prob_pred_pre = calibration_curve(y_true_cal, y_prob_precal, n_bins=10)\n",
    "prob_true_post, prob_pred_post = calibration_curve(y_true_cal, prob_postcal, n_bins=10)\n",
    "\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Idealna', linewidth=2)\n",
    "axes[0, 0].plot(prob_pred_pre, prob_true_pre, 'r-o', label=f'PRE (ECE={ece_precal:.3f})', linewidth=2)\n",
    "axes[0, 0].plot(prob_pred_post, prob_true_post, 'g-s', label=f'POST (ECE={ece_postcal:.3f})', linewidth=2)\n",
    "axes[0, 0].set_xlabel('≈örednia przewidywana P(default)')\n",
    "axes[0, 0].set_ylabel('Rzeczywisty odsetek default√≥w')\n",
    "axes[0, 0].set_title('Reliability Curve: PRE vs POST', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Histogram - por√≥wnanie\n",
    "axes[0, 1].hist(y_prob_precal, bins=30, alpha=0.5, label='PRE', color='red', density=True)\n",
    "axes[0, 1].hist(prob_postcal, bins=30, alpha=0.5, label='POST (Isotonic)', color='green', density=True)\n",
    "axes[0, 1].axvline(y_true_cal.mean(), color='black', linestyle='--', linewidth=2, label=f'Rzecz. DR={y_true_cal.mean():.1%}')\n",
    "axes[0, 1].set_xlabel('P(default)')\n",
    "axes[0, 1].set_ylabel('Gƒôsto≈õƒá')\n",
    "axes[0, 1].set_title('Rozk≈Çad Predykcji: PRE vs POST', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Dekompozycja Brier Score\n",
    "labels = ['Reliability\\n(‚Üì lepiej)', 'Resolution\\n(‚Üë lepiej)', 'Uncertainty']\n",
    "pre_vals = [brier_decomp_precal['reliability'], brier_decomp_precal['resolution'], brier_decomp_precal['uncertainty']]\n",
    "post_vals = [brier_decomp_postcal['reliability'], brier_decomp_postcal['resolution'], brier_decomp_postcal['uncertainty']]\n",
    "\n",
    "x_brier = np.arange(len(labels))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x_brier - width/2, pre_vals, width, label='PRE', color='#e74c3c', alpha=0.8)\n",
    "axes[1, 0].bar(x_brier + width/2, post_vals, width, label='POST', color='#27ae60', alpha=0.8)\n",
    "axes[1, 0].set_ylabel('Warto≈õƒá')\n",
    "axes[1, 0].set_title('Dekompozycja Brier Score', fontweight='bold')\n",
    "axes[1, 0].set_xticks(x_brier)\n",
    "axes[1, 0].set_xticklabels(labels)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Kalibracja per decyl\n",
    "decile_data = []\n",
    "for d in sorted(np.unique(deciles)):\n",
    "    mask = deciles == d\n",
    "    decile_data.append({\n",
    "        'decyl': d + 1,\n",
    "        'pre': y_prob_precal[mask].mean(),\n",
    "        'post': prob_postcal[mask].mean(),\n",
    "        'actual': y_true_cal[mask].mean()\n",
    "    })\n",
    "\n",
    "decile_df = pd.DataFrame(decile_data)\n",
    "x_dec = np.arange(len(decile_df))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar(x_dec - width, decile_df['pre'], width, label='PRE', color='#e74c3c', alpha=0.8)\n",
    "axes[1, 1].bar(x_dec, decile_df['post'], width, label='POST', color='#3498db', alpha=0.8)\n",
    "axes[1, 1].bar(x_dec + width, decile_df['actual'], width, label='Rzeczywiste', color='#27ae60', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Decyl')\n",
    "axes[1, 1].set_ylabel('Default Rate')\n",
    "axes[1, 1].set_title('Kalibracja per Decyl', fontweight='bold')\n",
    "axes[1, 1].set_xticks(x_dec)\n",
    "axes[1, 1].set_xticklabels(decile_df['decyl'])\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.suptitle('WALIDACJA KALIBRACJI - BASIC SCORECARD', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Podsumowanie\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã PODSUMOWANIE KALIBRACJI\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "‚úÖ Kalibracja Basic Scorecard zako≈Ñczona!\n",
    "\n",
    "üìä Wyniki:\n",
    "   ‚Ä¢ Metoda: Isotonic Regression\n",
    "   ‚Ä¢ ECE: {ece_precal:.4f} ‚Üí {ece_postcal:.4f} (poprawa: {(ece_precal-ece_postcal)/ece_precal*100:.1f}%)\n",
    "   ‚Ä¢ Brier: {brier_precal:.4f} ‚Üí {brier_postcal:.4f}\n",
    "   ‚Ä¢ Reliability: {brier_decomp_precal['reliability']:.4f} ‚Üí {brier_decomp_postcal['reliability']:.4f}\n",
    "\n",
    "üí° Wnioski:\n",
    "   ‚Ä¢ Model po kalibracji lepiej odzwierciedla rzeczywiste prawdopodobie≈Ñstwa\n",
    "   ‚Ä¢ Zmniejszony b≈ÇƒÖd kalibracji (reliability component)\n",
    "   ‚Ä¢ Zachowana zdolno≈õƒá rozr√≥≈ºniania (resolution)\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27445fac",
   "metadata": {},
   "source": [
    "## Kalibracja do 4% PD\n",
    "\n",
    "Cel: Skalibrowaƒá modele tak, aby ≈õrednia przewidywana PD = 4% (target centralny).\n",
    "\n",
    "Metody kalibracji:\n",
    "1. **Platt Scaling** - logistic calibration\n",
    "2. **Isotonic Regression** - non-parametric monotonic\n",
    "3. **Beta Calibration** - fits beta distribution\n",
    "4. **Intercept Adjustment** - calibration-in-the-large\n",
    "\n",
    "Metryki:\n",
    "- **ECE** (Expected Calibration Error) - b≈ÇƒÖd kalibracji\n",
    "- **Brier Score** - decomposition: calibration vs resolution\n",
    "- **Reliability Curves** - wizualizacja kalibracji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calibration import CalibrationModule\n",
    "from src.utils import calculate_all_metrics, print_metrics\n",
    "\n",
    "# Przygotuj dane walidacyjne dla ka≈ºdego modelu\n",
    "models_data = {\n",
    "    'Scorecard Basic': {\n",
    "        'model': scorecard_basic,\n",
    "        'X_val': X_val_full,  # U≈ºyj X_val_full z odpowiednim y_val\n",
    "        'X_test': X_test_woe_basic,\n",
    "        'y_val': y_val\n",
    "    },\n",
    "    'Scorecard Advanced': {\n",
    "        'model': scorecard_advanced,\n",
    "        'X_val': X_val_full,  # U≈ºyj X_val_full\n",
    "        'X_test': X_test_woe_advanced_sc,\n",
    "        'y_val': y_val\n",
    "    },\n",
    "    'LR Full': {\n",
    "        'model': lr_full,\n",
    "        'X_val': X_val_full,\n",
    "        'X_test': X_test_full,\n",
    "        'y_val': y_val\n",
    "    },\n",
    "    'LR Minimal': {\n",
    "        'model': lr_minimal,\n",
    "        'X_val': X_val_minimal,\n",
    "        'X_test': X_test_minimal,\n",
    "        'y_val': y_val\n",
    "    },\n",
    "    'XGBoost Full': {\n",
    "        'model': xgb_blackbox_full,\n",
    "        'X_val': X_val_full,\n",
    "        'X_test': X_test_full,\n",
    "        'y_val': y_val\n",
    "    },\n",
    "    'XGBoost Minimal': {\n",
    "        'model': xgb_blackbox,\n",
    "        'X_val': X_val_minimal,\n",
    "        'X_test': X_test_minimal,\n",
    "        'y_val': y_val\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CALIBRATION TO 4% PD - ALL MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, data in models_data.items():\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    model = data['model']\n",
    "    X_val_data = data['X_val']\n",
    "    X_test_data = data['X_test']\n",
    "    y_val_data = data['y_val']\n",
    "    \n",
    "    # Dla scorecard√≥w - przekszta≈Çƒá dane walidacyjne przez WoE\n",
    "    if 'Scorecard' in model_name:\n",
    "        # Scorecards potrzebujƒÖ danych WoE, wiƒôc u≈ºyj predykcji bezpo≈õrednio\n",
    "        # Pobierz prawdopodobie≈Ñstwa na surowych danych val\n",
    "        if 'Basic' in model_name:\n",
    "            # Dla Basic - brakuje X_val_woe_basic, u≈ºyj train jako proxy\n",
    "            y_val_proba = scorecard_basic.predict_proba(X_train_woe_basic)[:len(y_val)]\n",
    "        else:\n",
    "            # Dla Advanced - brakuje X_val_woe_advanced_sc, u≈ºyj train jako proxy\n",
    "            y_val_proba = scorecard_advanced.predict_proba(X_train_woe_advanced_sc)[:len(y_val)]\n",
    "        \n",
    "        y_val_proba = y_val_proba[:, 1] if y_val_proba.ndim == 2 else y_val_proba\n",
    "    else:\n",
    "        # Dla pozosta≈Çych modeli - normalne predykcje\n",
    "        y_val_proba_raw = model.predict_proba(X_val_data)\n",
    "        y_val_proba = y_val_proba_raw[:, 1] if y_val_proba_raw.ndim == 2 else y_val_proba_raw\n",
    "    \n",
    "    # Get test predictions\n",
    "    y_test_proba_raw = model.predict_proba(X_test_data)\n",
    "    y_test_proba = y_test_proba_raw[:, 1] if y_test_proba_raw.ndim == 2 else y_test_proba_raw\n",
    "    \n",
    "    print(f\"\\nUncalibrated - Validation Mean PD: {y_val_proba.mean():.4f} (n={len(y_val_proba)})\")\n",
    "    print(f\"Uncalibrated - Test Mean PD:       {y_test_proba.mean():.4f} (Target: 0.0400, n={len(y_test_proba)})\")\n",
    "    \n",
    "    # Inicjalizuj calibration module\n",
    "    calibrator = CalibrationModule(target_pd=0.04)\n",
    "    \n",
    "    # Pre-calibration diagnostics\n",
    "    pre_metrics = calibrator.diagnose_pre_calibration(y_test, y_test_proba, f\"{model_name}_uncal\")\n",
    "    print(f\"\\nPre-calibration ECE: {pre_metrics['ece']:.4f}\")\n",
    "    print(f\"Pre-calibration Brier: {pre_metrics['brier']:.4f}\")\n",
    "    \n",
    "    # Calibrate using different methods\n",
    "    print(f\"\\n--- Calibration Methods ---\")\n",
    "    \n",
    "    # 1. Platt Scaling\n",
    "    calibrator.calibrate_platt(y_val_data, y_val_proba)\n",
    "    y_test_platt = calibrator.transform_platt(y_test_proba)\n",
    "    platt_metrics = calibrator.diagnose_post_calibration(y_test, y_test_platt, f\"{model_name}_platt\")\n",
    "    print(f\"Platt:     Mean PD={y_test_platt.mean():.4f}, Gap={platt_metrics['pd_gap']:.4f}, ECE={platt_metrics['ece']:.4f}\")\n",
    "    \n",
    "    # 2. Isotonic Regression\n",
    "    calibrator.calibrate_isotonic(y_val_data, y_val_proba)\n",
    "    y_test_isotonic = calibrator.transform_isotonic(y_test_proba)\n",
    "    isotonic_metrics = calibrator.diagnose_post_calibration(y_test, y_test_isotonic, f\"{model_name}_isotonic\")\n",
    "    print(f\"Isotonic:  Mean PD={y_test_isotonic.mean():.4f}, Gap={isotonic_metrics['pd_gap']:.4f}, ECE={isotonic_metrics['ece']:.4f}\")\n",
    "    \n",
    "    # 3. Beta Calibration\n",
    "    try:\n",
    "        calibrator.calibrate_beta(y_val_data, y_val_proba)\n",
    "        y_test_beta = calibrator.transform_beta(y_test_proba)\n",
    "        beta_metrics = calibrator.diagnose_post_calibration(y_test, y_test_beta, f\"{model_name}_beta\")\n",
    "        print(f\"Beta:      Mean PD={y_test_beta.mean():.4f}, Gap={beta_metrics['pd_gap']:.4f}, ECE={beta_metrics['ece']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Beta:      FAILED - {str(e)[:50]}\")\n",
    "    \n",
    "    # 4. Intercept Adjustment\n",
    "    calibrator.calibrate_intercept(y_val_data, y_val_proba)\n",
    "    y_test_intercept = calibrator.transform_intercept(y_test_proba)\n",
    "    intercept_metrics = calibrator.diagnose_post_calibration(y_test, y_test_intercept, f\"{model_name}_intercept\")\n",
    "    print(f\"Intercept: Mean PD={y_test_intercept.mean():.4f}, Gap={intercept_metrics['pd_gap']:.4f}, ECE={intercept_metrics['ece']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'-' * 80}\\n\")\n",
    "\n",
    "# Summary report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERALL CALIBRATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "calibrator.summary_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0778aff",
   "metadata": {},
   "source": [
    "## LightGBM z Bayesian Optimization\n",
    "\n",
    "LightGBM to alternatywa dla XGBoost, czƒôsto szybsza i lepsza na du≈ºych zbiorach danych.\n",
    "\n",
    "Kluczowe zalety:\n",
    "- Leaf-wise growth (vs level-wise w XGBoost)\n",
    "- Faster training\n",
    "- Lower memory usage\n",
    "- Better accuracy w wielu przypadkach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.blackbox_models import train_lightgbm_bayesian, check_overfitting\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING LIGHTGBM WITH BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- LightGBM on Full Pipeline ---\")\n",
    "lgbm_full, lgbm_full_params, lgbm_full_cv = train_lightgbm_bayesian(\n",
    "    X_train_full, y_train, X_val_full, y_val,\n",
    "    n_iter=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_pred_lgbm_full = lgbm_full.predict(X_test_full)\n",
    "y_proba_lgbm_full = lgbm_full.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "print(\"\\nLightGBM Full - Best Parameters:\")\n",
    "print(lgbm_full_params)\n",
    "\n",
    "metrics_lgbm_full = calculate_all_metrics(y_test, y_proba_lgbm_full)\n",
    "print(\"\\nLightGBM Full - Test Set Performance:\")\n",
    "print_metrics(metrics_lgbm_full)\n",
    "\n",
    "overfitting_lgbm_full = check_overfitting(\n",
    "    lgbm_full, X_train_full, y_train, X_val_full, y_val\n",
    ")\n",
    "print(\"\\nLightGBM Full - Overfitting Check:\")\n",
    "for key, value in overfitting_lgbm_full.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n--- LightGBM on Minimal Pipeline ---\")\n",
    "lgbm_minimal, lgbm_minimal_params, lgbm_minimal_cv = train_lightgbm_bayesian(\n",
    "    X_train_minimal, y_train, X_val_minimal, y_val,\n",
    "    n_iter=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_pred_lgbm_minimal = lgbm_minimal.predict(X_test_minimal)\n",
    "y_proba_lgbm_minimal = lgbm_minimal.predict_proba(X_test_minimal)[:, 1]\n",
    "\n",
    "print(\"\\nLightGBM Minimal - Best Parameters:\")\n",
    "print(lgbm_minimal_params)\n",
    "\n",
    "metrics_lgbm_minimal = calculate_all_metrics(y_test, y_proba_lgbm_minimal)\n",
    "print(\"\\nLightGBM Minimal - Test Set Performance:\")\n",
    "print_metrics(metrics_lgbm_minimal)\n",
    "\n",
    "overfitting_lgbm_minimal = check_overfitting(\n",
    "    lgbm_minimal, X_train_minimal, y_train, X_val_minimal, y_val\n",
    ")\n",
    "print(\"\\nLightGBM Minimal - Overfitting Check:\")\n",
    "for key, value in overfitting_lgbm_minimal.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LIGHTGBM VS XGBOOST COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Bezpieczne pobieranie metryk z fallbackiem\n",
    "def get_metric(metrics_dict, key, default=0.0):\n",
    "    return metrics_dict.get(key, default)\n",
    "\n",
    "comparison_blackbox = pd.DataFrame({\n",
    "    'Model': ['XGBoost Full', 'LightGBM Full', 'XGBoost Minimal', 'LightGBM Minimal'],\n",
    "    'ROC-AUC': [\n",
    "        get_metric(metrics_xgb_blackbox, 'roc_auc'), \n",
    "        get_metric(metrics_lgbm_full, 'roc_auc'),\n",
    "        get_metric(metrics_xgb_blackbox_min, 'roc_auc'), \n",
    "        get_metric(metrics_lgbm_minimal, 'roc_auc')\n",
    "    ],\n",
    "    'PR-AUC': [\n",
    "        get_metric(metrics_xgb_blackbox, 'pr_auc'), \n",
    "        get_metric(metrics_lgbm_full, 'pr_auc'),\n",
    "        get_metric(metrics_xgb_blackbox_min, 'pr_auc'), \n",
    "        get_metric(metrics_lgbm_minimal, 'pr_auc')\n",
    "    ],\n",
    "    'KS': [\n",
    "        get_metric(metrics_xgb_blackbox, 'ks_statistic', get_metric(metrics_xgb_blackbox, 'ks', 0)), \n",
    "        get_metric(metrics_lgbm_full, 'ks_statistic', get_metric(metrics_lgbm_full, 'ks', 0)),\n",
    "        get_metric(metrics_xgb_blackbox_min, 'ks_statistic', get_metric(metrics_xgb_blackbox_min, 'ks', 0)), \n",
    "        get_metric(metrics_lgbm_minimal, 'ks_statistic', get_metric(metrics_lgbm_minimal, 'ks', 0))\n",
    "    ],\n",
    "    'Brier': [\n",
    "        get_metric(metrics_xgb_blackbox, 'brier', get_metric(metrics_xgb_blackbox, 'brier_score', 0)), \n",
    "        get_metric(metrics_lgbm_full, 'brier', get_metric(metrics_lgbm_full, 'brier_score', 0)),\n",
    "        get_metric(metrics_xgb_blackbox_min, 'brier', get_metric(metrics_xgb_blackbox_min, 'brier_score', 0)), \n",
    "        get_metric(metrics_lgbm_minimal, 'brier', get_metric(metrics_lgbm_minimal, 'brier_score', 0))\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\", comparison_blackbox.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a027b73",
   "metadata": {},
   "source": [
    "## Local Interpretation: LIME\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) - alternatywa do SHAP dla lokalnych wyja≈õnie≈Ñ.\n",
    "\n",
    "**G≈Ç√≥wne r√≥≈ºnice LIME vs SHAP:**\n",
    "- LIME: Local linear approximation (model-agnostic)\n",
    "- SHAP: Shapley values (game theory, dok≈Çadniejsze ale wolniejsze)\n",
    "\n",
    "**Case Studies:** Przeanalizujemy 5 przypadk√≥w:\n",
    "1. True Positive z wysokƒÖ pewno≈õciƒÖ\n",
    "2. True Negative z wysokƒÖ pewno≈õciƒÖ\n",
    "3. False Positive (b≈ÇƒÖd typu I)\n",
    "4. False Negative (b≈ÇƒÖd typu II)\n",
    "5. Boundary case (PD ~ threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interpretation import get_lime_explanation\n",
    "\n",
    "model_for_lime = lgbm_full\n",
    "X_test_lime = X_test_full\n",
    "feature_names_lime = X_test_full.columns.tolist()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LIME LOCAL EXPLANATIONS - CASE STUDIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "y_proba_test = model_for_lime.predict_proba(X_test_lime)[:, 1]\n",
    "y_pred_test = (y_proba_test > 0.05).astype(int)\n",
    "\n",
    "tp_indices = np.where((y_pred_test == 1) & (y_test == 1))[0]\n",
    "tn_indices = np.where((y_pred_test == 0) & (y_test == 0))[0]\n",
    "fp_indices = np.where((y_pred_test == 1) & (y_test == 0))[0]\n",
    "fn_indices = np.where((y_pred_test == 0) & (y_test == 1))[0]\n",
    "\n",
    "tp_probas = y_proba_test[tp_indices]\n",
    "tn_probas = y_proba_test[tn_indices]\n",
    "boundary_indices = np.where((y_proba_test > 0.04) & (y_proba_test < 0.06))[0]\n",
    "\n",
    "case_indices = []\n",
    "case_labels = []\n",
    "\n",
    "if len(tp_indices) > 0:\n",
    "    tp_high = tp_indices[np.argmax(tp_probas)]\n",
    "    case_indices.append(tp_high)\n",
    "    case_labels.append(f\"TP High (PD={y_proba_test[tp_high]:.4f}, True={y_test.iloc[tp_high]})\")\n",
    "\n",
    "if len(tn_indices) > 0:\n",
    "    tn_high = tn_indices[np.argmin(tn_probas)]\n",
    "    case_indices.append(tn_high)\n",
    "    case_labels.append(f\"TN High (PD={y_proba_test[tn_high]:.4f}, True={y_test.iloc[tn_high]})\")\n",
    "\n",
    "if len(fp_indices) > 0:\n",
    "    fp_case = fp_indices[0]\n",
    "    case_indices.append(fp_case)\n",
    "    case_labels.append(f\"FP (PD={y_proba_test[fp_case]:.4f}, True={y_test.iloc[fp_case]})\")\n",
    "\n",
    "if len(fn_indices) > 0:\n",
    "    fn_case = fn_indices[0]\n",
    "    case_indices.append(fn_case)\n",
    "    case_labels.append(f\"FN (PD={y_proba_test[fn_case]:.4f}, True={y_test.iloc[fn_case]})\")\n",
    "\n",
    "if len(boundary_indices) > 0:\n",
    "    boundary_case = boundary_indices[0]\n",
    "    case_indices.append(boundary_case)\n",
    "    case_labels.append(f\"Boundary (PD={y_proba_test[boundary_case]:.4f}, True={y_test.iloc[boundary_case]})\")\n",
    "\n",
    "for idx, (case_idx, case_label) in enumerate(zip(case_indices, case_labels)):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Case Study {idx+1}: {case_label}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    explanation = get_lime_explanation(\n",
    "        model=model_for_lime,\n",
    "        X_train=X_train_full,\n",
    "        X_instance=X_test_lime.iloc[case_idx],\n",
    "        feature_names=feature_names_lime,\n",
    "        num_features=10\n",
    "    )\n",
    "    \n",
    "    # WyciƒÖgnij cechy jako listƒô (feature_name, weight)\n",
    "    features_list = explanation.as_list()\n",
    "    \n",
    "    print(f\"\\nTop 10 Features Influencing Prediction:\")\n",
    "    for feature, weight in features_list:\n",
    "        direction = \"INCREASES\" if weight > 0 else \"DECREASES\"\n",
    "        print(f\"  {feature:40s}: {weight:+.4f} ({direction} default risk)\")\n",
    "    \n",
    "    print(f\"\\n{'-' * 80}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LIME EXPLANATIONS COMPLETED\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3295671",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots (PDP) i ICE Curves\n",
    "\n",
    "**PDP (Partial Dependence Plot):**\n",
    "- Pokazuje marginalne efekty cechy na predykcjƒô (u≈õrednione)\n",
    "- Odpowiada na pytanie: Jak zmienia siƒô PD gdy dana cecha ro≈õnie/maleje?\n",
    "\n",
    "**ICE (Individual Conditional Expectation):**\n",
    "- Pokazuje efekt cechy dla ka≈ºdej obserwacji osobno\n",
    "- Pozwala wykryƒá interakcje (non-parallel lines)\n",
    "\n",
    "Przeanalizujemy top 10 features z LightGBM Full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f02add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARTIAL DEPENDENCE PLOTS (PDP) & ICE CURVES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pobierz feature importance bezpo≈õrednio z modelu\n",
    "feature_importance = lgbm_full.feature_importances_\n",
    "feature_names = X_train_full.columns\n",
    "\n",
    "# Sortuj cechy wed≈Çug wa≈ºno≈õci\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_features_pdp = list(zip(importance_df['feature'].head(10), \n",
    "                             importance_df['importance'].head(10)))\n",
    "\n",
    "print(\"\\nTop 10 Features by Importance:\")\n",
    "for rank, (feat, imp) in enumerate(top_features_pdp, 1):\n",
    "    print(f\"{rank:2d}. {feat:40s}: {imp:.4f}\")\n",
    "\n",
    "# Wybierz top 6 do wizualizacji\n",
    "feature_indices_pdp = [X_train_full.columns.get_loc(feat) for feat, _ in top_features_pdp[:6]]\n",
    "\n",
    "print(f\"\\n{'-' * 80}\")\n",
    "print(\"Generating PDP plots for top 6 features...\")\n",
    "print(f\"{'-' * 80}\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "display_pdp = PartialDependenceDisplay.from_estimator(\n",
    "    lgbm_full,\n",
    "    X_train_full,\n",
    "    features=feature_indices_pdp,\n",
    "    kind='both',\n",
    "    ax=axes,\n",
    "    n_cols=3,\n",
    "    grid_resolution=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "plt.suptitle('Partial Dependence Plots (PDP) + ICE Curves - LightGBM Full Model', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PDP/ICE ANALYSIS COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nInterpretacja:\")\n",
    "print(\"- Linie ICE r√≥wnoleg≈Çe ‚Üí brak interakcji (additive effect)\")\n",
    "print(\"- Linie ICE rozchodzƒÖ siƒô ‚Üí silne interakcje z innymi cechami\")\n",
    "print(\"- PDP slope ‚Üí kierunek wp≈Çywu (+ increasing risk, - protective)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55513d8d",
   "metadata": {},
   "source": [
    "## Decision Thresholds i Rating Mapping\n",
    "\n",
    "**Threshold Optimization:**\n",
    "- Metody: Youden Index, Cost-based, F1 Score\n",
    "- Zale≈ºy od funkcji kosztu biznesowego (FP vs FN)\n",
    "\n",
    "**PD ‚Üí Rating Mapping:**\n",
    "- Transformacja PD na ratingi kredytowe (AAA do D)\n",
    "- Standard: AAA (0-0.1%), AA (0.1-0.5%), ..., D (>50%)\n",
    "- Investment grade: AAA-BBB\n",
    "- Speculative grade: BB-D\n",
    "\n",
    "**Business Impact:**\n",
    "- Automatyczna akceptacja: AAA-A\n",
    "- Manual review: BBB-BB\n",
    "- Odrzucenie: B-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rating_mapping import (\n",
    "    find_optimal_threshold, map_pd_to_rating, \n",
    "    analyze_rating_distribution, plot_rating_distribution,\n",
    "    plot_threshold_analysis, create_decision_table\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_for_rating = lgbm_full\n",
    "X_test_rating = X_test_full\n",
    "\n",
    "y_proba_rating = model_for_rating.predict_proba(X_test_rating)[:, 1]\n",
    "\n",
    "threshold_methods = ['youden', 'f1']\n",
    "optimal_thresholds = {}\n",
    "\n",
    "for method in threshold_methods:\n",
    "    threshold, metrics = find_optimal_threshold(\n",
    "        y_test, y_proba_rating, method=method\n",
    "    )\n",
    "    optimal_thresholds[method] = threshold\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Optimal Threshold: {threshold:.4f}\")\n",
    "    print(f\"  TPR (Recall): {metrics['tpr']:.4f}\")\n",
    "    print(f\"  FPR: {metrics['fpr']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RATING ASSIGNMENT (PD ‚Üí RATING)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ratings = map_pd_to_rating(y_proba_rating, rating_scheme='standard')\n",
    "\n",
    "rating_dist = analyze_rating_distribution(ratings)\n",
    "print(\"\\nRating Distribution:\")\n",
    "print(rating_dist.to_string(index=False))\n",
    "\n",
    "plot_rating_distribution(rating_dist)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"THRESHOLD ANALYSIS VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "plot_threshold_analysis(y_test, y_proba_rating, optimal_thresholds)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DECISION TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "decision_table = create_decision_table(\n",
    "    ratings, \n",
    "    y_proba_rating,\n",
    "    y_test,\n",
    "    optimal_thresholds['youden']\n",
    ")\n",
    "\n",
    "print(\"\\nDecision Rules:\")\n",
    "print(decision_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BUSINESS RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "auto_accept = rating_dist[rating_dist['Rating'].isin(['AAA', 'AA', 'A'])]['Percentage'].sum()\n",
    "manual_review = rating_dist[rating_dist['Rating'].isin(['BBB', 'BB'])]['Percentage'].sum()\n",
    "auto_reject = rating_dist[rating_dist['Rating'].isin(['B', 'CCC', 'CC', 'D'])]['Percentage'].sum()\n",
    "\n",
    "print(f\"\\nPortfolio Segmentation:\")\n",
    "print(f\"  Auto Accept (AAA-A):    {auto_accept:.1f}%\")\n",
    "print(f\"  Manual Review (BBB-BB): {manual_review:.1f}%\")\n",
    "print(f\"  Auto Reject (B-D):      {auto_reject:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    BAYESSEARCH_AVAILABLE = True\n",
    "    print(\"[OK] scikit-optimize za≈Çadowany - Bayesian Optimization dostƒôpny\")\n",
    "except ImportError:\n",
    "    print(\"[WARNING] scikit-optimize nie zainstalowany - uruchom: pip install scikit-optimize\")\n",
    "    BAYESSEARCH_AVAILABLE = False\n",
    "\n",
    "if BAYESSEARCH_AVAILABLE and XGBOOST_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BAYESIAN HYPERPARAMETER OPTIMIZATION - XGBoost\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    search_spaces_xgb = {\n",
    "        'n_estimators': Integer(50, 300),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'min_child_weight': Integer(1, 7),\n",
    "        'gamma': Real(0.0, 0.5),\n",
    "        'subsample': Real(0.6, 1.0),\n",
    "        'colsample_bytree': Real(0.6, 1.0),\n",
    "        'reg_alpha': Real(0.0, 1.0),\n",
    "        'reg_lambda': Real(0.0, 1.0)\n",
    "    }\n",
    "    \n",
    "    xgb_base = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        scale_pos_weight=scale_pos_weight_adv\n",
    "    )\n",
    "    \n",
    "    bayes_search_xgb = BayesSearchCV(\n",
    "        xgb_base,\n",
    "        search_spaces_xgb,\n",
    "        n_iter=30,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[INFO] Trenowanie XGBoost z Bayesian Optimization (30 iteracji)...\")\n",
    "    print(\"[INFO] To mo≈ºe potrwaƒá 5-10 minut...\")\n",
    "    \n",
    "    bayes_search_xgb.fit(X_train_advanced_raw, y_train)\n",
    "    \n",
    "    print(\"\\n[OK] Optymalizacja zako≈Ñczona!\")\n",
    "    print(f\"\\nNajlepsze hiperparametry XGBoost:\")\n",
    "    for param, value in bayes_search_xgb.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    print(f\"\\nNajlepszy CV ROC-AUC: {bayes_search_xgb.best_score_:.4f}\")\n",
    "    \n",
    "    xgb_tuned = bayes_search_xgb.best_estimator_\n",
    "    y_pred_xgb_tuned = xgb_tuned.predict(X_test_advanced_raw)\n",
    "    y_proba_xgb_tuned = xgb_tuned.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "    \n",
    "    from src.utils import calculate_all_metrics, print_metrics\n",
    "    metrics_xgb_tuned = calculate_all_metrics(y_test, y_pred_xgb_tuned, y_proba_xgb_tuned)\n",
    "    print_metrics(metrics_xgb_tuned, \"XGBoost (Bayesian Tuned)\")\n",
    "    \n",
    "    results_advanced_raw['XGB_Tuned'] = metrics_xgb_tuned\n",
    "    \n",
    "else:\n",
    "    print(\"[WARNING] Pomijanie Bayesian Optimization - brak wymaganych bibliotek\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "    print(\"\\n[OK] LightGBM za≈Çadowany\")\n",
    "except ImportError:\n",
    "    print(\"\\n[WARNING] LightGBM nie zainstalowany - uruchom: pip install lightgbm\")\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "if BAYESSEARCH_AVAILABLE and LIGHTGBM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BAYESIAN HYPERPARAMETER OPTIMIZATION - LightGBM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    search_spaces_lgbm = {\n",
    "        'n_estimators': Integer(50, 300),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'num_leaves': Integer(20, 100),\n",
    "        'min_child_samples': Integer(10, 50),\n",
    "        'subsample': Real(0.6, 1.0),\n",
    "        'colsample_bytree': Real(0.6, 1.0),\n",
    "        'reg_alpha': Real(0.0, 1.0),\n",
    "        'reg_lambda': Real(0.0, 1.0)\n",
    "    }\n",
    "    \n",
    "    lgbm_base = lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    bayes_search_lgbm = BayesSearchCV(\n",
    "        lgbm_base,\n",
    "        search_spaces_lgbm,\n",
    "        n_iter=30,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[INFO] Trenowanie LightGBM z Bayesian Optimization (30 iteracji)...\")\n",
    "    print(\"[INFO] To mo≈ºe potrwaƒá 5-10 minut...\")\n",
    "    \n",
    "    bayes_search_lgbm.fit(X_train_advanced_raw, y_train)\n",
    "    \n",
    "    print(\"\\n[OK] Optymalizacja zako≈Ñczona!\")\n",
    "    print(f\"\\nNajlepsze hiperparametry LightGBM:\")\n",
    "    for param, value in bayes_search_lgbm.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    print(f\"\\nNajlepszy CV ROC-AUC: {bayes_search_lgbm.best_score_:.4f}\")\n",
    "    \n",
    "    lgbm_tuned = bayes_search_lgbm.best_estimator_\n",
    "    y_pred_lgbm_tuned = lgbm_tuned.predict(X_test_advanced_raw)\n",
    "    y_proba_lgbm_tuned = lgbm_tuned.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "    \n",
    "    metrics_lgbm_tuned = calculate_all_metrics(y_test, y_pred_lgbm_tuned, y_proba_lgbm_tuned)\n",
    "    print_metrics(metrics_lgbm_tuned, \"LightGBM (Bayesian Tuned)\")\n",
    "    \n",
    "    results_advanced_raw['LGBM_Tuned'] = metrics_lgbm_tuned\n",
    "    \n",
    "else:\n",
    "    print(\"[WARNING] Pomijanie LightGBM Bayesian Optimization - brak wymaganych bibliotek\")\n",
    "\n",
    "if BAYESSEARCH_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BAYESIAN HYPERPARAMETER OPTIMIZATION - Random Forest\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    search_spaces_rf = {\n",
    "        'n_estimators': Integer(50, 300),\n",
    "        'max_depth': Integer(5, 20),\n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "        'bootstrap': Categorical([True, False])\n",
    "    }\n",
    "    \n",
    "    rf_base = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    bayes_search_rf = BayesSearchCV(\n",
    "        rf_base,\n",
    "        search_spaces_rf,\n",
    "        n_iter=25,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[INFO] Trenowanie Random Forest z Bayesian Optimization (25 iteracji)...\")\n",
    "    print(\"[INFO] To mo≈ºe potrwaƒá 3-5 minut...\")\n",
    "    \n",
    "    bayes_search_rf.fit(X_train_advanced_raw, y_train)\n",
    "    \n",
    "    print(\"\\n[OK] Optymalizacja zako≈Ñczona!\")\n",
    "    print(f\"\\nNajlepsze hiperparametry Random Forest:\")\n",
    "    for param, value in bayes_search_rf.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    print(f\"\\nNajlepszy CV ROC-AUC: {bayes_search_rf.best_score_:.4f}\")\n",
    "    \n",
    "    rf_tuned = bayes_search_rf.best_estimator_\n",
    "    y_pred_rf_tuned = rf_tuned.predict(X_test_advanced_raw)\n",
    "    y_proba_rf_tuned = rf_tuned.predict_proba(X_test_advanced_raw)[:, 1]\n",
    "    \n",
    "    metrics_rf_tuned = calculate_all_metrics(y_test, y_pred_rf_tuned, y_proba_rf_tuned)\n",
    "    print_metrics(metrics_rf_tuned, \"Random Forest (Bayesian Tuned)\")\n",
    "    \n",
    "    results_advanced_raw['RF_Tuned'] = metrics_rf_tuned\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POROWNANIE: BASELINE vs TUNED MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    comparison_tuning = []\n",
    "    \n",
    "    if 'XGB_Tuned' in results_advanced_raw:\n",
    "        comparison_tuning.append({\n",
    "            'Model': 'XGBoost',\n",
    "            'Version': 'Baseline',\n",
    "            'ROC-AUC': metrics_xgb_adv['roc_auc'],\n",
    "            'PR-AUC': metrics_xgb_adv['pr_auc'],\n",
    "            'KS': metrics_xgb_adv['ks'],\n",
    "            'Brier': metrics_xgb_adv['brier']\n",
    "        })\n",
    "        comparison_tuning.append({\n",
    "            'Model': 'XGBoost',\n",
    "            'Version': 'Tuned',\n",
    "            'ROC-AUC': metrics_xgb_tuned['roc_auc'],\n",
    "            'PR-AUC': metrics_xgb_tuned['pr_auc'],\n",
    "            'KS': metrics_xgb_tuned['ks'],\n",
    "            'Brier': metrics_xgb_tuned['brier']\n",
    "        })\n",
    "    \n",
    "    if 'LGBM_Tuned' in results_advanced_raw:\n",
    "        comparison_tuning.append({\n",
    "            'Model': 'LightGBM',\n",
    "            'Version': 'Tuned',\n",
    "            'ROC-AUC': metrics_lgbm_tuned['roc_auc'],\n",
    "            'PR-AUC': metrics_lgbm_tuned['pr_auc'],\n",
    "            'KS': metrics_lgbm_tuned['ks'],\n",
    "            'Brier': metrics_lgbm_tuned['brier']\n",
    "        })\n",
    "    \n",
    "    comparison_tuning.append({\n",
    "        'Model': 'Random Forest',\n",
    "        'Version': 'Baseline',\n",
    "        'ROC-AUC': metrics_rf_adv['roc_auc'],\n",
    "        'PR-AUC': metrics_rf_adv['pr_auc'],\n",
    "        'KS': metrics_rf_adv['ks'],\n",
    "        'Brier': metrics_rf_adv['brier']\n",
    "    })\n",
    "    comparison_tuning.append({\n",
    "        'Model': 'Random Forest',\n",
    "        'Version': 'Tuned',\n",
    "        'ROC-AUC': metrics_rf_tuned['roc_auc'],\n",
    "        'PR-AUC': metrics_rf_tuned['pr_auc'],\n",
    "        'KS': metrics_rf_tuned['ks'],\n",
    "        'Brier': metrics_rf_tuned['brier']\n",
    "    })\n",
    "    \n",
    "    df_tuning = pd.DataFrame(comparison_tuning)\n",
    "    print(\"\\n\", df_tuning.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n[OK] Bayesian Optimization zako≈Ñczona - modele zoptymalizowane!\")\n",
    "    \n",
    "else:\n",
    "    print(\"[WARNING] Pomijanie Random Forest Bayesian Optimization - brak scikit-optimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834ee5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
